{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9eb3ae1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 讀取成功，資料維度： (19649, 53)\n",
      "Train: (13754, 48), Val: (3930, 48), Test: (1965, 48)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 路徑\n",
    "load_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/combined_csi_rssi_slim2.4G.csv\"\n",
    "\n",
    "# 讀取資料\n",
    "df = pd.read_csv(load_path)\n",
    "print(\"✅ 讀取成功，資料維度：\", df.shape)\n",
    "\n",
    "# 只用CSI amplitude (前48欄)\n",
    "amp = df.iloc[:, 0:48].values   # shape: (N, 48)\n",
    "labels = df[\"Label\"].values     # shape: (N,)\n",
    "orig_idx = df.index.values      # 原始index方便對照\n",
    "\n",
    "# train/val/test切分\n",
    "amp_train, amp_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(\n",
    "    amp, labels, orig_idx, test_size=0.3, random_state=42\n",
    ")\n",
    "amp_val, amp_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
    "    amp_temp, y_temp, idx_temp, test_size=1/3, random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Train: {amp_train.shape}, Val: {amp_val.shape}, Test: {amp_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "556c1b45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練用\n",
    "train_data = amp_train    # shape: [N_train, 48]\n",
    "train_labels = y_train    # shape: [N_train]\n",
    "\n",
    "# 驗證用\n",
    "val_data = amp_val        # shape: [N_val, 48]\n",
    "val_labels = y_val        # shape: [N_val]\n",
    "\n",
    "# 測試用\n",
    "test_data = amp_test      # shape: [N_test, 48]\n",
    "test_labels = y_test      # shape: [N_test]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7aa06d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 假設你已經定義好這兩個class\n",
    "class SiameseSubNet1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(48, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subnet = SiameseSubNet1D()\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.subnet(x1)\n",
    "        out2 = self.subnet(x2)\n",
    "        distance = F.pairwise_distance(out1, out2)\n",
    "        return distance\n",
    "\n",
    "# 重點：這裡要用 SiameseNetwork1D\n",
    "model = SiameseNetwork1D().cuda()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cd42facc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=4.0):\n",
    "        super(ContrastiveLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "\n",
    "    def forward(self, output, label):\n",
    "        # label: 1=positive, 0=negative\n",
    "        loss = 0.5 * label * torch.pow(output, 2) + \\\n",
    "               0.5 * (1 - label) * torch.pow(torch.clamp(self.margin - output, min=0.0), 2)\n",
    "        return loss.mean()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a446aa1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class CSIAllPairsDataset(Dataset):\n",
    "    def __init__(self, X, y, T1):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = np.array(y)\n",
    "        self.label_set = np.unique(self.y)\n",
    "        self.T1 = T1\n",
    "        self.idx_by_label = {label: np.where(self.y == label)[0][:T1] for label in self.label_set}\n",
    "\n",
    "        self.fingerprints = {}  # 每個RP一個fingerprint\n",
    "        for label in self.label_set:\n",
    "            idxs = self.idx_by_label[label]\n",
    "            self.fingerprints[label] = self.X[idxs].mean(dim=0)  # 平均後 shape: [48]\n",
    "\n",
    "        # 所有pair列表：(data_idx, fp_label, label)\n",
    "        # 正樣本: label=1, 負樣本: label=0\n",
    "        self.pairs = []\n",
    "        for rp in self.label_set:\n",
    "            idxs = self.idx_by_label[rp]\n",
    "            for i in idxs:\n",
    "                for fp_rp in self.label_set:\n",
    "                    label = 1 if rp == fp_rp else 0\n",
    "                    self.pairs.append((i, fp_rp, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        i, fp_label, label = self.pairs[idx]\n",
    "        x1 = self.X[i]\n",
    "        x2 = self.fingerprints[fp_label]\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "97a8bcd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/5 ===\n",
      "Epoch [1], Train Loss: 29.7136, Val Loss: 17.1391, LR: 0.10000\n",
      "Epoch [2], Train Loss: 12.4512, Val Loss: 9.3461, LR: 0.10000\n",
      "Epoch [3], Train Loss: 6.8889, Val Loss: 6.1210, LR: 0.10000\n",
      "Epoch [4], Train Loss: 4.7025, Val Loss: 4.6615, LR: 0.10000\n",
      "Epoch [5], Train Loss: 3.5297, Val Loss: 3.7709, LR: 0.10000\n",
      "Epoch [6], Train Loss: 2.8663, Val Loss: 3.1602, LR: 0.10000\n",
      "Epoch [7], Train Loss: 2.3482, Val Loss: 2.7076, LR: 0.10000\n",
      "Epoch [8], Train Loss: 1.9965, Val Loss: 2.3872, LR: 0.10000\n",
      "Epoch [9], Train Loss: 1.6951, Val Loss: 2.0729, LR: 0.10000\n",
      "Epoch [10], Train Loss: 1.4644, Val Loss: 1.8494, LR: 0.10000\n",
      "Epoch [11], Train Loss: 1.2823, Val Loss: 1.6767, LR: 0.10000\n",
      "Epoch [12], Train Loss: 1.1272, Val Loss: 1.5215, LR: 0.10000\n",
      "Epoch [13], Train Loss: 1.0030, Val Loss: 1.4031, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.8987, Val Loss: 1.3184, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.8201, Val Loss: 1.2048, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.7367, Val Loss: 1.1408, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.6635, Val Loss: 1.0531, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.6066, Val Loss: 1.0031, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.5512, Val Loss: 0.9451, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.5173, Val Loss: 0.8914, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.4766, Val Loss: 0.8611, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.4451, Val Loss: 0.8279, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.4128, Val Loss: 0.7984, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.3834, Val Loss: 0.7661, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.3588, Val Loss: 0.7370, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.3382, Val Loss: 0.7030, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.3185, Val Loss: 0.6831, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.3017, Val Loss: 0.6691, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.2862, Val Loss: 0.6415, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.2703, Val Loss: 0.6197, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.2599, Val Loss: 0.5933, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.2430, Val Loss: 0.5932, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.2380, Val Loss: 0.5608, LR: 0.10000\n",
      "Epoch [34], Train Loss: 0.2298, Val Loss: 0.5536, LR: 0.10000\n",
      "Epoch [35], Train Loss: 0.2196, Val Loss: 0.5370, LR: 0.10000\n",
      "Epoch [36], Train Loss: 0.2133, Val Loss: 0.5256, LR: 0.10000\n",
      "Epoch [37], Train Loss: 0.2056, Val Loss: 0.5165, LR: 0.10000\n",
      "Epoch [38], Train Loss: 0.1981, Val Loss: 0.5074, LR: 0.10000\n",
      "Epoch [39], Train Loss: 0.1916, Val Loss: 0.4955, LR: 0.10000\n",
      "Epoch [40], Train Loss: 0.1877, Val Loss: 0.4879, LR: 0.10000\n",
      "Epoch [41], Train Loss: 0.1801, Val Loss: 0.4797, LR: 0.10000\n",
      "Epoch [42], Train Loss: 0.1766, Val Loss: 0.4661, LR: 0.10000\n",
      "Epoch [43], Train Loss: 0.1697, Val Loss: 0.4688, LR: 0.10000\n",
      "Epoch [44], Train Loss: 0.1676, Val Loss: 0.4574, LR: 0.10000\n",
      "Epoch [45], Train Loss: 0.1640, Val Loss: 0.4573, LR: 0.10000\n",
      "Epoch [46], Train Loss: 0.1635, Val Loss: 0.4470, LR: 0.10000\n",
      "Epoch [47], Train Loss: 0.1552, Val Loss: 0.4422, LR: 0.10000\n",
      "Epoch [48], Train Loss: 0.1527, Val Loss: 0.4256, LR: 0.10000\n",
      "Epoch [49], Train Loss: 0.1505, Val Loss: 0.4292, LR: 0.10000\n",
      "Epoch [50], Train Loss: 0.1457, Val Loss: 0.4259, LR: 0.10000\n",
      "Epoch [51], Train Loss: 0.1452, Val Loss: 0.4186, LR: 0.10000\n",
      "Epoch [52], Train Loss: 0.1398, Val Loss: 0.4113, LR: 0.10000\n",
      "Epoch [53], Train Loss: 0.1373, Val Loss: 0.4007, LR: 0.10000\n",
      "Epoch [54], Train Loss: 0.1347, Val Loss: 0.3969, LR: 0.10000\n",
      "Epoch [55], Train Loss: 0.1337, Val Loss: 0.4034, LR: 0.10000\n",
      "Epoch [56], Train Loss: 0.1321, Val Loss: 0.4014, LR: 0.10000\n",
      "Epoch [57], Train Loss: 0.1275, Val Loss: 0.3954, LR: 0.10000\n",
      "Epoch [58], Train Loss: 0.1249, Val Loss: 0.3884, LR: 0.10000\n",
      "Epoch [59], Train Loss: 0.1227, Val Loss: 0.3969, LR: 0.10000\n",
      "Epoch [60], Train Loss: 0.1218, Val Loss: 0.3871, LR: 0.10000\n",
      "Epoch [61], Train Loss: 0.1199, Val Loss: 0.3805, LR: 0.10000\n",
      "Epoch [62], Train Loss: 0.1188, Val Loss: 0.3757, LR: 0.10000\n",
      "Epoch [63], Train Loss: 0.1151, Val Loss: 0.3688, LR: 0.10000\n",
      "Epoch [64], Train Loss: 0.1155, Val Loss: 0.3655, LR: 0.10000\n",
      "Epoch [65], Train Loss: 0.1143, Val Loss: 0.3618, LR: 0.10000\n",
      "Epoch [66], Train Loss: 0.1107, Val Loss: 0.3559, LR: 0.10000\n",
      "Epoch [67], Train Loss: 0.1100, Val Loss: 0.3593, LR: 0.10000\n",
      "Epoch [68], Train Loss: 0.1081, Val Loss: 0.3575, LR: 0.10000\n",
      "Epoch [69], Train Loss: 0.1067, Val Loss: 0.3594, LR: 0.10000\n",
      "Epoch [70], Train Loss: 0.1048, Val Loss: 0.3445, LR: 0.10000\n",
      "Epoch [71], Train Loss: 0.1054, Val Loss: 0.3494, LR: 0.10000\n",
      "Epoch [72], Train Loss: 0.1042, Val Loss: 0.3466, LR: 0.10000\n",
      "Epoch [73], Train Loss: 0.1029, Val Loss: 0.3417, LR: 0.10000\n",
      "Epoch [74], Train Loss: 0.1018, Val Loss: 0.3519, LR: 0.10000\n",
      "Epoch [75], Train Loss: 0.1034, Val Loss: 0.3419, LR: 0.10000\n",
      "Epoch [76], Train Loss: 0.0988, Val Loss: 0.3447, LR: 0.10000\n",
      "Epoch [77], Train Loss: 0.0990, Val Loss: 0.3520, LR: 0.10000\n",
      "Epoch [78], Train Loss: 0.0978, Val Loss: 0.3482, LR: 0.10000\n",
      "Epoch [79], Train Loss: 0.0979, Val Loss: 0.3408, LR: 0.10000\n",
      "Epoch [80], Train Loss: 0.0977, Val Loss: 0.3346, LR: 0.10000\n",
      "Epoch [81], Train Loss: 0.0956, Val Loss: 0.3313, LR: 0.10000\n",
      "Epoch [82], Train Loss: 0.0939, Val Loss: 0.3285, LR: 0.10000\n",
      "Epoch [83], Train Loss: 0.0929, Val Loss: 0.3300, LR: 0.10000\n",
      "Epoch [84], Train Loss: 0.0926, Val Loss: 0.3335, LR: 0.10000\n",
      "Epoch [85], Train Loss: 0.0909, Val Loss: 0.3234, LR: 0.10000\n",
      "Epoch [86], Train Loss: 0.0905, Val Loss: 0.3239, LR: 0.10000\n",
      "Epoch [87], Train Loss: 0.0897, Val Loss: 0.3137, LR: 0.10000\n",
      "Epoch [88], Train Loss: 0.0890, Val Loss: 0.3180, LR: 0.10000\n",
      "Epoch [89], Train Loss: 0.0881, Val Loss: 0.3157, LR: 0.10000\n",
      "Epoch [90], Train Loss: 0.0872, Val Loss: 0.3136, LR: 0.10000\n",
      "Epoch [91], Train Loss: 0.0874, Val Loss: 0.3088, LR: 0.10000\n",
      "Epoch [92], Train Loss: 0.0855, Val Loss: 0.3177, LR: 0.10000\n",
      "Epoch [93], Train Loss: 0.0851, Val Loss: 0.3160, LR: 0.10000\n",
      "Epoch [94], Train Loss: 0.0855, Val Loss: 0.3027, LR: 0.10000\n",
      "Epoch [95], Train Loss: 0.0841, Val Loss: 0.3038, LR: 0.10000\n",
      "Epoch [96], Train Loss: 0.0840, Val Loss: 0.3018, LR: 0.10000\n",
      "Epoch [97], Train Loss: 0.0819, Val Loss: 0.3114, LR: 0.10000\n",
      "Epoch [98], Train Loss: 0.0809, Val Loss: 0.3030, LR: 0.10000\n",
      "Epoch [99], Train Loss: 0.0812, Val Loss: 0.3014, LR: 0.10000\n",
      "Epoch [100], Train Loss: 0.0813, Val Loss: 0.3014, LR: 0.10000\n",
      "Epoch [101], Train Loss: 0.0806, Val Loss: 0.2930, LR: 0.10000\n",
      "Epoch [102], Train Loss: 0.0796, Val Loss: 0.2979, LR: 0.10000\n",
      "Epoch [103], Train Loss: 0.0786, Val Loss: 0.2975, LR: 0.10000\n",
      "Epoch [104], Train Loss: 0.0795, Val Loss: 0.3088, LR: 0.10000\n",
      "Epoch [105], Train Loss: 0.0782, Val Loss: 0.2933, LR: 0.10000\n",
      "Epoch [106], Train Loss: 0.0774, Val Loss: 0.2898, LR: 0.10000\n",
      "Epoch [107], Train Loss: 0.0773, Val Loss: 0.2938, LR: 0.10000\n",
      "Epoch [108], Train Loss: 0.0776, Val Loss: 0.2916, LR: 0.10000\n",
      "Epoch [109], Train Loss: 0.0757, Val Loss: 0.2995, LR: 0.10000\n",
      "Epoch [110], Train Loss: 0.0756, Val Loss: 0.2884, LR: 0.10000\n",
      "Epoch [111], Train Loss: 0.0756, Val Loss: 0.2888, LR: 0.10000\n",
      "Epoch [112], Train Loss: 0.0741, Val Loss: 0.2923, LR: 0.10000\n",
      "Epoch [113], Train Loss: 0.0770, Val Loss: 0.2841, LR: 0.10000\n",
      "Epoch [114], Train Loss: 0.0744, Val Loss: 0.2824, LR: 0.10000\n",
      "Epoch [115], Train Loss: 0.0757, Val Loss: 0.2856, LR: 0.10000\n",
      "Epoch [116], Train Loss: 0.0736, Val Loss: 0.2837, LR: 0.10000\n",
      "Epoch [117], Train Loss: 0.0740, Val Loss: 0.2821, LR: 0.10000\n",
      "Epoch [118], Train Loss: 0.0722, Val Loss: 0.2874, LR: 0.10000\n",
      "Epoch [119], Train Loss: 0.0722, Val Loss: 0.2822, LR: 0.10000\n",
      "Epoch [120], Train Loss: 0.0729, Val Loss: 0.2766, LR: 0.10000\n",
      "Epoch [121], Train Loss: 0.0704, Val Loss: 0.2958, LR: 0.10000\n",
      "Epoch [122], Train Loss: 0.0711, Val Loss: 0.2764, LR: 0.10000\n",
      "Epoch [123], Train Loss: 0.0707, Val Loss: 0.2777, LR: 0.10000\n",
      "Epoch [124], Train Loss: 0.0707, Val Loss: 0.2766, LR: 0.10000\n",
      "Epoch [125], Train Loss: 0.0715, Val Loss: 0.2751, LR: 0.10000\n",
      "Epoch [126], Train Loss: 0.0710, Val Loss: 0.2686, LR: 0.10000\n",
      "Epoch [127], Train Loss: 0.0701, Val Loss: 0.2804, LR: 0.10000\n",
      "Epoch [128], Train Loss: 0.0703, Val Loss: 0.2680, LR: 0.10000\n",
      "Epoch [129], Train Loss: 0.0688, Val Loss: 0.2683, LR: 0.10000\n",
      "Epoch [130], Train Loss: 0.0687, Val Loss: 0.2665, LR: 0.10000\n",
      "Epoch [131], Train Loss: 0.0689, Val Loss: 0.2643, LR: 0.10000\n",
      "Epoch [132], Train Loss: 0.0680, Val Loss: 0.2663, LR: 0.10000\n",
      "Epoch [133], Train Loss: 0.0681, Val Loss: 0.2629, LR: 0.10000\n",
      "Epoch [134], Train Loss: 0.0687, Val Loss: 0.2628, LR: 0.10000\n",
      "Epoch [135], Train Loss: 0.0679, Val Loss: 0.2578, LR: 0.10000\n",
      "Epoch [136], Train Loss: 0.0668, Val Loss: 0.2662, LR: 0.10000\n",
      "Epoch [137], Train Loss: 0.0670, Val Loss: 0.2622, LR: 0.10000\n",
      "Epoch [138], Train Loss: 0.0674, Val Loss: 0.2586, LR: 0.10000\n",
      "Epoch [139], Train Loss: 0.0658, Val Loss: 0.2566, LR: 0.10000\n",
      "Epoch [140], Train Loss: 0.0672, Val Loss: 0.2613, LR: 0.10000\n",
      "Epoch [141], Train Loss: 0.0652, Val Loss: 0.2584, LR: 0.10000\n",
      "Epoch [142], Train Loss: 0.0663, Val Loss: 0.2648, LR: 0.10000\n",
      "Epoch [143], Train Loss: 0.0654, Val Loss: 0.2557, LR: 0.10000\n",
      "Epoch [144], Train Loss: 0.0658, Val Loss: 0.2589, LR: 0.10000\n",
      "Epoch [145], Train Loss: 0.0659, Val Loss: 0.2548, LR: 0.10000\n",
      "Epoch [146], Train Loss: 0.0652, Val Loss: 0.2576, LR: 0.10000\n",
      "Epoch [147], Train Loss: 0.0656, Val Loss: 0.2536, LR: 0.10000\n",
      "Epoch [148], Train Loss: 0.0644, Val Loss: 0.2547, LR: 0.10000\n",
      "Epoch [149], Train Loss: 0.0625, Val Loss: 0.2575, LR: 0.10000\n",
      "Epoch [150], Train Loss: 0.0627, Val Loss: 0.2509, LR: 0.10000\n",
      "Epoch [151], Train Loss: 0.0642, Val Loss: 0.2484, LR: 0.10000\n",
      "Epoch [152], Train Loss: 0.0630, Val Loss: 0.2457, LR: 0.10000\n",
      "Epoch [153], Train Loss: 0.0624, Val Loss: 0.2478, LR: 0.10000\n",
      "Epoch [154], Train Loss: 0.0619, Val Loss: 0.2443, LR: 0.10000\n",
      "Epoch [155], Train Loss: 0.0631, Val Loss: 0.2425, LR: 0.10000\n",
      "Epoch [156], Train Loss: 0.0624, Val Loss: 0.2471, LR: 0.10000\n",
      "Epoch [157], Train Loss: 0.0631, Val Loss: 0.2458, LR: 0.10000\n",
      "Epoch [158], Train Loss: 0.0620, Val Loss: 0.2394, LR: 0.10000\n",
      "Epoch [159], Train Loss: 0.0614, Val Loss: 0.2364, LR: 0.10000\n",
      "Epoch [160], Train Loss: 0.0605, Val Loss: 0.2482, LR: 0.10000\n",
      "Epoch [161], Train Loss: 0.0607, Val Loss: 0.2366, LR: 0.10000\n",
      "Epoch [162], Train Loss: 0.0612, Val Loss: 0.2419, LR: 0.10000\n",
      "Epoch [163], Train Loss: 0.0595, Val Loss: 0.2345, LR: 0.10000\n",
      "Epoch [164], Train Loss: 0.0605, Val Loss: 0.2315, LR: 0.10000\n",
      "Epoch [165], Train Loss: 0.0602, Val Loss: 0.2326, LR: 0.10000\n",
      "Epoch [166], Train Loss: 0.0611, Val Loss: 0.2347, LR: 0.10000\n",
      "Epoch [167], Train Loss: 0.0594, Val Loss: 0.2409, LR: 0.10000\n",
      "Epoch [168], Train Loss: 0.0595, Val Loss: 0.2328, LR: 0.10000\n",
      "Epoch [169], Train Loss: 0.0583, Val Loss: 0.2316, LR: 0.10000\n",
      "Epoch [170], Train Loss: 0.0592, Val Loss: 0.2320, LR: 0.10000\n",
      "Epoch [171], Train Loss: 0.0581, Val Loss: 0.2244, LR: 0.10000\n",
      "Epoch [172], Train Loss: 0.0589, Val Loss: 0.2289, LR: 0.10000\n",
      "Epoch [173], Train Loss: 0.0586, Val Loss: 0.2303, LR: 0.10000\n",
      "Epoch [174], Train Loss: 0.0582, Val Loss: 0.2265, LR: 0.10000\n",
      "Epoch [175], Train Loss: 0.0577, Val Loss: 0.2319, LR: 0.10000\n",
      "Epoch [176], Train Loss: 0.0582, Val Loss: 0.2258, LR: 0.10000\n",
      "Epoch [177], Train Loss: 0.0574, Val Loss: 0.2255, LR: 0.10000\n",
      "Epoch [178], Train Loss: 0.0584, Val Loss: 0.2216, LR: 0.10000\n",
      "Epoch [179], Train Loss: 0.0570, Val Loss: 0.2255, LR: 0.10000\n",
      "Epoch [180], Train Loss: 0.0574, Val Loss: 0.2249, LR: 0.10000\n",
      "Epoch [181], Train Loss: 0.0563, Val Loss: 0.2289, LR: 0.10000\n",
      "Epoch [182], Train Loss: 0.0568, Val Loss: 0.2350, LR: 0.10000\n",
      "Epoch [183], Train Loss: 0.0569, Val Loss: 0.2232, LR: 0.10000\n",
      "Epoch [184], Train Loss: 0.0560, Val Loss: 0.2250, LR: 0.10000\n",
      "Epoch [185], Train Loss: 0.0571, Val Loss: 0.2225, LR: 0.10000\n",
      "Epoch [186], Train Loss: 0.0565, Val Loss: 0.2204, LR: 0.10000\n",
      "Epoch [187], Train Loss: 0.0561, Val Loss: 0.2228, LR: 0.10000\n",
      "Epoch [188], Train Loss: 0.0555, Val Loss: 0.2224, LR: 0.10000\n",
      "Epoch [189], Train Loss: 0.0569, Val Loss: 0.2204, LR: 0.10000\n",
      "Epoch [190], Train Loss: 0.0559, Val Loss: 0.2255, LR: 0.10000\n",
      "Epoch [191], Train Loss: 0.0565, Val Loss: 0.2158, LR: 0.10000\n",
      "Epoch [192], Train Loss: 0.0558, Val Loss: 0.2194, LR: 0.10000\n",
      "Epoch [193], Train Loss: 0.0559, Val Loss: 0.2178, LR: 0.10000\n",
      "Epoch [194], Train Loss: 0.0561, Val Loss: 0.2228, LR: 0.10000\n",
      "Epoch [195], Train Loss: 0.0552, Val Loss: 0.2136, LR: 0.10000\n",
      "Epoch [196], Train Loss: 0.0558, Val Loss: 0.2239, LR: 0.10000\n",
      "Epoch [197], Train Loss: 0.0553, Val Loss: 0.2203, LR: 0.10000\n",
      "Epoch [198], Train Loss: 0.0561, Val Loss: 0.2210, LR: 0.10000\n",
      "Epoch [199], Train Loss: 0.0537, Val Loss: 0.2191, LR: 0.10000\n",
      "Epoch [200], Train Loss: 0.0542, Val Loss: 0.2115, LR: 0.10000\n",
      "✅ Run 1: Acc = 74.81%, MDE = 1.1231\n",
      "\n",
      "=== Run 2/5 ===\n",
      "Epoch [1], Train Loss: 35.3187, Val Loss: 16.1994, LR: 0.10000\n",
      "Epoch [2], Train Loss: 10.0992, Val Loss: 7.0032, LR: 0.10000\n",
      "Epoch [3], Train Loss: 4.8797, Val Loss: 4.2920, LR: 0.10000\n",
      "Epoch [4], Train Loss: 3.1212, Val Loss: 3.0856, LR: 0.10000\n",
      "Epoch [5], Train Loss: 2.1943, Val Loss: 2.3753, LR: 0.10000\n",
      "Epoch [6], Train Loss: 1.6386, Val Loss: 1.9116, LR: 0.10000\n",
      "Epoch [7], Train Loss: 1.3061, Val Loss: 1.6085, LR: 0.10000\n",
      "Epoch [8], Train Loss: 1.0805, Val Loss: 1.4065, LR: 0.10000\n",
      "Epoch [9], Train Loss: 0.8957, Val Loss: 1.2137, LR: 0.10000\n",
      "Epoch [10], Train Loss: 0.7661, Val Loss: 1.0823, LR: 0.10000\n",
      "Epoch [11], Train Loss: 0.6635, Val Loss: 0.9778, LR: 0.10000\n",
      "Epoch [12], Train Loss: 0.5696, Val Loss: 0.8801, LR: 0.10000\n",
      "Epoch [13], Train Loss: 0.4977, Val Loss: 0.7845, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.4403, Val Loss: 0.7307, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.3958, Val Loss: 0.6767, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.3570, Val Loss: 0.6400, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.3263, Val Loss: 0.6092, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.3019, Val Loss: 0.5781, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.2770, Val Loss: 0.5570, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.2596, Val Loss: 0.5317, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.2442, Val Loss: 0.5125, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.2269, Val Loss: 0.4788, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.2110, Val Loss: 0.4616, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.2040, Val Loss: 0.4521, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.1932, Val Loss: 0.4412, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.1857, Val Loss: 0.4221, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.1765, Val Loss: 0.4148, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.1698, Val Loss: 0.3990, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.1637, Val Loss: 0.3908, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.1586, Val Loss: 0.3875, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.1548, Val Loss: 0.3803, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.1483, Val Loss: 0.3715, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.1454, Val Loss: 0.3652, LR: 0.10000\n",
      "Epoch [34], Train Loss: 0.1406, Val Loss: 0.3569, LR: 0.10000\n",
      "Epoch [35], Train Loss: 0.1391, Val Loss: 0.3582, LR: 0.10000\n",
      "Epoch [36], Train Loss: 0.1320, Val Loss: 0.3509, LR: 0.10000\n",
      "Epoch [37], Train Loss: 0.1281, Val Loss: 0.3487, LR: 0.10000\n",
      "Epoch [38], Train Loss: 0.1273, Val Loss: 0.3419, LR: 0.10000\n",
      "Epoch [39], Train Loss: 0.1241, Val Loss: 0.3387, LR: 0.10000\n",
      "Epoch [40], Train Loss: 0.1210, Val Loss: 0.3360, LR: 0.10000\n",
      "Epoch [41], Train Loss: 0.1184, Val Loss: 0.3362, LR: 0.10000\n",
      "Epoch [42], Train Loss: 0.1176, Val Loss: 0.3270, LR: 0.10000\n",
      "Epoch [43], Train Loss: 0.1142, Val Loss: 0.3231, LR: 0.10000\n",
      "Epoch [44], Train Loss: 0.1121, Val Loss: 0.3154, LR: 0.10000\n",
      "Epoch [45], Train Loss: 0.1114, Val Loss: 0.3078, LR: 0.10000\n",
      "Epoch [46], Train Loss: 0.1094, Val Loss: 0.3056, LR: 0.10000\n",
      "Epoch [47], Train Loss: 0.1078, Val Loss: 0.3022, LR: 0.10000\n",
      "Epoch [48], Train Loss: 0.1066, Val Loss: 0.3019, LR: 0.10000\n",
      "Epoch [49], Train Loss: 0.1038, Val Loss: 0.3057, LR: 0.10000\n",
      "Epoch [50], Train Loss: 0.1030, Val Loss: 0.2993, LR: 0.10000\n",
      "Epoch [51], Train Loss: 0.1025, Val Loss: 0.2947, LR: 0.10000\n",
      "Epoch [52], Train Loss: 0.0997, Val Loss: 0.2885, LR: 0.10000\n",
      "Epoch [53], Train Loss: 0.0994, Val Loss: 0.2921, LR: 0.10000\n",
      "Epoch [54], Train Loss: 0.0975, Val Loss: 0.2880, LR: 0.10000\n",
      "Epoch [55], Train Loss: 0.0965, Val Loss: 0.2790, LR: 0.10000\n",
      "Epoch [56], Train Loss: 0.0943, Val Loss: 0.2899, LR: 0.10000\n",
      "Epoch [57], Train Loss: 0.0952, Val Loss: 0.2782, LR: 0.10000\n",
      "Epoch [58], Train Loss: 0.0926, Val Loss: 0.2775, LR: 0.10000\n",
      "Epoch [59], Train Loss: 0.0924, Val Loss: 0.2759, LR: 0.10000\n",
      "Epoch [60], Train Loss: 0.0913, Val Loss: 0.2728, LR: 0.10000\n",
      "Epoch [61], Train Loss: 0.0906, Val Loss: 0.2713, LR: 0.10000\n",
      "Epoch [62], Train Loss: 0.0902, Val Loss: 0.2735, LR: 0.10000\n",
      "Epoch [63], Train Loss: 0.0876, Val Loss: 0.2696, LR: 0.10000\n",
      "Epoch [64], Train Loss: 0.0879, Val Loss: 0.2688, LR: 0.10000\n",
      "Epoch [65], Train Loss: 0.0875, Val Loss: 0.2614, LR: 0.10000\n",
      "Epoch [66], Train Loss: 0.0853, Val Loss: 0.2639, LR: 0.10000\n",
      "Epoch [67], Train Loss: 0.0871, Val Loss: 0.2546, LR: 0.10000\n",
      "Epoch [68], Train Loss: 0.0838, Val Loss: 0.2532, LR: 0.10000\n",
      "Epoch [69], Train Loss: 0.0843, Val Loss: 0.2544, LR: 0.10000\n",
      "Epoch [70], Train Loss: 0.0824, Val Loss: 0.2495, LR: 0.10000\n",
      "Epoch [71], Train Loss: 0.0833, Val Loss: 0.2503, LR: 0.10000\n",
      "Epoch [72], Train Loss: 0.0817, Val Loss: 0.2445, LR: 0.10000\n",
      "Epoch [73], Train Loss: 0.0812, Val Loss: 0.2464, LR: 0.10000\n",
      "Epoch [74], Train Loss: 0.0799, Val Loss: 0.2479, LR: 0.10000\n",
      "Epoch [75], Train Loss: 0.0796, Val Loss: 0.2560, LR: 0.10000\n",
      "Epoch [76], Train Loss: 0.0803, Val Loss: 0.2422, LR: 0.10000\n",
      "Epoch [77], Train Loss: 0.0775, Val Loss: 0.2448, LR: 0.10000\n",
      "Epoch [78], Train Loss: 0.0774, Val Loss: 0.2402, LR: 0.10000\n",
      "Epoch [79], Train Loss: 0.0769, Val Loss: 0.2332, LR: 0.10000\n",
      "Epoch [80], Train Loss: 0.0771, Val Loss: 0.2469, LR: 0.10000\n",
      "Epoch [81], Train Loss: 0.0754, Val Loss: 0.2373, LR: 0.10000\n",
      "Epoch [82], Train Loss: 0.0791, Val Loss: 0.2435, LR: 0.10000\n",
      "Epoch [83], Train Loss: 0.0753, Val Loss: 0.2360, LR: 0.10000\n",
      "Epoch [84], Train Loss: 0.0748, Val Loss: 0.2350, LR: 0.10000\n",
      "Epoch [85], Train Loss: 0.0772, Val Loss: 0.2283, LR: 0.10000\n",
      "Epoch [86], Train Loss: 0.0724, Val Loss: 0.2304, LR: 0.10000\n",
      "Epoch [87], Train Loss: 0.0739, Val Loss: 0.2320, LR: 0.10000\n",
      "Epoch [88], Train Loss: 0.0735, Val Loss: 0.2348, LR: 0.10000\n",
      "Epoch [89], Train Loss: 0.0731, Val Loss: 0.2253, LR: 0.10000\n",
      "Epoch [90], Train Loss: 0.0710, Val Loss: 0.2271, LR: 0.10000\n",
      "Epoch [91], Train Loss: 0.0716, Val Loss: 0.2252, LR: 0.10000\n",
      "Epoch [92], Train Loss: 0.0708, Val Loss: 0.2247, LR: 0.10000\n",
      "Epoch [93], Train Loss: 0.0709, Val Loss: 0.2237, LR: 0.10000\n",
      "Epoch [94], Train Loss: 0.0711, Val Loss: 0.2245, LR: 0.10000\n",
      "Epoch [95], Train Loss: 0.0712, Val Loss: 0.2239, LR: 0.10000\n",
      "Epoch [96], Train Loss: 0.0703, Val Loss: 0.2187, LR: 0.10000\n",
      "Epoch [97], Train Loss: 0.0700, Val Loss: 0.2263, LR: 0.10000\n",
      "Epoch [98], Train Loss: 0.0714, Val Loss: 0.2242, LR: 0.10000\n",
      "Epoch [99], Train Loss: 0.0684, Val Loss: 0.2237, LR: 0.10000\n",
      "Epoch [100], Train Loss: 0.0683, Val Loss: 0.2217, LR: 0.10000\n",
      "Epoch [101], Train Loss: 0.0679, Val Loss: 0.2248, LR: 0.10000\n",
      "Epoch [102], Train Loss: 0.0675, Val Loss: 0.2226, LR: 0.10000\n",
      "Epoch [103], Train Loss: 0.0664, Val Loss: 0.2198, LR: 0.10000\n",
      "Epoch [104], Train Loss: 0.0672, Val Loss: 0.2203, LR: 0.10000\n",
      "Epoch [105], Train Loss: 0.0674, Val Loss: 0.2139, LR: 0.10000\n",
      "Epoch [106], Train Loss: 0.0655, Val Loss: 0.2238, LR: 0.10000\n",
      "Epoch [107], Train Loss: 0.0668, Val Loss: 0.2233, LR: 0.10000\n",
      "Epoch [108], Train Loss: 0.0655, Val Loss: 0.2255, LR: 0.10000\n",
      "Epoch [109], Train Loss: 0.0658, Val Loss: 0.2266, LR: 0.10000\n",
      "Epoch [110], Train Loss: 0.0658, Val Loss: 0.2183, LR: 0.10000\n",
      "Epoch [111], Train Loss: 0.0654, Val Loss: 0.2144, LR: 0.10000\n",
      "Epoch [112], Train Loss: 0.0639, Val Loss: 0.2213, LR: 0.10000\n",
      "Epoch [113], Train Loss: 0.0649, Val Loss: 0.2203, LR: 0.10000\n",
      "Epoch [114], Train Loss: 0.0641, Val Loss: 0.2221, LR: 0.10000\n",
      "Epoch [115], Train Loss: 0.0642, Val Loss: 0.2205, LR: 0.10000\n",
      "Epoch [116], Train Loss: 0.0635, Val Loss: 0.2175, LR: 0.10000\n",
      "Epoch [117], Train Loss: 0.0634, Val Loss: 0.2287, LR: 0.10000\n",
      "Epoch [118], Train Loss: 0.0630, Val Loss: 0.2187, LR: 0.10000\n",
      "Epoch [119], Train Loss: 0.0632, Val Loss: 0.2179, LR: 0.10000\n",
      "Epoch [120], Train Loss: 0.0622, Val Loss: 0.2236, LR: 0.10000\n",
      "Epoch [121], Train Loss: 0.0624, Val Loss: 0.2230, LR: 0.05000\n",
      "Epoch [122], Train Loss: 0.0584, Val Loss: 0.2187, LR: 0.05000\n",
      "Epoch [123], Train Loss: 0.0571, Val Loss: 0.2181, LR: 0.05000\n",
      "Epoch [124], Train Loss: 0.0577, Val Loss: 0.2161, LR: 0.05000\n",
      "Epoch [125], Train Loss: 0.0580, Val Loss: 0.2173, LR: 0.05000\n",
      "Early stopping at epoch 125\n",
      "✅ Run 2: Acc = 73.79%, MDE = 1.1288\n",
      "\n",
      "=== Run 3/5 ===\n",
      "Epoch [1], Train Loss: 36.7949, Val Loss: 17.4521, LR: 0.10000\n",
      "Epoch [2], Train Loss: 11.3196, Val Loss: 8.3940, LR: 0.10000\n",
      "Epoch [3], Train Loss: 6.1003, Val Loss: 5.5049, LR: 0.10000\n",
      "Epoch [4], Train Loss: 4.1547, Val Loss: 4.1114, LR: 0.10000\n",
      "Epoch [5], Train Loss: 3.0573, Val Loss: 3.3289, LR: 0.10000\n",
      "Epoch [6], Train Loss: 2.4178, Val Loss: 2.8135, LR: 0.10000\n",
      "Epoch [7], Train Loss: 1.9673, Val Loss: 2.3850, LR: 0.10000\n",
      "Epoch [8], Train Loss: 1.6431, Val Loss: 2.1413, LR: 0.10000\n",
      "Epoch [9], Train Loss: 1.4174, Val Loss: 1.9219, LR: 0.10000\n",
      "Epoch [10], Train Loss: 1.2421, Val Loss: 1.7463, LR: 0.10000\n",
      "Epoch [11], Train Loss: 1.0949, Val Loss: 1.6050, LR: 0.10000\n",
      "Epoch [12], Train Loss: 0.9749, Val Loss: 1.4771, LR: 0.10000\n",
      "Epoch [13], Train Loss: 0.8729, Val Loss: 1.4034, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.7996, Val Loss: 1.3062, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.7203, Val Loss: 1.2451, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.6639, Val Loss: 1.1723, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.6191, Val Loss: 1.1195, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.5684, Val Loss: 1.0570, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.5202, Val Loss: 1.0164, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.4786, Val Loss: 0.9675, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.4524, Val Loss: 0.9286, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.4195, Val Loss: 0.9130, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.3888, Val Loss: 0.8747, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.3634, Val Loss: 0.8344, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.3400, Val Loss: 0.8097, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.3202, Val Loss: 0.7920, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.3030, Val Loss: 0.7668, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.2865, Val Loss: 0.7461, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.2756, Val Loss: 0.7278, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.2629, Val Loss: 0.7142, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.2507, Val Loss: 0.7055, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.2477, Val Loss: 0.6895, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.2326, Val Loss: 0.6897, LR: 0.10000\n",
      "Epoch [34], Train Loss: 0.2292, Val Loss: 0.6688, LR: 0.10000\n",
      "Epoch [35], Train Loss: 0.2194, Val Loss: 0.6583, LR: 0.10000\n",
      "Epoch [36], Train Loss: 0.2091, Val Loss: 0.6465, LR: 0.10000\n",
      "Epoch [37], Train Loss: 0.2068, Val Loss: 0.6288, LR: 0.10000\n",
      "Epoch [38], Train Loss: 0.1980, Val Loss: 0.6281, LR: 0.10000\n",
      "Epoch [39], Train Loss: 0.1891, Val Loss: 0.6157, LR: 0.10000\n",
      "Epoch [40], Train Loss: 0.1867, Val Loss: 0.6103, LR: 0.10000\n",
      "Epoch [41], Train Loss: 0.1807, Val Loss: 0.5980, LR: 0.10000\n",
      "Epoch [42], Train Loss: 0.1753, Val Loss: 0.5869, LR: 0.10000\n",
      "Epoch [43], Train Loss: 0.1722, Val Loss: 0.5844, LR: 0.10000\n",
      "Epoch [44], Train Loss: 0.1665, Val Loss: 0.5738, LR: 0.10000\n",
      "Epoch [45], Train Loss: 0.1623, Val Loss: 0.5625, LR: 0.10000\n",
      "Epoch [46], Train Loss: 0.1599, Val Loss: 0.5457, LR: 0.10000\n",
      "Epoch [47], Train Loss: 0.1546, Val Loss: 0.5462, LR: 0.10000\n",
      "Epoch [48], Train Loss: 0.1578, Val Loss: 0.5404, LR: 0.10000\n",
      "Epoch [49], Train Loss: 0.1517, Val Loss: 0.5334, LR: 0.10000\n",
      "Epoch [50], Train Loss: 0.1489, Val Loss: 0.5344, LR: 0.10000\n",
      "Epoch [51], Train Loss: 0.1468, Val Loss: 0.5242, LR: 0.10000\n",
      "Epoch [52], Train Loss: 0.1392, Val Loss: 0.5253, LR: 0.10000\n",
      "Epoch [53], Train Loss: 0.1399, Val Loss: 0.5182, LR: 0.10000\n",
      "Epoch [54], Train Loss: 0.1381, Val Loss: 0.5066, LR: 0.10000\n",
      "Epoch [55], Train Loss: 0.1350, Val Loss: 0.5066, LR: 0.10000\n",
      "Epoch [56], Train Loss: 0.1324, Val Loss: 0.5083, LR: 0.10000\n",
      "Epoch [57], Train Loss: 0.1318, Val Loss: 0.4935, LR: 0.10000\n",
      "Epoch [58], Train Loss: 0.1290, Val Loss: 0.5035, LR: 0.10000\n",
      "Epoch [59], Train Loss: 0.1281, Val Loss: 0.4957, LR: 0.10000\n",
      "Epoch [60], Train Loss: 0.1262, Val Loss: 0.4854, LR: 0.10000\n",
      "Epoch [61], Train Loss: 0.1238, Val Loss: 0.4891, LR: 0.10000\n",
      "Epoch [62], Train Loss: 0.1215, Val Loss: 0.5002, LR: 0.10000\n",
      "Epoch [63], Train Loss: 0.1209, Val Loss: 0.4809, LR: 0.10000\n",
      "Epoch [64], Train Loss: 0.1196, Val Loss: 0.4798, LR: 0.10000\n",
      "Epoch [65], Train Loss: 0.1181, Val Loss: 0.4760, LR: 0.10000\n",
      "Epoch [66], Train Loss: 0.1158, Val Loss: 0.4688, LR: 0.10000\n",
      "Epoch [67], Train Loss: 0.1185, Val Loss: 0.4715, LR: 0.10000\n",
      "Epoch [68], Train Loss: 0.1136, Val Loss: 0.4687, LR: 0.10000\n",
      "Epoch [69], Train Loss: 0.1138, Val Loss: 0.4622, LR: 0.10000\n",
      "Epoch [70], Train Loss: 0.1127, Val Loss: 0.4676, LR: 0.10000\n",
      "Epoch [71], Train Loss: 0.1102, Val Loss: 0.4652, LR: 0.10000\n",
      "Epoch [72], Train Loss: 0.1089, Val Loss: 0.4553, LR: 0.10000\n",
      "Epoch [73], Train Loss: 0.1094, Val Loss: 0.4569, LR: 0.10000\n",
      "Epoch [74], Train Loss: 0.1081, Val Loss: 0.4583, LR: 0.10000\n",
      "Epoch [75], Train Loss: 0.1085, Val Loss: 0.4628, LR: 0.10000\n",
      "Epoch [76], Train Loss: 0.1047, Val Loss: 0.4529, LR: 0.10000\n",
      "Epoch [77], Train Loss: 0.1075, Val Loss: 0.4519, LR: 0.10000\n",
      "Epoch [78], Train Loss: 0.1059, Val Loss: 0.4448, LR: 0.10000\n",
      "Epoch [79], Train Loss: 0.1030, Val Loss: 0.4414, LR: 0.10000\n",
      "Epoch [80], Train Loss: 0.1033, Val Loss: 0.4501, LR: 0.10000\n",
      "Epoch [81], Train Loss: 0.1024, Val Loss: 0.4410, LR: 0.10000\n",
      "Epoch [82], Train Loss: 0.1010, Val Loss: 0.4387, LR: 0.10000\n",
      "Epoch [83], Train Loss: 0.1010, Val Loss: 0.4388, LR: 0.10000\n",
      "Epoch [84], Train Loss: 0.1003, Val Loss: 0.4362, LR: 0.10000\n",
      "Epoch [85], Train Loss: 0.0998, Val Loss: 0.4350, LR: 0.10000\n",
      "Epoch [86], Train Loss: 0.1004, Val Loss: 0.4348, LR: 0.10000\n",
      "Epoch [87], Train Loss: 0.0991, Val Loss: 0.4396, LR: 0.10000\n",
      "Epoch [88], Train Loss: 0.0968, Val Loss: 0.4340, LR: 0.10000\n",
      "Epoch [89], Train Loss: 0.0973, Val Loss: 0.4350, LR: 0.10000\n",
      "Epoch [90], Train Loss: 0.0963, Val Loss: 0.4455, LR: 0.10000\n",
      "Epoch [91], Train Loss: 0.0965, Val Loss: 0.4185, LR: 0.10000\n",
      "Epoch [92], Train Loss: 0.0944, Val Loss: 0.4250, LR: 0.10000\n",
      "Epoch [93], Train Loss: 0.0947, Val Loss: 0.4204, LR: 0.10000\n",
      "Epoch [94], Train Loss: 0.0948, Val Loss: 0.4154, LR: 0.10000\n",
      "Epoch [95], Train Loss: 0.0930, Val Loss: 0.4194, LR: 0.10000\n",
      "Epoch [96], Train Loss: 0.0923, Val Loss: 0.4185, LR: 0.10000\n",
      "Epoch [97], Train Loss: 0.0922, Val Loss: 0.4195, LR: 0.10000\n",
      "Epoch [98], Train Loss: 0.0916, Val Loss: 0.4127, LR: 0.10000\n",
      "Epoch [99], Train Loss: 0.0908, Val Loss: 0.4109, LR: 0.10000\n",
      "Epoch [100], Train Loss: 0.0902, Val Loss: 0.4134, LR: 0.10000\n",
      "Epoch [101], Train Loss: 0.0912, Val Loss: 0.4037, LR: 0.10000\n",
      "Epoch [102], Train Loss: 0.0912, Val Loss: 0.4097, LR: 0.10000\n",
      "Epoch [103], Train Loss: 0.0908, Val Loss: 0.4086, LR: 0.10000\n",
      "Epoch [104], Train Loss: 0.0883, Val Loss: 0.4080, LR: 0.10000\n",
      "Epoch [105], Train Loss: 0.0890, Val Loss: 0.4112, LR: 0.10000\n",
      "Epoch [106], Train Loss: 0.0872, Val Loss: 0.4059, LR: 0.10000\n",
      "Epoch [107], Train Loss: 0.0900, Val Loss: 0.4071, LR: 0.10000\n",
      "Epoch [108], Train Loss: 0.0875, Val Loss: 0.4040, LR: 0.10000\n",
      "Epoch [109], Train Loss: 0.0868, Val Loss: 0.4134, LR: 0.10000\n",
      "Epoch [110], Train Loss: 0.0870, Val Loss: 0.3974, LR: 0.10000\n",
      "Epoch [111], Train Loss: 0.0863, Val Loss: 0.4059, LR: 0.10000\n",
      "Epoch [112], Train Loss: 0.0900, Val Loss: 0.4017, LR: 0.10000\n",
      "Epoch [113], Train Loss: 0.0855, Val Loss: 0.4001, LR: 0.10000\n",
      "Epoch [114], Train Loss: 0.0844, Val Loss: 0.3998, LR: 0.10000\n",
      "Epoch [115], Train Loss: 0.0841, Val Loss: 0.3925, LR: 0.10000\n",
      "Epoch [116], Train Loss: 0.0842, Val Loss: 0.3908, LR: 0.10000\n",
      "Epoch [117], Train Loss: 0.0849, Val Loss: 0.3851, LR: 0.10000\n",
      "Epoch [118], Train Loss: 0.0841, Val Loss: 0.3876, LR: 0.10000\n",
      "Epoch [119], Train Loss: 0.0830, Val Loss: 0.3792, LR: 0.10000\n",
      "Epoch [120], Train Loss: 0.0829, Val Loss: 0.3774, LR: 0.10000\n",
      "Epoch [121], Train Loss: 0.0828, Val Loss: 0.3801, LR: 0.10000\n",
      "Epoch [122], Train Loss: 0.0817, Val Loss: 0.3765, LR: 0.10000\n",
      "Epoch [123], Train Loss: 0.0819, Val Loss: 0.3772, LR: 0.10000\n",
      "Epoch [124], Train Loss: 0.0819, Val Loss: 0.3731, LR: 0.10000\n",
      "Epoch [125], Train Loss: 0.0819, Val Loss: 0.3721, LR: 0.10000\n",
      "Epoch [126], Train Loss: 0.0799, Val Loss: 0.3695, LR: 0.10000\n",
      "Epoch [127], Train Loss: 0.0808, Val Loss: 0.3667, LR: 0.10000\n",
      "Epoch [128], Train Loss: 0.0807, Val Loss: 0.3718, LR: 0.10000\n",
      "Epoch [129], Train Loss: 0.0801, Val Loss: 0.3657, LR: 0.10000\n",
      "Epoch [130], Train Loss: 0.0795, Val Loss: 0.3639, LR: 0.10000\n",
      "Epoch [131], Train Loss: 0.0798, Val Loss: 0.3637, LR: 0.10000\n",
      "Epoch [132], Train Loss: 0.0805, Val Loss: 0.3647, LR: 0.10000\n",
      "Epoch [133], Train Loss: 0.0791, Val Loss: 0.3799, LR: 0.10000\n",
      "Epoch [134], Train Loss: 0.0795, Val Loss: 0.3596, LR: 0.10000\n",
      "Epoch [135], Train Loss: 0.0793, Val Loss: 0.3663, LR: 0.10000\n",
      "Epoch [136], Train Loss: 0.0772, Val Loss: 0.3612, LR: 0.10000\n",
      "Epoch [137], Train Loss: 0.0776, Val Loss: 0.3581, LR: 0.10000\n",
      "Epoch [138], Train Loss: 0.0780, Val Loss: 0.3638, LR: 0.10000\n",
      "Epoch [139], Train Loss: 0.0771, Val Loss: 0.3582, LR: 0.10000\n",
      "Epoch [140], Train Loss: 0.0772, Val Loss: 0.3570, LR: 0.10000\n",
      "Epoch [141], Train Loss: 0.0769, Val Loss: 0.3622, LR: 0.10000\n",
      "Epoch [142], Train Loss: 0.0778, Val Loss: 0.3558, LR: 0.10000\n",
      "Epoch [143], Train Loss: 0.0755, Val Loss: 0.3505, LR: 0.10000\n",
      "Epoch [144], Train Loss: 0.0764, Val Loss: 0.3629, LR: 0.10000\n",
      "Epoch [145], Train Loss: 0.0756, Val Loss: 0.3563, LR: 0.10000\n",
      "Epoch [146], Train Loss: 0.0756, Val Loss: 0.3509, LR: 0.10000\n",
      "Epoch [147], Train Loss: 0.0735, Val Loss: 0.3479, LR: 0.10000\n",
      "Epoch [148], Train Loss: 0.0770, Val Loss: 0.3541, LR: 0.10000\n",
      "Epoch [149], Train Loss: 0.0746, Val Loss: 0.3531, LR: 0.10000\n",
      "Epoch [150], Train Loss: 0.0750, Val Loss: 0.3586, LR: 0.10000\n",
      "Epoch [151], Train Loss: 0.0748, Val Loss: 0.3455, LR: 0.10000\n",
      "Epoch [152], Train Loss: 0.0746, Val Loss: 0.3578, LR: 0.10000\n",
      "Epoch [153], Train Loss: 0.0757, Val Loss: 0.3493, LR: 0.10000\n",
      "Epoch [154], Train Loss: 0.0734, Val Loss: 0.3504, LR: 0.10000\n",
      "Epoch [155], Train Loss: 0.0738, Val Loss: 0.3576, LR: 0.10000\n",
      "Epoch [156], Train Loss: 0.0743, Val Loss: 0.3576, LR: 0.10000\n",
      "Epoch [157], Train Loss: 0.0737, Val Loss: 0.3412, LR: 0.10000\n",
      "Epoch [158], Train Loss: 0.0731, Val Loss: 0.3480, LR: 0.10000\n",
      "Epoch [159], Train Loss: 0.0719, Val Loss: 0.3396, LR: 0.10000\n",
      "Epoch [160], Train Loss: 0.0736, Val Loss: 0.3407, LR: 0.10000\n",
      "Epoch [161], Train Loss: 0.0727, Val Loss: 0.3438, LR: 0.10000\n",
      "Epoch [162], Train Loss: 0.0734, Val Loss: 0.3432, LR: 0.10000\n",
      "Epoch [163], Train Loss: 0.0735, Val Loss: 0.3383, LR: 0.10000\n",
      "Epoch [164], Train Loss: 0.0723, Val Loss: 0.3395, LR: 0.10000\n",
      "Epoch [165], Train Loss: 0.0719, Val Loss: 0.3442, LR: 0.10000\n",
      "Epoch [166], Train Loss: 0.0720, Val Loss: 0.3324, LR: 0.10000\n",
      "Epoch [167], Train Loss: 0.0709, Val Loss: 0.3395, LR: 0.10000\n",
      "Epoch [168], Train Loss: 0.0718, Val Loss: 0.3296, LR: 0.10000\n",
      "Epoch [169], Train Loss: 0.0701, Val Loss: 0.3364, LR: 0.10000\n",
      "Epoch [170], Train Loss: 0.0715, Val Loss: 0.3448, LR: 0.10000\n",
      "Epoch [171], Train Loss: 0.0715, Val Loss: 0.3319, LR: 0.10000\n",
      "Epoch [172], Train Loss: 0.0697, Val Loss: 0.3432, LR: 0.10000\n",
      "Epoch [173], Train Loss: 0.0709, Val Loss: 0.3326, LR: 0.10000\n",
      "Epoch [174], Train Loss: 0.0698, Val Loss: 0.3315, LR: 0.10000\n",
      "Epoch [175], Train Loss: 0.0700, Val Loss: 0.3323, LR: 0.10000\n",
      "Epoch [176], Train Loss: 0.0690, Val Loss: 0.3279, LR: 0.10000\n",
      "Epoch [177], Train Loss: 0.0710, Val Loss: 0.3286, LR: 0.10000\n",
      "Epoch [178], Train Loss: 0.0695, Val Loss: 0.3277, LR: 0.10000\n",
      "Epoch [179], Train Loss: 0.0696, Val Loss: 0.3295, LR: 0.10000\n",
      "Epoch [180], Train Loss: 0.0706, Val Loss: 0.3326, LR: 0.10000\n",
      "Epoch [181], Train Loss: 0.0712, Val Loss: 0.3255, LR: 0.10000\n",
      "Epoch [182], Train Loss: 0.0683, Val Loss: 0.3356, LR: 0.10000\n",
      "Epoch [183], Train Loss: 0.0692, Val Loss: 0.3252, LR: 0.10000\n",
      "Epoch [184], Train Loss: 0.0703, Val Loss: 0.3319, LR: 0.10000\n",
      "Epoch [185], Train Loss: 0.0682, Val Loss: 0.3252, LR: 0.10000\n",
      "Epoch [186], Train Loss: 0.0678, Val Loss: 0.3306, LR: 0.10000\n",
      "Epoch [187], Train Loss: 0.0675, Val Loss: 0.3318, LR: 0.10000\n",
      "Epoch [188], Train Loss: 0.0687, Val Loss: 0.3224, LR: 0.10000\n",
      "Epoch [189], Train Loss: 0.0683, Val Loss: 0.3239, LR: 0.10000\n",
      "Epoch [190], Train Loss: 0.0684, Val Loss: 0.3169, LR: 0.10000\n",
      "Epoch [191], Train Loss: 0.0680, Val Loss: 0.3189, LR: 0.10000\n",
      "Epoch [192], Train Loss: 0.0678, Val Loss: 0.3165, LR: 0.10000\n",
      "Epoch [193], Train Loss: 0.0673, Val Loss: 0.3126, LR: 0.10000\n",
      "Epoch [194], Train Loss: 0.0682, Val Loss: 0.3157, LR: 0.10000\n",
      "Epoch [195], Train Loss: 0.0675, Val Loss: 0.3077, LR: 0.10000\n",
      "Epoch [196], Train Loss: 0.0671, Val Loss: 0.3190, LR: 0.10000\n",
      "Epoch [197], Train Loss: 0.0679, Val Loss: 0.3103, LR: 0.10000\n",
      "Epoch [198], Train Loss: 0.0665, Val Loss: 0.3106, LR: 0.10000\n",
      "Epoch [199], Train Loss: 0.0658, Val Loss: 0.3118, LR: 0.10000\n",
      "Epoch [200], Train Loss: 0.0664, Val Loss: 0.3091, LR: 0.10000\n",
      "✅ Run 3: Acc = 75.22%, MDE = 1.1005\n",
      "\n",
      "=== Run 4/5 ===\n",
      "Epoch [1], Train Loss: 32.1118, Val Loss: 16.3801, LR: 0.10000\n",
      "Epoch [2], Train Loss: 11.4944, Val Loss: 8.0205, LR: 0.10000\n",
      "Epoch [3], Train Loss: 5.9024, Val Loss: 5.3787, LR: 0.10000\n",
      "Epoch [4], Train Loss: 3.7313, Val Loss: 3.9165, LR: 0.10000\n",
      "Epoch [5], Train Loss: 2.7078, Val Loss: 3.0472, LR: 0.10000\n",
      "Epoch [6], Train Loss: 2.0552, Val Loss: 2.4927, LR: 0.10000\n",
      "Epoch [7], Train Loss: 1.5917, Val Loss: 2.1493, LR: 0.10000\n",
      "Epoch [8], Train Loss: 1.3250, Val Loss: 1.9189, LR: 0.10000\n",
      "Epoch [9], Train Loss: 1.1362, Val Loss: 1.7244, LR: 0.10000\n",
      "Epoch [10], Train Loss: 0.9910, Val Loss: 1.5451, LR: 0.10000\n",
      "Epoch [11], Train Loss: 0.8586, Val Loss: 1.3603, LR: 0.10000\n",
      "Epoch [12], Train Loss: 0.7306, Val Loss: 1.2347, LR: 0.10000\n",
      "Epoch [13], Train Loss: 0.6207, Val Loss: 1.1339, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.5436, Val Loss: 1.0421, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.4856, Val Loss: 1.0018, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.4391, Val Loss: 0.9498, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.4002, Val Loss: 0.9070, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.3699, Val Loss: 0.8722, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.3360, Val Loss: 0.8223, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.3079, Val Loss: 0.7920, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.2855, Val Loss: 0.7727, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.2637, Val Loss: 0.7481, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.2498, Val Loss: 0.7224, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.2343, Val Loss: 0.7133, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.2249, Val Loss: 0.6973, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.2116, Val Loss: 0.6703, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.2024, Val Loss: 0.6594, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.1952, Val Loss: 0.6394, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.1856, Val Loss: 0.6313, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.1780, Val Loss: 0.6177, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.1727, Val Loss: 0.6101, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.1660, Val Loss: 0.5868, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.1606, Val Loss: 0.5936, LR: 0.10000\n",
      "Epoch [34], Train Loss: 0.1561, Val Loss: 0.5816, LR: 0.10000\n",
      "Epoch [35], Train Loss: 0.1487, Val Loss: 0.5670, LR: 0.10000\n",
      "Epoch [36], Train Loss: 0.1470, Val Loss: 0.5636, LR: 0.10000\n",
      "Epoch [37], Train Loss: 0.1423, Val Loss: 0.5480, LR: 0.10000\n",
      "Epoch [38], Train Loss: 0.1389, Val Loss: 0.5332, LR: 0.10000\n",
      "Epoch [39], Train Loss: 0.1349, Val Loss: 0.5368, LR: 0.10000\n",
      "Epoch [40], Train Loss: 0.1326, Val Loss: 0.5250, LR: 0.10000\n",
      "Epoch [41], Train Loss: 0.1302, Val Loss: 0.5120, LR: 0.10000\n",
      "Epoch [42], Train Loss: 0.1272, Val Loss: 0.5182, LR: 0.10000\n",
      "Epoch [43], Train Loss: 0.1226, Val Loss: 0.5142, LR: 0.10000\n",
      "Epoch [44], Train Loss: 0.1223, Val Loss: 0.5052, LR: 0.10000\n",
      "Epoch [45], Train Loss: 0.1186, Val Loss: 0.5055, LR: 0.10000\n",
      "Epoch [46], Train Loss: 0.1189, Val Loss: 0.5080, LR: 0.10000\n",
      "Epoch [47], Train Loss: 0.1150, Val Loss: 0.5022, LR: 0.10000\n",
      "Epoch [48], Train Loss: 0.1136, Val Loss: 0.4938, LR: 0.10000\n",
      "Epoch [49], Train Loss: 0.1125, Val Loss: 0.4928, LR: 0.10000\n",
      "Epoch [50], Train Loss: 0.1113, Val Loss: 0.4859, LR: 0.10000\n",
      "Epoch [51], Train Loss: 0.1085, Val Loss: 0.4967, LR: 0.10000\n",
      "Epoch [52], Train Loss: 0.1083, Val Loss: 0.4815, LR: 0.10000\n",
      "Epoch [53], Train Loss: 0.1066, Val Loss: 0.4738, LR: 0.10000\n",
      "Epoch [54], Train Loss: 0.1050, Val Loss: 0.4766, LR: 0.10000\n",
      "Epoch [55], Train Loss: 0.1027, Val Loss: 0.4775, LR: 0.10000\n",
      "Epoch [56], Train Loss: 0.1025, Val Loss: 0.4694, LR: 0.10000\n",
      "Epoch [57], Train Loss: 0.1008, Val Loss: 0.4755, LR: 0.10000\n",
      "Epoch [58], Train Loss: 0.0999, Val Loss: 0.4688, LR: 0.10000\n",
      "Epoch [59], Train Loss: 0.0987, Val Loss: 0.4654, LR: 0.10000\n",
      "Epoch [60], Train Loss: 0.0988, Val Loss: 0.4652, LR: 0.10000\n",
      "Epoch [61], Train Loss: 0.0964, Val Loss: 0.4604, LR: 0.10000\n",
      "Epoch [62], Train Loss: 0.0966, Val Loss: 0.4617, LR: 0.10000\n",
      "Epoch [63], Train Loss: 0.0959, Val Loss: 0.4778, LR: 0.10000\n",
      "Epoch [64], Train Loss: 0.0934, Val Loss: 0.4604, LR: 0.10000\n",
      "Epoch [65], Train Loss: 0.0941, Val Loss: 0.4610, LR: 0.10000\n",
      "Epoch [66], Train Loss: 0.0927, Val Loss: 0.4675, LR: 0.10000\n",
      "Epoch [67], Train Loss: 0.0915, Val Loss: 0.4575, LR: 0.10000\n",
      "Epoch [68], Train Loss: 0.0898, Val Loss: 0.4663, LR: 0.10000\n",
      "Epoch [69], Train Loss: 0.0904, Val Loss: 0.4557, LR: 0.10000\n",
      "Epoch [70], Train Loss: 0.0902, Val Loss: 0.4535, LR: 0.10000\n",
      "Epoch [71], Train Loss: 0.0884, Val Loss: 0.4483, LR: 0.10000\n",
      "Epoch [72], Train Loss: 0.0878, Val Loss: 0.4845, LR: 0.10000\n",
      "Epoch [73], Train Loss: 0.0879, Val Loss: 0.4516, LR: 0.10000\n",
      "Epoch [74], Train Loss: 0.0868, Val Loss: 0.4464, LR: 0.10000\n",
      "Epoch [75], Train Loss: 0.0856, Val Loss: 0.4438, LR: 0.10000\n",
      "Epoch [76], Train Loss: 0.0859, Val Loss: 0.4486, LR: 0.10000\n",
      "Epoch [77], Train Loss: 0.0850, Val Loss: 0.4361, LR: 0.10000\n",
      "Epoch [78], Train Loss: 0.0841, Val Loss: 0.4445, LR: 0.10000\n",
      "Epoch [79], Train Loss: 0.0831, Val Loss: 0.4427, LR: 0.10000\n",
      "Epoch [80], Train Loss: 0.0820, Val Loss: 0.4474, LR: 0.10000\n",
      "Epoch [81], Train Loss: 0.0830, Val Loss: 0.4423, LR: 0.10000\n",
      "Epoch [82], Train Loss: 0.0816, Val Loss: 0.4437, LR: 0.10000\n",
      "Epoch [83], Train Loss: 0.0819, Val Loss: 0.4391, LR: 0.10000\n",
      "Epoch [84], Train Loss: 0.0836, Val Loss: 0.4344, LR: 0.10000\n",
      "Epoch [85], Train Loss: 0.0820, Val Loss: 0.4309, LR: 0.10000\n",
      "Epoch [86], Train Loss: 0.0794, Val Loss: 0.4284, LR: 0.10000\n",
      "Epoch [87], Train Loss: 0.0788, Val Loss: 0.4327, LR: 0.10000\n",
      "Epoch [88], Train Loss: 0.0802, Val Loss: 0.4292, LR: 0.10000\n",
      "Epoch [89], Train Loss: 0.0778, Val Loss: 0.4286, LR: 0.10000\n",
      "Epoch [90], Train Loss: 0.0778, Val Loss: 0.4280, LR: 0.10000\n",
      "Epoch [91], Train Loss: 0.0772, Val Loss: 0.4273, LR: 0.10000\n",
      "Epoch [92], Train Loss: 0.0765, Val Loss: 0.4260, LR: 0.10000\n",
      "Epoch [93], Train Loss: 0.0775, Val Loss: 0.4208, LR: 0.10000\n",
      "Epoch [94], Train Loss: 0.0762, Val Loss: 0.4238, LR: 0.10000\n",
      "Epoch [95], Train Loss: 0.0778, Val Loss: 0.4161, LR: 0.10000\n",
      "Epoch [96], Train Loss: 0.0772, Val Loss: 0.4199, LR: 0.10000\n",
      "Epoch [97], Train Loss: 0.0760, Val Loss: 0.4119, LR: 0.10000\n",
      "Epoch [98], Train Loss: 0.0735, Val Loss: 0.4123, LR: 0.10000\n",
      "Epoch [99], Train Loss: 0.0743, Val Loss: 0.4140, LR: 0.10000\n",
      "Epoch [100], Train Loss: 0.0744, Val Loss: 0.4103, LR: 0.10000\n",
      "Epoch [101], Train Loss: 0.0735, Val Loss: 0.4081, LR: 0.10000\n",
      "Epoch [102], Train Loss: 0.0736, Val Loss: 0.4127, LR: 0.10000\n",
      "Epoch [103], Train Loss: 0.0727, Val Loss: 0.4169, LR: 0.10000\n",
      "Epoch [104], Train Loss: 0.0722, Val Loss: 0.4072, LR: 0.10000\n",
      "Epoch [105], Train Loss: 0.0731, Val Loss: 0.4091, LR: 0.10000\n",
      "Epoch [106], Train Loss: 0.0719, Val Loss: 0.4006, LR: 0.10000\n",
      "Epoch [107], Train Loss: 0.0714, Val Loss: 0.4060, LR: 0.10000\n",
      "Epoch [108], Train Loss: 0.0716, Val Loss: 0.4054, LR: 0.10000\n",
      "Epoch [109], Train Loss: 0.0712, Val Loss: 0.4025, LR: 0.10000\n",
      "Epoch [110], Train Loss: 0.0705, Val Loss: 0.4024, LR: 0.10000\n",
      "Epoch [111], Train Loss: 0.0711, Val Loss: 0.3987, LR: 0.10000\n",
      "Epoch [112], Train Loss: 0.0703, Val Loss: 0.4044, LR: 0.10000\n",
      "Epoch [113], Train Loss: 0.0706, Val Loss: 0.3982, LR: 0.10000\n",
      "Epoch [114], Train Loss: 0.0691, Val Loss: 0.4056, LR: 0.10000\n",
      "Epoch [115], Train Loss: 0.0691, Val Loss: 0.4008, LR: 0.10000\n",
      "Epoch [116], Train Loss: 0.0686, Val Loss: 0.4015, LR: 0.10000\n",
      "Epoch [117], Train Loss: 0.0688, Val Loss: 0.4001, LR: 0.10000\n",
      "Epoch [118], Train Loss: 0.0699, Val Loss: 0.3984, LR: 0.10000\n",
      "Epoch [119], Train Loss: 0.0681, Val Loss: 0.3972, LR: 0.10000\n",
      "Epoch [120], Train Loss: 0.0684, Val Loss: 0.4011, LR: 0.10000\n",
      "Epoch [121], Train Loss: 0.0691, Val Loss: 0.3944, LR: 0.10000\n",
      "Epoch [122], Train Loss: 0.0675, Val Loss: 0.3951, LR: 0.10000\n",
      "Epoch [123], Train Loss: 0.0665, Val Loss: 0.3931, LR: 0.10000\n",
      "Epoch [124], Train Loss: 0.0678, Val Loss: 0.3904, LR: 0.10000\n",
      "Epoch [125], Train Loss: 0.0675, Val Loss: 0.3886, LR: 0.10000\n",
      "Epoch [126], Train Loss: 0.0664, Val Loss: 0.3872, LR: 0.10000\n",
      "Epoch [127], Train Loss: 0.0662, Val Loss: 0.3909, LR: 0.10000\n",
      "Epoch [128], Train Loss: 0.0664, Val Loss: 0.3849, LR: 0.10000\n",
      "Epoch [129], Train Loss: 0.0657, Val Loss: 0.3907, LR: 0.10000\n",
      "Epoch [130], Train Loss: 0.0659, Val Loss: 0.3909, LR: 0.10000\n",
      "Epoch [131], Train Loss: 0.0656, Val Loss: 0.3882, LR: 0.10000\n",
      "Epoch [132], Train Loss: 0.0651, Val Loss: 0.3870, LR: 0.10000\n",
      "Epoch [133], Train Loss: 0.0643, Val Loss: 0.3901, LR: 0.10000\n",
      "Epoch [134], Train Loss: 0.0642, Val Loss: 0.3860, LR: 0.10000\n",
      "Epoch [135], Train Loss: 0.0648, Val Loss: 0.3882, LR: 0.10000\n",
      "Epoch [136], Train Loss: 0.0641, Val Loss: 0.3887, LR: 0.10000\n",
      "Epoch [137], Train Loss: 0.0643, Val Loss: 0.3842, LR: 0.10000\n",
      "Epoch [138], Train Loss: 0.0636, Val Loss: 0.3828, LR: 0.10000\n",
      "Epoch [139], Train Loss: 0.0637, Val Loss: 0.3868, LR: 0.10000\n",
      "Epoch [140], Train Loss: 0.0638, Val Loss: 0.3845, LR: 0.10000\n",
      "Epoch [141], Train Loss: 0.0633, Val Loss: 0.3802, LR: 0.10000\n",
      "Epoch [142], Train Loss: 0.0629, Val Loss: 0.3839, LR: 0.10000\n",
      "Epoch [143], Train Loss: 0.0627, Val Loss: 0.3824, LR: 0.10000\n",
      "Epoch [144], Train Loss: 0.0622, Val Loss: 0.3806, LR: 0.10000\n",
      "Epoch [145], Train Loss: 0.0631, Val Loss: 0.3773, LR: 0.10000\n",
      "Epoch [146], Train Loss: 0.0624, Val Loss: 0.3839, LR: 0.10000\n",
      "Epoch [147], Train Loss: 0.0618, Val Loss: 0.3817, LR: 0.10000\n",
      "Epoch [148], Train Loss: 0.0615, Val Loss: 0.3795, LR: 0.10000\n",
      "Epoch [149], Train Loss: 0.0619, Val Loss: 0.3798, LR: 0.10000\n",
      "Epoch [150], Train Loss: 0.0602, Val Loss: 0.3866, LR: 0.10000\n",
      "Epoch [151], Train Loss: 0.0617, Val Loss: 0.3785, LR: 0.10000\n",
      "Epoch [152], Train Loss: 0.0611, Val Loss: 0.3824, LR: 0.10000\n",
      "Epoch [153], Train Loss: 0.0610, Val Loss: 0.3809, LR: 0.10000\n",
      "Epoch [154], Train Loss: 0.0606, Val Loss: 0.3771, LR: 0.10000\n",
      "Epoch [155], Train Loss: 0.0600, Val Loss: 0.3721, LR: 0.10000\n",
      "Epoch [156], Train Loss: 0.0590, Val Loss: 0.3804, LR: 0.10000\n",
      "Epoch [157], Train Loss: 0.0598, Val Loss: 0.3709, LR: 0.10000\n",
      "Epoch [158], Train Loss: 0.0598, Val Loss: 0.3782, LR: 0.10000\n",
      "Epoch [159], Train Loss: 0.0601, Val Loss: 0.3765, LR: 0.10000\n",
      "Epoch [160], Train Loss: 0.0593, Val Loss: 0.3758, LR: 0.10000\n",
      "Epoch [161], Train Loss: 0.0600, Val Loss: 0.3736, LR: 0.10000\n",
      "Epoch [162], Train Loss: 0.0601, Val Loss: 0.3725, LR: 0.10000\n",
      "Epoch [163], Train Loss: 0.0588, Val Loss: 0.3790, LR: 0.10000\n",
      "Epoch [164], Train Loss: 0.0592, Val Loss: 0.3736, LR: 0.10000\n",
      "Epoch [165], Train Loss: 0.0591, Val Loss: 0.3714, LR: 0.10000\n",
      "Epoch [166], Train Loss: 0.0581, Val Loss: 0.3789, LR: 0.10000\n",
      "Epoch [167], Train Loss: 0.0586, Val Loss: 0.3766, LR: 0.10000\n",
      "Epoch [168], Train Loss: 0.0586, Val Loss: 0.3748, LR: 0.10000\n",
      "Epoch [169], Train Loss: 0.0583, Val Loss: 0.3799, LR: 0.10000\n",
      "Epoch [170], Train Loss: 0.0581, Val Loss: 0.3678, LR: 0.10000\n",
      "Epoch [171], Train Loss: 0.0575, Val Loss: 0.3700, LR: 0.10000\n",
      "Epoch [172], Train Loss: 0.0577, Val Loss: 0.3703, LR: 0.10000\n",
      "Epoch [173], Train Loss: 0.0578, Val Loss: 0.3700, LR: 0.10000\n",
      "Epoch [174], Train Loss: 0.0577, Val Loss: 0.3788, LR: 0.10000\n",
      "Epoch [175], Train Loss: 0.0567, Val Loss: 0.3697, LR: 0.10000\n",
      "Epoch [176], Train Loss: 0.0567, Val Loss: 0.3663, LR: 0.10000\n",
      "Epoch [177], Train Loss: 0.0566, Val Loss: 0.3670, LR: 0.10000\n",
      "Epoch [178], Train Loss: 0.0567, Val Loss: 0.3678, LR: 0.10000\n",
      "Epoch [179], Train Loss: 0.0574, Val Loss: 0.3614, LR: 0.10000\n",
      "Epoch [180], Train Loss: 0.0559, Val Loss: 0.3616, LR: 0.10000\n",
      "Epoch [181], Train Loss: 0.0570, Val Loss: 0.3640, LR: 0.10000\n",
      "Epoch [182], Train Loss: 0.0568, Val Loss: 0.3659, LR: 0.10000\n",
      "Epoch [183], Train Loss: 0.0562, Val Loss: 0.3626, LR: 0.10000\n",
      "Epoch [184], Train Loss: 0.0563, Val Loss: 0.3578, LR: 0.10000\n",
      "Epoch [185], Train Loss: 0.0554, Val Loss: 0.3628, LR: 0.10000\n",
      "Epoch [186], Train Loss: 0.0556, Val Loss: 0.3633, LR: 0.10000\n",
      "Epoch [187], Train Loss: 0.0553, Val Loss: 0.3612, LR: 0.10000\n",
      "Epoch [188], Train Loss: 0.0557, Val Loss: 0.3572, LR: 0.10000\n",
      "Epoch [189], Train Loss: 0.0558, Val Loss: 0.3558, LR: 0.10000\n",
      "Epoch [190], Train Loss: 0.0552, Val Loss: 0.3588, LR: 0.10000\n",
      "Epoch [191], Train Loss: 0.0555, Val Loss: 0.3604, LR: 0.10000\n",
      "Epoch [192], Train Loss: 0.0553, Val Loss: 0.3535, LR: 0.10000\n",
      "Epoch [193], Train Loss: 0.0547, Val Loss: 0.3664, LR: 0.10000\n",
      "Epoch [194], Train Loss: 0.0542, Val Loss: 0.3612, LR: 0.10000\n",
      "Epoch [195], Train Loss: 0.0554, Val Loss: 0.3641, LR: 0.10000\n",
      "Epoch [196], Train Loss: 0.0534, Val Loss: 0.3609, LR: 0.10000\n",
      "Epoch [197], Train Loss: 0.0553, Val Loss: 0.3584, LR: 0.10000\n",
      "Epoch [198], Train Loss: 0.0546, Val Loss: 0.3579, LR: 0.10000\n",
      "Epoch [199], Train Loss: 0.0546, Val Loss: 0.3603, LR: 0.10000\n",
      "Epoch [200], Train Loss: 0.0541, Val Loss: 0.3572, LR: 0.10000\n",
      "✅ Run 4: Acc = 76.08%, MDE = 1.0491\n",
      "\n",
      "=== Run 5/5 ===\n",
      "Epoch [1], Train Loss: 27.7177, Val Loss: 15.9718, LR: 0.10000\n",
      "Epoch [2], Train Loss: 12.0393, Val Loss: 9.5548, LR: 0.10000\n",
      "Epoch [3], Train Loss: 7.3710, Val Loss: 6.6392, LR: 0.10000\n",
      "Epoch [4], Train Loss: 4.8051, Val Loss: 4.9303, LR: 0.10000\n",
      "Epoch [5], Train Loss: 3.6658, Val Loss: 4.0199, LR: 0.10000\n",
      "Epoch [6], Train Loss: 2.8643, Val Loss: 3.3891, LR: 0.10000\n",
      "Epoch [7], Train Loss: 2.3220, Val Loss: 2.9632, LR: 0.10000\n",
      "Epoch [8], Train Loss: 1.9492, Val Loss: 2.6287, LR: 0.10000\n",
      "Epoch [9], Train Loss: 1.6805, Val Loss: 2.3424, LR: 0.10000\n",
      "Epoch [10], Train Loss: 1.4785, Val Loss: 2.1714, LR: 0.10000\n",
      "Epoch [11], Train Loss: 1.3049, Val Loss: 1.9792, LR: 0.10000\n",
      "Epoch [12], Train Loss: 1.1631, Val Loss: 1.8394, LR: 0.10000\n",
      "Epoch [13], Train Loss: 1.0470, Val Loss: 1.7453, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.9578, Val Loss: 1.6230, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.8794, Val Loss: 1.5619, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.7971, Val Loss: 1.4674, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.7369, Val Loss: 1.4062, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.6905, Val Loss: 1.3386, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.6501, Val Loss: 1.2770, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.5973, Val Loss: 1.2099, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.5520, Val Loss: 1.1511, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.5248, Val Loss: 1.1006, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.4883, Val Loss: 1.0800, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.4562, Val Loss: 1.0295, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.4307, Val Loss: 0.9833, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.4113, Val Loss: 0.9823, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.3835, Val Loss: 0.9213, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.3638, Val Loss: 0.8981, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.3558, Val Loss: 0.8638, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.3323, Val Loss: 0.8466, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.3161, Val Loss: 0.8190, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.3055, Val Loss: 0.8271, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.2971, Val Loss: 0.7744, LR: 0.10000\n",
      "Epoch [34], Train Loss: 0.2796, Val Loss: 0.7593, LR: 0.10000\n",
      "Epoch [35], Train Loss: 0.2695, Val Loss: 0.7490, LR: 0.10000\n",
      "Epoch [36], Train Loss: 0.2614, Val Loss: 0.7341, LR: 0.10000\n",
      "Epoch [37], Train Loss: 0.2543, Val Loss: 0.7249, LR: 0.10000\n",
      "Epoch [38], Train Loss: 0.2461, Val Loss: 0.6993, LR: 0.10000\n",
      "Epoch [39], Train Loss: 0.2415, Val Loss: 0.6974, LR: 0.10000\n",
      "Epoch [40], Train Loss: 0.2321, Val Loss: 0.6621, LR: 0.10000\n",
      "Epoch [41], Train Loss: 0.2243, Val Loss: 0.6608, LR: 0.10000\n",
      "Epoch [42], Train Loss: 0.2206, Val Loss: 0.6477, LR: 0.10000\n",
      "Epoch [43], Train Loss: 0.2115, Val Loss: 0.6485, LR: 0.10000\n",
      "Epoch [44], Train Loss: 0.2068, Val Loss: 0.6349, LR: 0.10000\n",
      "Epoch [45], Train Loss: 0.2043, Val Loss: 0.6233, LR: 0.10000\n",
      "Epoch [46], Train Loss: 0.1986, Val Loss: 0.6071, LR: 0.10000\n",
      "Epoch [47], Train Loss: 0.1917, Val Loss: 0.6057, LR: 0.10000\n",
      "Epoch [48], Train Loss: 0.1897, Val Loss: 0.6033, LR: 0.10000\n",
      "Epoch [49], Train Loss: 0.1856, Val Loss: 0.5844, LR: 0.10000\n",
      "Epoch [50], Train Loss: 0.1816, Val Loss: 0.5790, LR: 0.10000\n",
      "Epoch [51], Train Loss: 0.1774, Val Loss: 0.5843, LR: 0.10000\n",
      "Epoch [52], Train Loss: 0.1765, Val Loss: 0.5629, LR: 0.10000\n",
      "Epoch [53], Train Loss: 0.1776, Val Loss: 0.5582, LR: 0.10000\n",
      "Epoch [54], Train Loss: 0.1682, Val Loss: 0.5440, LR: 0.10000\n",
      "Epoch [55], Train Loss: 0.1665, Val Loss: 0.5468, LR: 0.10000\n",
      "Epoch [56], Train Loss: 0.1607, Val Loss: 0.5491, LR: 0.10000\n",
      "Epoch [57], Train Loss: 0.1617, Val Loss: 0.5291, LR: 0.10000\n",
      "Epoch [58], Train Loss: 0.1596, Val Loss: 0.5221, LR: 0.10000\n",
      "Epoch [59], Train Loss: 0.1575, Val Loss: 0.5309, LR: 0.10000\n",
      "Epoch [60], Train Loss: 0.1572, Val Loss: 0.5203, LR: 0.10000\n",
      "Epoch [61], Train Loss: 0.1524, Val Loss: 0.4988, LR: 0.10000\n",
      "Epoch [62], Train Loss: 0.1501, Val Loss: 0.5122, LR: 0.10000\n",
      "Epoch [63], Train Loss: 0.1485, Val Loss: 0.5000, LR: 0.10000\n",
      "Epoch [64], Train Loss: 0.1452, Val Loss: 0.4977, LR: 0.10000\n",
      "Epoch [65], Train Loss: 0.1444, Val Loss: 0.4967, LR: 0.10000\n",
      "Epoch [66], Train Loss: 0.1428, Val Loss: 0.4728, LR: 0.10000\n",
      "Epoch [67], Train Loss: 0.1418, Val Loss: 0.4704, LR: 0.10000\n",
      "Epoch [68], Train Loss: 0.1393, Val Loss: 0.4773, LR: 0.10000\n",
      "Epoch [69], Train Loss: 0.1370, Val Loss: 0.4650, LR: 0.10000\n",
      "Epoch [70], Train Loss: 0.1346, Val Loss: 0.4615, LR: 0.10000\n",
      "Epoch [71], Train Loss: 0.1361, Val Loss: 0.4569, LR: 0.10000\n",
      "Epoch [72], Train Loss: 0.1369, Val Loss: 0.4568, LR: 0.10000\n",
      "Epoch [73], Train Loss: 0.1331, Val Loss: 0.4523, LR: 0.10000\n",
      "Epoch [74], Train Loss: 0.1322, Val Loss: 0.4411, LR: 0.10000\n",
      "Epoch [75], Train Loss: 0.1300, Val Loss: 0.4551, LR: 0.10000\n",
      "Epoch [76], Train Loss: 0.1279, Val Loss: 0.4372, LR: 0.10000\n",
      "Epoch [77], Train Loss: 0.1264, Val Loss: 0.4328, LR: 0.10000\n",
      "Epoch [78], Train Loss: 0.1254, Val Loss: 0.4370, LR: 0.10000\n",
      "Epoch [79], Train Loss: 0.1257, Val Loss: 0.4229, LR: 0.10000\n",
      "Epoch [80], Train Loss: 0.1242, Val Loss: 0.4256, LR: 0.10000\n",
      "Epoch [81], Train Loss: 0.1222, Val Loss: 0.4199, LR: 0.10000\n",
      "Epoch [82], Train Loss: 0.1222, Val Loss: 0.4194, LR: 0.10000\n",
      "Epoch [83], Train Loss: 0.1210, Val Loss: 0.4047, LR: 0.10000\n",
      "Epoch [84], Train Loss: 0.1208, Val Loss: 0.4200, LR: 0.10000\n",
      "Epoch [85], Train Loss: 0.1192, Val Loss: 0.4109, LR: 0.10000\n",
      "Epoch [86], Train Loss: 0.1196, Val Loss: 0.4147, LR: 0.10000\n",
      "Epoch [87], Train Loss: 0.1173, Val Loss: 0.4088, LR: 0.10000\n",
      "Epoch [88], Train Loss: 0.1163, Val Loss: 0.3956, LR: 0.10000\n",
      "Epoch [89], Train Loss: 0.1165, Val Loss: 0.4113, LR: 0.10000\n",
      "Epoch [90], Train Loss: 0.1159, Val Loss: 0.3992, LR: 0.10000\n",
      "Epoch [91], Train Loss: 0.1145, Val Loss: 0.3895, LR: 0.10000\n",
      "Epoch [92], Train Loss: 0.1123, Val Loss: 0.3887, LR: 0.10000\n",
      "Epoch [93], Train Loss: 0.1110, Val Loss: 0.3935, LR: 0.10000\n",
      "Epoch [94], Train Loss: 0.1119, Val Loss: 0.3845, LR: 0.10000\n",
      "Epoch [95], Train Loss: 0.1111, Val Loss: 0.3825, LR: 0.10000\n",
      "Epoch [96], Train Loss: 0.1101, Val Loss: 0.3759, LR: 0.10000\n",
      "Epoch [97], Train Loss: 0.1100, Val Loss: 0.3794, LR: 0.10000\n",
      "Epoch [98], Train Loss: 0.1070, Val Loss: 0.3807, LR: 0.10000\n",
      "Epoch [99], Train Loss: 0.1100, Val Loss: 0.3766, LR: 0.10000\n",
      "Epoch [100], Train Loss: 0.1082, Val Loss: 0.3879, LR: 0.10000\n",
      "Epoch [101], Train Loss: 0.1077, Val Loss: 0.3703, LR: 0.10000\n",
      "Epoch [102], Train Loss: 0.1060, Val Loss: 0.3838, LR: 0.10000\n",
      "Epoch [103], Train Loss: 0.1067, Val Loss: 0.3757, LR: 0.10000\n",
      "Epoch [104], Train Loss: 0.1051, Val Loss: 0.3742, LR: 0.10000\n",
      "Epoch [105], Train Loss: 0.1046, Val Loss: 0.3703, LR: 0.10000\n",
      "Epoch [106], Train Loss: 0.1057, Val Loss: 0.3614, LR: 0.10000\n",
      "Epoch [107], Train Loss: 0.1045, Val Loss: 0.3619, LR: 0.10000\n",
      "Epoch [108], Train Loss: 0.1041, Val Loss: 0.3623, LR: 0.10000\n",
      "Epoch [109], Train Loss: 0.1035, Val Loss: 0.3672, LR: 0.10000\n",
      "Epoch [110], Train Loss: 0.1018, Val Loss: 0.3615, LR: 0.10000\n",
      "Epoch [111], Train Loss: 0.1028, Val Loss: 0.3586, LR: 0.10000\n",
      "Epoch [112], Train Loss: 0.1031, Val Loss: 0.3517, LR: 0.10000\n",
      "Epoch [113], Train Loss: 0.1011, Val Loss: 0.3514, LR: 0.10000\n",
      "Epoch [114], Train Loss: 0.0991, Val Loss: 0.3483, LR: 0.10000\n",
      "Epoch [115], Train Loss: 0.1014, Val Loss: 0.3467, LR: 0.10000\n",
      "Epoch [116], Train Loss: 0.0987, Val Loss: 0.3483, LR: 0.10000\n",
      "Epoch [117], Train Loss: 0.0990, Val Loss: 0.3640, LR: 0.10000\n",
      "Epoch [118], Train Loss: 0.0990, Val Loss: 0.3405, LR: 0.10000\n",
      "Epoch [119], Train Loss: 0.0978, Val Loss: 0.3383, LR: 0.10000\n",
      "Epoch [120], Train Loss: 0.0981, Val Loss: 0.3455, LR: 0.10000\n",
      "Epoch [121], Train Loss: 0.0984, Val Loss: 0.3484, LR: 0.10000\n",
      "Epoch [122], Train Loss: 0.0964, Val Loss: 0.3454, LR: 0.10000\n",
      "Epoch [123], Train Loss: 0.0965, Val Loss: 0.3421, LR: 0.10000\n",
      "Epoch [124], Train Loss: 0.0966, Val Loss: 0.3432, LR: 0.10000\n",
      "Epoch [125], Train Loss: 0.0968, Val Loss: 0.3357, LR: 0.10000\n",
      "Epoch [126], Train Loss: 0.0958, Val Loss: 0.3475, LR: 0.10000\n",
      "Epoch [127], Train Loss: 0.0956, Val Loss: 0.3399, LR: 0.10000\n",
      "Epoch [128], Train Loss: 0.0939, Val Loss: 0.3302, LR: 0.10000\n",
      "Epoch [129], Train Loss: 0.0941, Val Loss: 0.3248, LR: 0.10000\n",
      "Epoch [130], Train Loss: 0.0949, Val Loss: 0.3302, LR: 0.10000\n",
      "Epoch [131], Train Loss: 0.0949, Val Loss: 0.3458, LR: 0.10000\n",
      "Epoch [132], Train Loss: 0.0929, Val Loss: 0.3326, LR: 0.10000\n",
      "Epoch [133], Train Loss: 0.0953, Val Loss: 0.3335, LR: 0.10000\n",
      "Epoch [134], Train Loss: 0.0928, Val Loss: 0.3299, LR: 0.10000\n",
      "Epoch [135], Train Loss: 0.0922, Val Loss: 0.3250, LR: 0.10000\n",
      "Epoch [136], Train Loss: 0.0918, Val Loss: 0.3258, LR: 0.10000\n",
      "Epoch [137], Train Loss: 0.0910, Val Loss: 0.3263, LR: 0.10000\n",
      "Epoch [138], Train Loss: 0.0915, Val Loss: 0.3309, LR: 0.10000\n",
      "Epoch [139], Train Loss: 0.0912, Val Loss: 0.3265, LR: 0.10000\n",
      "Epoch [140], Train Loss: 0.0908, Val Loss: 0.3236, LR: 0.10000\n",
      "Epoch [141], Train Loss: 0.0925, Val Loss: 0.3242, LR: 0.10000\n",
      "Epoch [142], Train Loss: 0.0901, Val Loss: 0.3165, LR: 0.10000\n",
      "Epoch [143], Train Loss: 0.0893, Val Loss: 0.3282, LR: 0.10000\n",
      "Epoch [144], Train Loss: 0.0901, Val Loss: 0.3265, LR: 0.10000\n",
      "Epoch [145], Train Loss: 0.0888, Val Loss: 0.3241, LR: 0.10000\n",
      "Epoch [146], Train Loss: 0.0874, Val Loss: 0.3173, LR: 0.10000\n",
      "Epoch [147], Train Loss: 0.0901, Val Loss: 0.3144, LR: 0.10000\n",
      "Epoch [148], Train Loss: 0.0886, Val Loss: 0.3407, LR: 0.10000\n",
      "Epoch [149], Train Loss: 0.0881, Val Loss: 0.3153, LR: 0.10000\n",
      "Epoch [150], Train Loss: 0.0882, Val Loss: 0.3215, LR: 0.10000\n",
      "Epoch [151], Train Loss: 0.0897, Val Loss: 0.3141, LR: 0.10000\n",
      "Epoch [152], Train Loss: 0.0870, Val Loss: 0.3206, LR: 0.10000\n",
      "Epoch [153], Train Loss: 0.0906, Val Loss: 0.3118, LR: 0.10000\n",
      "Epoch [154], Train Loss: 0.0925, Val Loss: 0.3220, LR: 0.10000\n",
      "Epoch [155], Train Loss: 0.0890, Val Loss: 0.3223, LR: 0.10000\n",
      "Epoch [156], Train Loss: 0.0889, Val Loss: 0.3196, LR: 0.10000\n",
      "Epoch [157], Train Loss: 0.0892, Val Loss: 0.3186, LR: 0.10000\n",
      "Epoch [158], Train Loss: 0.0877, Val Loss: 0.3157, LR: 0.10000\n",
      "Epoch [159], Train Loss: 0.0889, Val Loss: 0.3098, LR: 0.10000\n",
      "Epoch [160], Train Loss: 0.0852, Val Loss: 0.3168, LR: 0.10000\n",
      "Epoch [161], Train Loss: 0.0851, Val Loss: 0.3021, LR: 0.10000\n",
      "Epoch [162], Train Loss: 0.0868, Val Loss: 0.3179, LR: 0.10000\n",
      "Epoch [163], Train Loss: 0.0833, Val Loss: 0.3154, LR: 0.10000\n",
      "Epoch [164], Train Loss: 0.0846, Val Loss: 0.3099, LR: 0.10000\n",
      "Epoch [165], Train Loss: 0.0839, Val Loss: 0.3231, LR: 0.10000\n",
      "Epoch [166], Train Loss: 0.0841, Val Loss: 0.3141, LR: 0.10000\n",
      "Epoch [167], Train Loss: 0.0841, Val Loss: 0.3167, LR: 0.10000\n",
      "Epoch [168], Train Loss: 0.0829, Val Loss: 0.3123, LR: 0.10000\n",
      "Epoch [169], Train Loss: 0.0855, Val Loss: 0.3153, LR: 0.10000\n",
      "Epoch [170], Train Loss: 0.0833, Val Loss: 0.3174, LR: 0.10000\n",
      "Epoch [171], Train Loss: 0.0822, Val Loss: 0.3134, LR: 0.10000\n",
      "Epoch [172], Train Loss: 0.0839, Val Loss: 0.3118, LR: 0.10000\n",
      "Epoch [173], Train Loss: 0.0828, Val Loss: 0.3075, LR: 0.10000\n",
      "Epoch [174], Train Loss: 0.0838, Val Loss: 0.3124, LR: 0.10000\n",
      "Epoch [175], Train Loss: 0.0822, Val Loss: 0.3047, LR: 0.10000\n",
      "Epoch [176], Train Loss: 0.0836, Val Loss: 0.3031, LR: 0.10000\n",
      "Epoch [177], Train Loss: 0.0833, Val Loss: 0.3007, LR: 0.10000\n",
      "Epoch [178], Train Loss: 0.0829, Val Loss: 0.3079, LR: 0.10000\n",
      "Epoch [179], Train Loss: 0.0818, Val Loss: 0.3103, LR: 0.10000\n",
      "Epoch [180], Train Loss: 0.0827, Val Loss: 0.3076, LR: 0.10000\n",
      "Epoch [181], Train Loss: 0.0804, Val Loss: 0.3148, LR: 0.10000\n",
      "Epoch [182], Train Loss: 0.0810, Val Loss: 0.3031, LR: 0.10000\n",
      "Epoch [183], Train Loss: 0.0808, Val Loss: 0.3128, LR: 0.10000\n",
      "Epoch [184], Train Loss: 0.0802, Val Loss: 0.2995, LR: 0.10000\n",
      "Epoch [185], Train Loss: 0.0818, Val Loss: 0.3057, LR: 0.10000\n",
      "Epoch [186], Train Loss: 0.0805, Val Loss: 0.2997, LR: 0.10000\n",
      "Epoch [187], Train Loss: 0.0810, Val Loss: 0.3134, LR: 0.10000\n",
      "Epoch [188], Train Loss: 0.0806, Val Loss: 0.3076, LR: 0.10000\n",
      "Epoch [189], Train Loss: 0.0816, Val Loss: 0.3067, LR: 0.10000\n",
      "Epoch [190], Train Loss: 0.0792, Val Loss: 0.2972, LR: 0.10000\n",
      "Epoch [191], Train Loss: 0.0805, Val Loss: 0.2973, LR: 0.10000\n",
      "Epoch [192], Train Loss: 0.0820, Val Loss: 0.3032, LR: 0.10000\n",
      "Epoch [193], Train Loss: 0.0799, Val Loss: 0.3120, LR: 0.10000\n",
      "Epoch [194], Train Loss: 0.0797, Val Loss: 0.3034, LR: 0.10000\n",
      "Epoch [195], Train Loss: 0.0803, Val Loss: 0.3093, LR: 0.10000\n",
      "Epoch [196], Train Loss: 0.0808, Val Loss: 0.3028, LR: 0.10000\n",
      "Epoch [197], Train Loss: 0.0785, Val Loss: 0.3081, LR: 0.10000\n",
      "Epoch [198], Train Loss: 0.0798, Val Loss: 0.3026, LR: 0.10000\n",
      "Epoch [199], Train Loss: 0.0772, Val Loss: 0.3085, LR: 0.10000\n",
      "Epoch [200], Train Loss: 0.0809, Val Loss: 0.3008, LR: 0.10000\n",
      "✅ Run 5: Acc = 72.77%, MDE = 1.2009\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def eval_siamese_acc(model, data_loader, margin=4.0):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, label in data_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            distance = model(x1, x2)\n",
    "            pred = (distance < margin).float()  # 距離小於 margin 視為同類\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            print(f\"Warning: Label {true_label} or {pred_label} not in coordinates, skipping.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        errors.append(np.linalg.norm(pred_coord - true_coord))\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "\n",
    "\n",
    "COORDINATES = {\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# ==== 1. 模型定義 ====\n",
    "\n",
    "class SiameseSubNet1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(48, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class SiameseNetwork1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subnet = SiameseSubNet1D()\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.subnet(x1)\n",
    "        out2 = self.subnet(x2)\n",
    "        distance = F.pairwise_distance(out1, out2)\n",
    "        return distance\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=4.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, output, label):\n",
    "        loss = 0.5 * label * torch.pow(output, 2) + \\\n",
    "               0.5 * (1 - label) * torch.pow(torch.clamp(self.margin - output, min=0.0), 2)\n",
    "        return loss.mean()\n",
    "\n",
    "# ==== 2. Dataset 全pair實作 ====\n",
    "\n",
    "class CSIAllPairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, T1):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = np.array(y)\n",
    "        self.label_set = np.unique(self.y)\n",
    "        self.T1 = T1\n",
    "        self.idx_by_label = {label: np.where(self.y == label)[0][:T1] for label in self.label_set}\n",
    "        self.fingerprints = {}\n",
    "        for label in self.label_set:\n",
    "            idxs = self.idx_by_label[label]\n",
    "            self.fingerprints[label] = self.X[idxs].mean(dim=0)\n",
    "        self.pairs = []\n",
    "        for rp in self.label_set:\n",
    "            idxs = self.idx_by_label[rp]\n",
    "            for i in idxs:\n",
    "                for fp_rp in self.label_set:\n",
    "                    label = 1 if rp == fp_rp else 0\n",
    "                    self.pairs.append((i, fp_rp, label))\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        i, fp_label, label = self.pairs[idx]\n",
    "        x1 = self.X[i]\n",
    "        x2 = self.fingerprints[fp_label]\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# ==== 3. 評估函數 ====\n",
    "\n",
    "def eval_siamese_acc(model, data_loader, margin=4.0):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, label in data_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            distance = model(x1, x2)\n",
    "            pred = (distance < margin).float()\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        errors.append(np.linalg.norm(pred_coord - true_coord))\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "# ==== 4. 訓練參數與資料準備 ====\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_runs = 5\n",
    "epochs = 200\n",
    "patience = 20\n",
    "T1 = 7\n",
    "margin = 4.0\n",
    "\n",
    "\n",
    "train_dataset = CSIAllPairsDataset(amp_train, y_train, T1)\n",
    "val_dataset = CSIAllPairsDataset(amp_val, y_val, T1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_accs, test_mdes, all_run_errors = [], [], []\n",
    "os.makedirs(\"siamese_results\", exist_ok=True)\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    print(f\"\\n=== Run {run}/{num_runs} ===\")\n",
    "\n",
    "    model = SiameseNetwork1D().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=0.1, rho=0.95, eps=1e-8)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6, verbose=True)\n",
    "    criterion = ContrastiveLoss(margin=4.0)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x1, x2, label in train_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            distance = model(x1, x2)\n",
    "            loss = criterion(distance, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # === 驗證（以 contrastive loss 作 early stop/lr decay依據） ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, label in val_loader:\n",
    "                x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "                distance = model(x1, x2)\n",
    "                val_loss += criterion(distance, label).item() * x1.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch [{epoch+1}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # ==== 測試階段：fingerprint-based 位置預測與 MDE ====\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    # 準備 fingerprint\n",
    "    fingerprints = []\n",
    "    rp_list = np.unique(y_train)\n",
    "    for rp in rp_list:\n",
    "        idxs = np.where(y_train == rp)[0][:T1]\n",
    "        fp = torch.tensor(amp_train[idxs].mean(axis=0), dtype=torch.float32).to(device)\n",
    "        fingerprints.append(fp)\n",
    "    fingerprints = torch.stack(fingerprints)\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    for i in range(len(amp_test)):\n",
    "        test_csi = torch.tensor(amp_test[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        min_dist = float('inf')\n",
    "        pred_rp = None\n",
    "        with torch.no_grad():\n",
    "            for j, fp in enumerate(fingerprints):\n",
    "                dist = model.subnet(test_csi).sub(model.subnet(fp.unsqueeze(0))).norm(p=2).item()\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    pred_rp = rp_list[j]\n",
    "        all_true.append(y_test[i])\n",
    "        all_pred.append(pred_rp)\n",
    "    y_true = np.array(all_true)\n",
    "    y_pred = np.array(all_pred)\n",
    "    acc = 100 * np.mean(y_true == y_pred)\n",
    "    mde, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "    all_run_errors.append(errors)\n",
    "    test_accs.append(acc)\n",
    "    test_mdes.append(mde)\n",
    "    print(f\"✅ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "# ==== 儲存 summary 結果 ====\n",
    "df = pd.DataFrame({\n",
    "    \"run\": list(range(1, num_runs+1)),\n",
    "    \"accuracy\": test_accs,\n",
    "    \"mde\": test_mdes\n",
    "})\n",
    "df.to_csv(\"siamese_results/siamese_results.csv\", index=False)\n",
    "\n",
    "error_records = []\n",
    "for run_idx, errors in enumerate(all_run_errors):\n",
    "    for sample_idx, e in enumerate(errors):\n",
    "        error_records.append({\n",
    "            \"run\": run_idx + 1,\n",
    "            \"sample_idx\": sample_idx + 1,\n",
    "            \"error\": e\n",
    "        })\n",
    "df_errors = pd.DataFrame(error_records)\n",
    "df_errors.to_csv(\"siamese_results/siamese_all_errors.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eafcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Loss: 54.6189\n",
      "Epoch [2], Loss: 52.8977\n",
      "Epoch [3], Loss: 52.1739\n",
      "Epoch [4], Loss: 51.5112\n",
      "Epoch [5], Loss: 51.8643\n",
      "Epoch [6], Loss: 50.1378\n",
      "Epoch [7], Loss: 49.5170\n",
      "Epoch [8], Loss: 48.8729\n",
      "Epoch [9], Loss: 48.2600\n",
      "Epoch [10], Loss: 47.6342\n",
      "Epoch [11], Loss: 47.0214\n",
      "Epoch [12], Loss: 46.4497\n",
      "Epoch [13], Loss: 45.8008\n",
      "Epoch [14], Loss: 45.1735\n",
      "Epoch [15], Loss: 44.6020\n",
      "Epoch [16], Loss: 43.9874\n",
      "Epoch [17], Loss: 43.3621\n",
      "Epoch [18], Loss: 42.7914\n",
      "Epoch [19], Loss: 42.2601\n",
      "Epoch [20], Loss: 41.6988\n",
      "Epoch [21], Loss: 41.1418\n",
      "Epoch [22], Loss: 40.5986\n",
      "Epoch [23], Loss: 40.0189\n",
      "Epoch [24], Loss: 40.1967\n",
      "Epoch [25], Loss: 38.9518\n",
      "Epoch [26], Loss: 38.4030\n",
      "Epoch [27], Loss: 37.8993\n",
      "Epoch [28], Loss: 37.3822\n",
      "Epoch [29], Loss: 36.9129\n",
      "Epoch [30], Loss: 36.4061\n",
      "Epoch [31], Loss: 35.9453\n",
      "Epoch [32], Loss: 35.7501\n",
      "Epoch [33], Loss: 34.9988\n",
      "Epoch [34], Loss: 34.5065\n",
      "Epoch [35], Loss: 34.0637\n",
      "Epoch [36], Loss: 33.5875\n",
      "Epoch [37], Loss: 33.1199\n",
      "Epoch [38], Loss: 32.6558\n",
      "Epoch [39], Loss: 32.2387\n",
      "Epoch [40], Loss: 31.7974\n",
      "Epoch [41], Loss: 31.3650\n",
      "Epoch [42], Loss: 30.9331\n",
      "Epoch [43], Loss: 30.5690\n",
      "Epoch [44], Loss: 30.1293\n",
      "Epoch [45], Loss: 29.7304\n",
      "Epoch [46], Loss: 29.8002\n",
      "Epoch [47], Loss: 28.9519\n",
      "Epoch [48], Loss: 28.5956\n",
      "Epoch [49], Loss: 28.2353\n",
      "Epoch [50], Loss: 27.8599\n",
      "Epoch [51], Loss: 27.5078\n",
      "Epoch [52], Loss: 27.1549\n",
      "Epoch [53], Loss: 26.8344\n",
      "Epoch [54], Loss: 27.0911\n",
      "Epoch [55], Loss: 26.1787\n",
      "Epoch [56], Loss: 25.8685\n",
      "Epoch [57], Loss: 25.5673\n",
      "Epoch [58], Loss: 25.2357\n",
      "Epoch [59], Loss: 24.9348\n",
      "Epoch [60], Loss: 24.6108\n",
      "Epoch [61], Loss: 24.3402\n",
      "Epoch [62], Loss: 24.0187\n",
      "Epoch [63], Loss: 24.2000\n",
      "Epoch [64], Loss: 23.4655\n",
      "Epoch [65], Loss: 23.1678\n",
      "Epoch [66], Loss: 22.9016\n",
      "Epoch [67], Loss: 22.6364\n",
      "Epoch [68], Loss: 22.3643\n",
      "Epoch [69], Loss: 22.0917\n",
      "Epoch [70], Loss: 21.8188\n",
      "Epoch [71], Loss: 21.6123\n",
      "Epoch [72], Loss: 21.3222\n",
      "Epoch [73], Loss: 21.0874\n",
      "Epoch [74], Loss: 20.8245\n",
      "Epoch [75], Loss: 20.6020\n",
      "Epoch [76], Loss: 20.7521\n",
      "Epoch [77], Loss: 20.0994\n",
      "Epoch [78], Loss: 19.8719\n",
      "Epoch [79], Loss: 19.6364\n",
      "Epoch [80], Loss: 19.4098\n",
      "Epoch [81], Loss: 19.1951\n",
      "Epoch [82], Loss: 18.9604\n",
      "Epoch [83], Loss: 18.7637\n",
      "Epoch [84], Loss: 18.5778\n",
      "Epoch [85], Loss: 18.3421\n",
      "Epoch [86], Loss: 18.1451\n",
      "Epoch [87], Loss: 17.9400\n",
      "Epoch [88], Loss: 17.7404\n",
      "Epoch [89], Loss: 17.5460\n",
      "Epoch [90], Loss: 17.3400\n",
      "Epoch [91], Loss: 17.1697\n",
      "Epoch [92], Loss: 16.9722\n",
      "Epoch [93], Loss: 16.7858\n",
      "Epoch [94], Loss: 16.6079\n",
      "Epoch [95], Loss: 16.4284\n",
      "Epoch [96], Loss: 16.2530\n",
      "Epoch [97], Loss: 16.0801\n",
      "Epoch [98], Loss: 15.9159\n",
      "Epoch [99], Loss: 15.7295\n",
      "Epoch [100], Loss: 15.5607\n",
      "Epoch [101], Loss: 15.3907\n",
      "Epoch [102], Loss: 15.2331\n",
      "Epoch [103], Loss: 15.0736\n",
      "Epoch [104], Loss: 14.9203\n",
      "Epoch [105], Loss: 14.7740\n",
      "Epoch [106], Loss: 14.6206\n",
      "Epoch [107], Loss: 14.4829\n",
      "Epoch [108], Loss: 14.3294\n",
      "Epoch [109], Loss: 14.1726\n",
      "Epoch [110], Loss: 14.0284\n",
      "Epoch [111], Loss: 13.8934\n",
      "Epoch [112], Loss: 13.8688\n",
      "Epoch [113], Loss: 13.6043\n",
      "Epoch [114], Loss: 13.4718\n",
      "Epoch [115], Loss: 13.3331\n",
      "Epoch [116], Loss: 13.2023\n",
      "Epoch [117], Loss: 13.0638\n",
      "Epoch [118], Loss: 12.9319\n",
      "Epoch [119], Loss: 12.8035\n",
      "Epoch [120], Loss: 12.6887\n",
      "Epoch [121], Loss: 12.5643\n",
      "Epoch [122], Loss: 12.4405\n",
      "Epoch [123], Loss: 12.3812\n",
      "Epoch [124], Loss: 12.2010\n",
      "Epoch [125], Loss: 12.0849\n",
      "Epoch [126], Loss: 11.9748\n",
      "Epoch [127], Loss: 11.8667\n",
      "Epoch [128], Loss: 11.7511\n",
      "Epoch [129], Loss: 11.7988\n",
      "Epoch [130], Loss: 11.5388\n",
      "Epoch [131], Loss: 11.8959\n",
      "Epoch [132], Loss: 11.3316\n",
      "Epoch [133], Loss: 11.2343\n",
      "Epoch [134], Loss: 11.1296\n",
      "Epoch [135], Loss: 11.0296\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 14\u001b[0m\n\u001b[1;32m     12\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     13\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 14\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[32], line 35\u001b[0m, in \u001b[0;36mCSIAllPairsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     33\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[i]\n\u001b[1;32m     34\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfingerprints[fp_label]\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x1, x2, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# from torch.utils.data import DataLoader\n",
    "\n",
    "# T1 = 7  # 或你想要的訓練CSI數，與dataset一致\n",
    "# train_dataset = CSIAllPairsDataset(amp_train, y_train, T1)\n",
    "# train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)  # batch size可根據顯存調整\n",
    "\n",
    "# model = SiameseNetwork1D().cuda()\n",
    "# criterion = ContrastiveLoss(margin=4.0)\n",
    "# optimizer = torch.optim.Adadelta(model.parameters(), lr=0.001, rho=0.95, eps=1e-8)\n",
    "\n",
    "# for epoch in range(200):\n",
    "#     model.train()\n",
    "#     running_loss = 0.0\n",
    "#     for x1, x2, label in train_loader:\n",
    "#         x1, x2, label = x1.cuda(), x2.cuda(), label.cuda()\n",
    "#         optimizer.zero_grad()\n",
    "#         distance = model(x1, x2)\n",
    "#         loss = criterion(distance, label)\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "#         running_loss += loss.item()\n",
    "#     print(f\"Epoch [{epoch+1}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "ec419262",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss: 0.2833, Val Loss: 0.7335, LR: 0.10000\n",
      "Epoch [2], Train Loss: 0.2433, Val Loss: 0.5971, LR: 0.10000\n",
      "Epoch [3], Train Loss: 0.2263, Val Loss: 0.5448, LR: 0.10000\n",
      "Epoch [4], Train Loss: 0.2217, Val Loss: 0.5168, LR: 0.10000\n",
      "Epoch [5], Train Loss: 0.2127, Val Loss: 0.5649, LR: 0.10000\n",
      "Epoch [6], Train Loss: 0.2066, Val Loss: 0.4569, LR: 0.10000\n",
      "Epoch [7], Train Loss: 0.2021, Val Loss: 0.4405, LR: 0.10000\n",
      "Epoch [8], Train Loss: 0.2078, Val Loss: 0.5367, LR: 0.10000\n",
      "Epoch [9], Train Loss: 0.1965, Val Loss: 0.4707, LR: 0.10000\n",
      "Epoch [10], Train Loss: 0.1935, Val Loss: 0.4696, LR: 0.10000\n",
      "Epoch [11], Train Loss: 0.1882, Val Loss: 0.4474, LR: 0.10000\n",
      "Epoch [12], Train Loss: 0.1842, Val Loss: 0.4412, LR: 0.10000\n",
      "Epoch [13], Train Loss: 0.1876, Val Loss: 0.4261, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.1824, Val Loss: 0.4417, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.1818, Val Loss: 0.4186, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.1835, Val Loss: 0.4401, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.1754, Val Loss: 0.3908, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.1784, Val Loss: 0.4023, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.1780, Val Loss: 0.3852, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.1783, Val Loss: 0.3694, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.1749, Val Loss: 0.3717, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.1742, Val Loss: 0.3647, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.1712, Val Loss: 0.3880, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.1724, Val Loss: 0.3771, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.1687, Val Loss: 0.3195, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.1655, Val Loss: 0.3678, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.1694, Val Loss: 0.3667, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.1651, Val Loss: 0.3301, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.1685, Val Loss: 0.3372, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.1673, Val Loss: 0.3262, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.1675, Val Loss: 0.3527, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.1644, Val Loss: 0.3278, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.1678, Val Loss: 0.3294, LR: 0.10000\n",
      "Epoch [34], Train Loss: 0.1637, Val Loss: 0.3102, LR: 0.10000\n",
      "Epoch [35], Train Loss: 0.1642, Val Loss: 0.3154, LR: 0.10000\n",
      "Epoch [36], Train Loss: 0.1672, Val Loss: 0.3154, LR: 0.10000\n",
      "Epoch [37], Train Loss: 0.1639, Val Loss: 0.3186, LR: 0.10000\n",
      "Epoch [38], Train Loss: 0.1623, Val Loss: 0.3196, LR: 0.10000\n",
      "Epoch [39], Train Loss: 0.1631, Val Loss: 0.3350, LR: 0.10000\n",
      "Epoch [40], Train Loss: 0.1641, Val Loss: 0.3168, LR: 0.10000\n",
      "Epoch [41], Train Loss: 0.1592, Val Loss: 0.3015, LR: 0.10000\n",
      "Epoch [42], Train Loss: 0.1621, Val Loss: 0.2760, LR: 0.10000\n",
      "Epoch [43], Train Loss: 0.1583, Val Loss: 0.2881, LR: 0.10000\n",
      "Epoch [44], Train Loss: 0.1599, Val Loss: 0.2926, LR: 0.10000\n",
      "Epoch [45], Train Loss: 0.1580, Val Loss: 0.3025, LR: 0.10000\n",
      "Epoch [46], Train Loss: 0.1656, Val Loss: 0.2767, LR: 0.10000\n",
      "Epoch [47], Train Loss: 0.1575, Val Loss: 0.3091, LR: 0.10000\n",
      "Epoch [48], Train Loss: 0.1604, Val Loss: 0.2902, LR: 0.10000\n",
      "Epoch [49], Train Loss: 0.1551, Val Loss: 0.2887, LR: 0.10000\n",
      "Epoch [50], Train Loss: 0.1541, Val Loss: 0.2511, LR: 0.10000\n",
      "Epoch [51], Train Loss: 0.1616, Val Loss: 0.2705, LR: 0.10000\n",
      "Epoch [52], Train Loss: 0.1617, Val Loss: 0.2799, LR: 0.10000\n",
      "Epoch [53], Train Loss: 0.1565, Val Loss: 0.2863, LR: 0.10000\n",
      "Epoch [54], Train Loss: 0.1565, Val Loss: 0.2455, LR: 0.10000\n",
      "Epoch [55], Train Loss: 0.1565, Val Loss: 0.2773, LR: 0.10000\n",
      "Epoch [56], Train Loss: 0.1519, Val Loss: 0.2724, LR: 0.10000\n",
      "Epoch [57], Train Loss: 0.1523, Val Loss: 0.2498, LR: 0.10000\n",
      "Epoch [58], Train Loss: 0.1547, Val Loss: 0.2631, LR: 0.10000\n",
      "Epoch [59], Train Loss: 0.1548, Val Loss: 0.2544, LR: 0.10000\n",
      "Epoch [60], Train Loss: 0.1529, Val Loss: 0.2473, LR: 0.10000\n",
      "Epoch [61], Train Loss: 0.1545, Val Loss: 0.2580, LR: 0.10000\n",
      "Epoch [62], Train Loss: 0.1534, Val Loss: 0.2145, LR: 0.10000\n",
      "Epoch [63], Train Loss: 0.1544, Val Loss: 0.2413, LR: 0.10000\n",
      "Epoch [64], Train Loss: 0.1483, Val Loss: 0.2422, LR: 0.10000\n",
      "Epoch [65], Train Loss: 0.1535, Val Loss: 0.2473, LR: 0.10000\n",
      "Epoch [66], Train Loss: 0.1536, Val Loss: 0.2426, LR: 0.10000\n",
      "Epoch [67], Train Loss: 0.1481, Val Loss: 0.2273, LR: 0.10000\n",
      "Epoch [68], Train Loss: 0.1545, Val Loss: 0.2401, LR: 0.10000\n",
      "Epoch [69], Train Loss: 0.1483, Val Loss: 0.2272, LR: 0.10000\n",
      "Epoch [70], Train Loss: 0.1508, Val Loss: 0.2219, LR: 0.10000\n",
      "Epoch [71], Train Loss: 0.1455, Val Loss: 0.2315, LR: 0.10000\n",
      "Epoch [72], Train Loss: 0.1500, Val Loss: 0.2408, LR: 0.10000\n",
      "Epoch [73], Train Loss: 0.1505, Val Loss: 0.2127, LR: 0.10000\n",
      "Epoch [74], Train Loss: 0.1491, Val Loss: 0.2351, LR: 0.10000\n",
      "Epoch [75], Train Loss: 0.1474, Val Loss: 0.2255, LR: 0.10000\n",
      "Epoch [76], Train Loss: 0.1463, Val Loss: 0.2213, LR: 0.10000\n",
      "Epoch [77], Train Loss: 0.1433, Val Loss: 0.2361, LR: 0.10000\n",
      "Epoch [78], Train Loss: 0.1475, Val Loss: 0.2290, LR: 0.10000\n",
      "Epoch [79], Train Loss: 0.1504, Val Loss: 0.2166, LR: 0.10000\n",
      "Epoch [80], Train Loss: 0.1448, Val Loss: 0.2040, LR: 0.10000\n",
      "Epoch [81], Train Loss: 0.1462, Val Loss: 0.2196, LR: 0.10000\n",
      "Epoch [82], Train Loss: 0.1459, Val Loss: 0.2269, LR: 0.10000\n",
      "Epoch [83], Train Loss: 0.1467, Val Loss: 0.2057, LR: 0.10000\n",
      "Epoch [84], Train Loss: 0.1468, Val Loss: 0.1851, LR: 0.10000\n",
      "Epoch [85], Train Loss: 0.1463, Val Loss: 0.2099, LR: 0.10000\n",
      "Epoch [86], Train Loss: 0.1458, Val Loss: 0.1963, LR: 0.10000\n",
      "Epoch [87], Train Loss: 0.1467, Val Loss: 0.1973, LR: 0.10000\n",
      "Epoch [88], Train Loss: 0.1489, Val Loss: 0.1990, LR: 0.10000\n",
      "Epoch [89], Train Loss: 0.1439, Val Loss: 0.1880, LR: 0.10000\n",
      "Epoch [90], Train Loss: 0.1496, Val Loss: 0.1731, LR: 0.10000\n",
      "Epoch [91], Train Loss: 0.1404, Val Loss: 0.1940, LR: 0.10000\n",
      "Epoch [92], Train Loss: 0.1464, Val Loss: 0.2024, LR: 0.10000\n",
      "Epoch [93], Train Loss: 0.1422, Val Loss: 0.1828, LR: 0.10000\n",
      "Epoch [94], Train Loss: 0.1450, Val Loss: 0.1792, LR: 0.10000\n",
      "Epoch [95], Train Loss: 0.1444, Val Loss: 0.1684, LR: 0.10000\n",
      "Epoch [96], Train Loss: 0.1408, Val Loss: 0.1723, LR: 0.10000\n",
      "Epoch [97], Train Loss: 0.1414, Val Loss: 0.2029, LR: 0.10000\n",
      "Epoch [98], Train Loss: 0.1422, Val Loss: 0.1810, LR: 0.10000\n",
      "Epoch [99], Train Loss: 0.1413, Val Loss: 0.1987, LR: 0.10000\n",
      "Epoch [100], Train Loss: 0.1475, Val Loss: 0.1754, LR: 0.10000\n",
      "Epoch [101], Train Loss: 0.1384, Val Loss: 0.1763, LR: 0.10000\n",
      "Epoch [102], Train Loss: 0.1412, Val Loss: 0.1653, LR: 0.10000\n",
      "Epoch [103], Train Loss: 0.1429, Val Loss: 0.1843, LR: 0.10000\n",
      "Epoch [104], Train Loss: 0.1392, Val Loss: 0.1895, LR: 0.10000\n",
      "Epoch [105], Train Loss: 0.1436, Val Loss: 0.1814, LR: 0.10000\n",
      "Epoch [106], Train Loss: 0.1422, Val Loss: 0.1836, LR: 0.10000\n",
      "Epoch [107], Train Loss: 0.1387, Val Loss: 0.1788, LR: 0.10000\n",
      "Epoch [108], Train Loss: 0.1413, Val Loss: 0.1814, LR: 0.10000\n",
      "Epoch [109], Train Loss: 0.1414, Val Loss: 0.1750, LR: 0.10000\n",
      "Epoch [110], Train Loss: 0.1379, Val Loss: 0.1741, LR: 0.10000\n",
      "Epoch [111], Train Loss: 0.1361, Val Loss: 0.1615, LR: 0.10000\n",
      "Epoch [112], Train Loss: 0.1379, Val Loss: 0.1732, LR: 0.10000\n",
      "Epoch [113], Train Loss: 0.1409, Val Loss: 0.1659, LR: 0.10000\n",
      "Epoch [114], Train Loss: 0.1383, Val Loss: 0.1789, LR: 0.10000\n",
      "Epoch [115], Train Loss: 0.1350, Val Loss: 0.1735, LR: 0.10000\n",
      "Epoch [116], Train Loss: 0.1405, Val Loss: 0.1763, LR: 0.10000\n",
      "Epoch [117], Train Loss: 0.1371, Val Loss: 0.1813, LR: 0.10000\n",
      "Epoch [118], Train Loss: 0.1426, Val Loss: 0.1622, LR: 0.10000\n",
      "Epoch [119], Train Loss: 0.1351, Val Loss: 0.1748, LR: 0.10000\n",
      "Epoch [120], Train Loss: 0.1337, Val Loss: 0.1614, LR: 0.10000\n",
      "Epoch [121], Train Loss: 0.1368, Val Loss: 0.1620, LR: 0.10000\n",
      "Epoch [122], Train Loss: 0.1363, Val Loss: 0.1585, LR: 0.10000\n",
      "Epoch [123], Train Loss: 0.1442, Val Loss: 0.1656, LR: 0.10000\n",
      "Epoch [124], Train Loss: 0.1376, Val Loss: 0.1772, LR: 0.10000\n",
      "Epoch [125], Train Loss: 0.1397, Val Loss: 0.1635, LR: 0.10000\n",
      "Epoch [126], Train Loss: 0.1342, Val Loss: 0.1638, LR: 0.10000\n",
      "Epoch [127], Train Loss: 0.1355, Val Loss: 0.1469, LR: 0.10000\n",
      "Epoch [128], Train Loss: 0.1391, Val Loss: 0.1642, LR: 0.10000\n",
      "Epoch [129], Train Loss: 0.1312, Val Loss: 0.1622, LR: 0.10000\n",
      "Epoch [130], Train Loss: 0.1346, Val Loss: 0.1531, LR: 0.10000\n",
      "Epoch [131], Train Loss: 0.1332, Val Loss: 0.1572, LR: 0.10000\n",
      "Epoch [132], Train Loss: 0.1342, Val Loss: 0.1502, LR: 0.10000\n",
      "Epoch [133], Train Loss: 0.1322, Val Loss: 0.1472, LR: 0.10000\n",
      "Epoch [134], Train Loss: 0.1334, Val Loss: 0.1717, LR: 0.10000\n",
      "Epoch [135], Train Loss: 0.1357, Val Loss: 0.1535, LR: 0.10000\n",
      "Epoch [136], Train Loss: 0.1376, Val Loss: 0.1495, LR: 0.10000\n",
      "Epoch [137], Train Loss: 0.1339, Val Loss: 0.1552, LR: 0.10000\n",
      "Epoch [138], Train Loss: 0.1350, Val Loss: 0.1666, LR: 0.10000\n",
      "Epoch [139], Train Loss: 0.1359, Val Loss: 0.1556, LR: 0.10000\n",
      "Epoch [140], Train Loss: 0.1311, Val Loss: 0.1422, LR: 0.10000\n",
      "Epoch [141], Train Loss: 0.1333, Val Loss: 0.1538, LR: 0.10000\n",
      "Epoch [142], Train Loss: 0.1353, Val Loss: 0.1462, LR: 0.10000\n",
      "Epoch [143], Train Loss: 0.1336, Val Loss: 0.1561, LR: 0.10000\n",
      "Epoch [144], Train Loss: 0.1363, Val Loss: 0.1354, LR: 0.10000\n",
      "Epoch [145], Train Loss: 0.1318, Val Loss: 0.1543, LR: 0.10000\n",
      "Epoch [146], Train Loss: 0.1347, Val Loss: 0.1402, LR: 0.10000\n",
      "Epoch [147], Train Loss: 0.1307, Val Loss: 0.1404, LR: 0.10000\n",
      "Epoch [148], Train Loss: 0.1338, Val Loss: 0.1481, LR: 0.10000\n",
      "Epoch [149], Train Loss: 0.1305, Val Loss: 0.1390, LR: 0.10000\n",
      "Epoch [150], Train Loss: 0.1311, Val Loss: 0.1342, LR: 0.10000\n",
      "Epoch [151], Train Loss: 0.1359, Val Loss: 0.1539, LR: 0.10000\n",
      "Epoch [152], Train Loss: 0.1300, Val Loss: 0.1415, LR: 0.10000\n",
      "Epoch [153], Train Loss: 0.1302, Val Loss: 0.1437, LR: 0.10000\n",
      "Epoch [154], Train Loss: 0.1318, Val Loss: 0.1441, LR: 0.10000\n",
      "Epoch [155], Train Loss: 0.1313, Val Loss: 0.1421, LR: 0.10000\n",
      "Epoch [156], Train Loss: 0.1290, Val Loss: 0.1460, LR: 0.10000\n",
      "Epoch [157], Train Loss: 0.1289, Val Loss: 0.1405, LR: 0.10000\n",
      "Epoch [158], Train Loss: 0.1296, Val Loss: 0.1513, LR: 0.10000\n",
      "Epoch [159], Train Loss: 0.1312, Val Loss: 0.1501, LR: 0.10000\n",
      "Epoch [160], Train Loss: 0.1308, Val Loss: 0.1343, LR: 0.10000\n",
      "Epoch [161], Train Loss: 0.1300, Val Loss: 0.1500, LR: 0.10000\n",
      "Epoch [162], Train Loss: 0.1278, Val Loss: 0.1267, LR: 0.10000\n",
      "Epoch [163], Train Loss: 0.1249, Val Loss: 0.1392, LR: 0.10000\n",
      "Epoch [164], Train Loss: 0.1310, Val Loss: 0.1526, LR: 0.10000\n",
      "Epoch [165], Train Loss: 0.1280, Val Loss: 0.1190, LR: 0.10000\n",
      "Epoch [166], Train Loss: 0.1291, Val Loss: 0.1377, LR: 0.10000\n",
      "Epoch [167], Train Loss: 0.1306, Val Loss: 0.1321, LR: 0.10000\n",
      "Epoch [168], Train Loss: 0.1262, Val Loss: 0.1295, LR: 0.10000\n",
      "Epoch [169], Train Loss: 0.1284, Val Loss: 0.1364, LR: 0.10000\n",
      "Epoch [170], Train Loss: 0.1268, Val Loss: 0.1430, LR: 0.10000\n",
      "Epoch [171], Train Loss: 0.1268, Val Loss: 0.1561, LR: 0.10000\n",
      "Epoch [172], Train Loss: 0.1284, Val Loss: 0.1282, LR: 0.10000\n",
      "Epoch [173], Train Loss: 0.1298, Val Loss: 0.1258, LR: 0.10000\n",
      "Epoch [174], Train Loss: 0.1280, Val Loss: 0.1241, LR: 0.10000\n",
      "Epoch [175], Train Loss: 0.1303, Val Loss: 0.1289, LR: 0.10000\n",
      "Epoch [176], Train Loss: 0.1265, Val Loss: 0.1313, LR: 0.10000\n",
      "Epoch [177], Train Loss: 0.1284, Val Loss: 0.1208, LR: 0.10000\n",
      "Epoch [178], Train Loss: 0.1280, Val Loss: 0.1285, LR: 0.10000\n",
      "Epoch [179], Train Loss: 0.1260, Val Loss: 0.1292, LR: 0.10000\n",
      "Epoch [180], Train Loss: 0.1253, Val Loss: 0.1421, LR: 0.10000\n",
      "Epoch [181], Train Loss: 0.1237, Val Loss: 0.1424, LR: 0.05000\n",
      "Epoch [182], Train Loss: 0.1238, Val Loss: 0.1345, LR: 0.05000\n",
      "Epoch [183], Train Loss: 0.1255, Val Loss: 0.1219, LR: 0.05000\n",
      "Epoch [184], Train Loss: 0.1257, Val Loss: 0.1305, LR: 0.05000\n",
      "Epoch [185], Train Loss: 0.1305, Val Loss: 0.1329, LR: 0.05000\n",
      "Early stopping at epoch 185\n",
      "✅ Run 1: Acc = 74.30%, MDE = 1.1932\n",
      "\n",
      "=== Run 2/5 ===\n",
      "Epoch [1], Train Loss: 0.2750, Val Loss: 0.6369, LR: 0.10000\n",
      "Epoch [2], Train Loss: 0.2409, Val Loss: 0.6034, LR: 0.10000\n",
      "Epoch [3], Train Loss: 0.2339, Val Loss: 0.4746, LR: 0.10000\n",
      "Epoch [4], Train Loss: 0.2229, Val Loss: 0.4676, LR: 0.10000\n",
      "Epoch [5], Train Loss: 0.2229, Val Loss: 0.4678, LR: 0.10000\n",
      "Epoch [6], Train Loss: 0.2105, Val Loss: 0.4448, LR: 0.10000\n",
      "Epoch [7], Train Loss: 0.2078, Val Loss: 0.4760, LR: 0.10000\n",
      "Epoch [8], Train Loss: 0.1998, Val Loss: 0.4636, LR: 0.10000\n",
      "Epoch [9], Train Loss: 0.2010, Val Loss: 0.4867, LR: 0.10000\n",
      "Epoch [10], Train Loss: 0.1993, Val Loss: 0.5179, LR: 0.10000\n",
      "Epoch [11], Train Loss: 0.1930, Val Loss: 0.4031, LR: 0.10000\n",
      "Epoch [12], Train Loss: 0.1949, Val Loss: 0.4069, LR: 0.10000\n",
      "Epoch [13], Train Loss: 0.1931, Val Loss: 0.3884, LR: 0.10000\n",
      "Epoch [14], Train Loss: 0.1877, Val Loss: 0.4119, LR: 0.10000\n",
      "Epoch [15], Train Loss: 0.1846, Val Loss: 0.4387, LR: 0.10000\n",
      "Epoch [16], Train Loss: 0.1842, Val Loss: 0.3766, LR: 0.10000\n",
      "Epoch [17], Train Loss: 0.1830, Val Loss: 0.4425, LR: 0.10000\n",
      "Epoch [18], Train Loss: 0.1808, Val Loss: 0.3967, LR: 0.10000\n",
      "Epoch [19], Train Loss: 0.1804, Val Loss: 0.4193, LR: 0.10000\n",
      "Epoch [20], Train Loss: 0.1797, Val Loss: 0.4135, LR: 0.10000\n",
      "Epoch [21], Train Loss: 0.1738, Val Loss: 0.3730, LR: 0.10000\n",
      "Epoch [22], Train Loss: 0.1774, Val Loss: 0.3676, LR: 0.10000\n",
      "Epoch [23], Train Loss: 0.1794, Val Loss: 0.3664, LR: 0.10000\n",
      "Epoch [24], Train Loss: 0.1758, Val Loss: 0.3964, LR: 0.10000\n",
      "Epoch [25], Train Loss: 0.1729, Val Loss: 0.3965, LR: 0.10000\n",
      "Epoch [26], Train Loss: 0.1736, Val Loss: 0.3345, LR: 0.10000\n",
      "Epoch [27], Train Loss: 0.1711, Val Loss: 0.3554, LR: 0.10000\n",
      "Epoch [28], Train Loss: 0.1697, Val Loss: 0.3491, LR: 0.10000\n",
      "Epoch [29], Train Loss: 0.1743, Val Loss: 0.3091, LR: 0.10000\n",
      "Epoch [30], Train Loss: 0.1677, Val Loss: 0.3237, LR: 0.10000\n",
      "Epoch [31], Train Loss: 0.1721, Val Loss: 0.3653, LR: 0.10000\n",
      "Epoch [32], Train Loss: 0.1696, Val Loss: 0.3668, LR: 0.10000\n",
      "Epoch [33], Train Loss: 0.1707, Val Loss: 0.3154, LR: 0.10000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 189\u001b[0m\n\u001b[1;32m    187\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m    188\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m--> 189\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    190\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mx1\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzero_grad\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m~/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m~/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[44], line 128\u001b[0m, in \u001b[0;36mCSIAllPairsDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    126\u001b[0m x1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mX[i]\n\u001b[1;32m    127\u001b[0m x2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfingerprints[fp_label]\n\u001b[0;32m--> 128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m x1, x2, \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfloat32\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def eval_siamese_acc(model, data_loader, margin=4.0):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, label in data_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            distance = model(x1, x2)\n",
    "            pred = (distance < margin).float()  # 距離小於 margin 視為同類\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            print(f\"Warning: Label {true_label} or {pred_label} not in coordinates, skipping.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        errors.append(np.linalg.norm(pred_coord - true_coord))\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "\n",
    "\n",
    "COORDINATES = {\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# ==== 1. 模型定義 ====\n",
    "\n",
    "class SiameseSubNet1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(48, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1(self.fc1(x)))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.bn2(self.fc2(x)))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SiameseNetwork1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.subnet = SiameseSubNet1D()\n",
    "    def forward(self, x1, x2):\n",
    "        out1 = self.subnet(x1)\n",
    "        out2 = self.subnet(x2)\n",
    "        distance = F.pairwise_distance(out1, out2)\n",
    "        return distance\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    def __init__(self, margin=4.0):\n",
    "        super().__init__()\n",
    "        self.margin = margin\n",
    "    def forward(self, output, label):\n",
    "        loss = 0.5 * label * torch.pow(output, 2) + \\\n",
    "               0.5 * (1 - label) * torch.pow(torch.clamp(self.margin - output, min=0.0), 2)\n",
    "        return loss.mean()\n",
    "\n",
    "# ==== 2. Dataset 全pair實作 ====\n",
    "\n",
    "class CSIAllPairsDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, X, y, T1):\n",
    "        self.X = torch.tensor(X, dtype=torch.float32)\n",
    "        self.y = np.array(y)\n",
    "        self.label_set = np.unique(self.y)\n",
    "        self.T1 = T1\n",
    "        self.idx_by_label = {label: np.where(self.y == label)[0][:T1] for label in self.label_set}\n",
    "        self.fingerprints = {}\n",
    "        for label in self.label_set:\n",
    "            idxs = self.idx_by_label[label]\n",
    "            self.fingerprints[label] = self.X[idxs].mean(dim=0)\n",
    "        self.pairs = []\n",
    "        for rp in self.label_set:\n",
    "            idxs = self.idx_by_label[rp]\n",
    "            for i in idxs:\n",
    "                for fp_rp in self.label_set:\n",
    "                    label = 1 if rp == fp_rp else 0\n",
    "                    self.pairs.append((i, fp_rp, label))\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)\n",
    "    def __getitem__(self, idx):\n",
    "        i, fp_label, label = self.pairs[idx]\n",
    "        x1 = self.X[i]\n",
    "        x2 = self.fingerprints[fp_label]\n",
    "        return x1, x2, torch.tensor(label, dtype=torch.float32)\n",
    "\n",
    "# ==== 3. 評估函數 ====\n",
    "\n",
    "def eval_siamese_acc(model, data_loader, margin=4.0):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for x1, x2, label in data_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            distance = model(x1, x2)\n",
    "            pred = (distance < margin).float()\n",
    "            correct += (pred == label).sum().item()\n",
    "            total += label.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        errors.append(np.linalg.norm(pred_coord - true_coord))\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "# ==== 4. 訓練參數與資料準備 ====\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "num_runs = 5\n",
    "epochs = 200\n",
    "patience = 20\n",
    "T1 = 7\n",
    "margin = 4.0\n",
    "\n",
    "\n",
    "train_dataset = CSIAllPairsDataset(amp_train, y_train, T1)\n",
    "val_dataset = CSIAllPairsDataset(amp_val, y_val, T1)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "test_accs, test_mdes, all_run_errors = [], [], []\n",
    "os.makedirs(\"siamese_results\", exist_ok=True)\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    print(f\"\\n=== Run {run}/{num_runs} ===\")\n",
    "\n",
    "    model = SiameseNetwork1D().to(device)\n",
    "    optimizer = optim.Adadelta(model.parameters(), lr=0.1, rho=0.95, eps=1e-8)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=15, min_lr=1e-6, verbose=True)\n",
    "    criterion = ContrastiveLoss(margin=4.0)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "    best_state = None\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for x1, x2, label in train_loader:\n",
    "            x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            distance = model(x1, x2)\n",
    "            loss = criterion(distance, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "\n",
    "        # === 驗證（以 contrastive loss 作 early stop/lr decay依據） ===\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for x1, x2, label in val_loader:\n",
    "                x1, x2, label = x1.to(device), x2.to(device), label.to(device)\n",
    "                distance = model(x1, x2)\n",
    "                val_loss += criterion(distance, label).item() * x1.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch [{epoch+1}], Train Loss: {avg_loss:.4f}, Val Loss: {val_loss:.4f}, LR: {optimizer.param_groups[0]['lr']:.5f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_state = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}\")\n",
    "                break\n",
    "\n",
    "    # ==== 測試階段：fingerprint-based 位置預測與 MDE ====\n",
    "    model.load_state_dict(best_state)\n",
    "    model.eval()\n",
    "    # 準備 fingerprint\n",
    "    fingerprints = []\n",
    "    rp_list = np.unique(y_train)\n",
    "    for rp in rp_list:\n",
    "        idxs = np.where(y_train == rp)[0][:T1]\n",
    "        fp = torch.tensor(amp_train[idxs].mean(axis=0), dtype=torch.float32).to(device)\n",
    "        fingerprints.append(fp)\n",
    "    fingerprints = torch.stack(fingerprints)\n",
    "\n",
    "    all_true, all_pred = [], []\n",
    "    for i in range(len(amp_test)):\n",
    "        test_csi = torch.tensor(amp_test[i], dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        min_dist = float('inf')\n",
    "        pred_rp = None\n",
    "        with torch.no_grad():\n",
    "            for j, fp in enumerate(fingerprints):\n",
    "                dist = model.subnet(test_csi).sub(model.subnet(fp.unsqueeze(0))).norm(p=2).item()\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    pred_rp = rp_list[j]\n",
    "        all_true.append(y_test[i])\n",
    "        all_pred.append(pred_rp)\n",
    "    y_true = np.array(all_true)\n",
    "    y_pred = np.array(all_pred)\n",
    "    acc = 100 * np.mean(y_true == y_pred)\n",
    "    mde, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "    all_run_errors.append(errors)\n",
    "    test_accs.append(acc)\n",
    "    test_mdes.append(mde)\n",
    "    print(f\"✅ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "# ==== 儲存 summary 結果 ====\n",
    "df = pd.DataFrame({\n",
    "    \"run\": list(range(1, num_runs+1)),\n",
    "    \"accuracy\": test_accs,\n",
    "    \"mde\": test_mdes\n",
    "})\n",
    "df.to_csv(\"siamese_results/siamese_results.csv\", index=False)\n",
    "\n",
    "error_records = []\n",
    "for run_idx, errors in enumerate(all_run_errors):\n",
    "    for sample_idx, e in enumerate(errors):\n",
    "        error_records.append({\n",
    "            \"run\": run_idx + 1,\n",
    "            \"sample_idx\": sample_idx + 1,\n",
    "            \"error\": e\n",
    "        })\n",
    "df_errors = pd.DataFrame(error_records)\n",
    "df_errors.to_csv(\"siamese_results/siamese_all_errors.csv\", index=False)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
