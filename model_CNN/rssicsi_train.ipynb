{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for RSSI + CSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csidataset import *\n",
    "import data_loader\n",
    "from data_loader import *\n",
    "sys.path.append(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool\")\n",
    "import denoise\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/1223_phone/5G/20MHz/csv/all\"\n",
    "rssi_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/RSSI/timestamp_allignment_Balanced_2024_12_14_rtt_logs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data CSI\n",
    "reference_points = {}\n",
    "spacing = 0.6  # 每隔 0.6m\n",
    "\n",
    "for ref_id, coord in data_loader.COORDINATES.items():\n",
    "    folder_path = os.path.join(csi_path, f\"reference_point_{ref_id}.xlsx\")\n",
    "    reference_points[folder_path] = (ref_id, coord)\n",
    "\n",
    "data, rp_labels, coord_labels = load_data(reference_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>893.573164</td>\n",
       "      <td>888.473973</td>\n",
       "      <td>871.023536</td>\n",
       "      <td>836.316328</td>\n",
       "      <td>841.770753</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387518</td>\n",
       "      <td>-1.581549</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>-2.080869</td>\n",
       "      <td>-2.287379</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>672.521375</td>\n",
       "      <td>687.674342</td>\n",
       "      <td>780.946221</td>\n",
       "      <td>803.560203</td>\n",
       "      <td>863.103702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.677159</td>\n",
       "      <td>-2.877730</td>\n",
       "      <td>-3.051501</td>\n",
       "      <td>2.738820</td>\n",
       "      <td>2.503722</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>713.950278</td>\n",
       "      <td>736.619983</td>\n",
       "      <td>852.877482</td>\n",
       "      <td>846.442556</td>\n",
       "      <td>880.032954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176051</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.251636</td>\n",
       "      <td>-0.776139</td>\n",
       "      <td>-1.023850</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>800.870776</td>\n",
       "      <td>791.982323</td>\n",
       "      <td>778.923616</td>\n",
       "      <td>763.872372</td>\n",
       "      <td>766.167084</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.031820</td>\n",
       "      <td>-1.194379</td>\n",
       "      <td>-1.338067</td>\n",
       "      <td>-1.666445</td>\n",
       "      <td>-1.878363</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>875.416472</td>\n",
       "      <td>878.300632</td>\n",
       "      <td>854.687077</td>\n",
       "      <td>857.886939</td>\n",
       "      <td>843.664033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445534</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-1.090942</td>\n",
       "      <td>-1.286103</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>397.615392</td>\n",
       "      <td>416.889674</td>\n",
       "      <td>395.588170</td>\n",
       "      <td>413.019370</td>\n",
       "      <td>382.733589</td>\n",
       "      <td>381.136459</td>\n",
       "      <td>393.814677</td>\n",
       "      <td>434.337426</td>\n",
       "      <td>465.842248</td>\n",
       "      <td>483.735465</td>\n",
       "      <td>...</td>\n",
       "      <td>2.183710</td>\n",
       "      <td>2.005190</td>\n",
       "      <td>1.826107</td>\n",
       "      <td>1.364322</td>\n",
       "      <td>1.150287</td>\n",
       "      <td>0.872978</td>\n",
       "      <td>0.573063</td>\n",
       "      <td>0.255977</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24496</th>\n",
       "      <td>643.708785</td>\n",
       "      <td>646.260010</td>\n",
       "      <td>627.624888</td>\n",
       "      <td>621.498994</td>\n",
       "      <td>605.152873</td>\n",
       "      <td>628.458431</td>\n",
       "      <td>617.272225</td>\n",
       "      <td>592.763865</td>\n",
       "      <td>581.986254</td>\n",
       "      <td>587.911558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494557</td>\n",
       "      <td>-0.782703</td>\n",
       "      <td>-1.053078</td>\n",
       "      <td>-1.631902</td>\n",
       "      <td>-1.969954</td>\n",
       "      <td>-2.328378</td>\n",
       "      <td>-2.758960</td>\n",
       "      <td>3.087458</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24497</th>\n",
       "      <td>762.685387</td>\n",
       "      <td>745.553486</td>\n",
       "      <td>706.884007</td>\n",
       "      <td>695.708272</td>\n",
       "      <td>698.951357</td>\n",
       "      <td>710.169698</td>\n",
       "      <td>720.184004</td>\n",
       "      <td>675.370269</td>\n",
       "      <td>654.120020</td>\n",
       "      <td>669.209982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621717</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.356470</td>\n",
       "      <td>-0.131807</td>\n",
       "      <td>-0.426306</td>\n",
       "      <td>-0.748795</td>\n",
       "      <td>-1.031706</td>\n",
       "      <td>-1.339351</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>401.220638</td>\n",
       "      <td>416.688133</td>\n",
       "      <td>388.561449</td>\n",
       "      <td>390.436935</td>\n",
       "      <td>370.411933</td>\n",
       "      <td>382.498366</td>\n",
       "      <td>394.452786</td>\n",
       "      <td>436.664631</td>\n",
       "      <td>446.643034</td>\n",
       "      <td>495.258518</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.801749</td>\n",
       "      <td>-2.120022</td>\n",
       "      <td>-2.410138</td>\n",
       "      <td>3.138034</td>\n",
       "      <td>2.751942</td>\n",
       "      <td>2.306627</td>\n",
       "      <td>1.868269</td>\n",
       "      <td>1.432071</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>753.160010</td>\n",
       "      <td>739.957431</td>\n",
       "      <td>724.996552</td>\n",
       "      <td>704.264865</td>\n",
       "      <td>701.806954</td>\n",
       "      <td>717.674717</td>\n",
       "      <td>715.455100</td>\n",
       "      <td>674.985185</td>\n",
       "      <td>658.325907</td>\n",
       "      <td>679.605768</td>\n",
       "      <td>...</td>\n",
       "      <td>1.746490</td>\n",
       "      <td>1.586924</td>\n",
       "      <td>1.427713</td>\n",
       "      <td>1.118018</td>\n",
       "      <td>0.870126</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24500 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "0      919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
       "1      698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
       "2      745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
       "3      830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
       "4      906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "24495  397.615392  416.889674  395.588170  413.019370  382.733589  381.136459   \n",
       "24496  643.708785  646.260010  627.624888  621.498994  605.152873  628.458431   \n",
       "24497  762.685387  745.553486  706.884007  695.708272  698.951357  710.169698   \n",
       "24498  401.220638  416.688133  388.561449  390.436935  370.411933  382.498366   \n",
       "24499  753.160010  739.957431  724.996552  704.264865  701.806954  717.674717   \n",
       "\n",
       "               6           7           8           9   ...        88  \\\n",
       "0      888.473973  871.023536  836.316328  841.770753  ... -1.387518   \n",
       "1      687.674342  780.946221  803.560203  863.103702  ... -2.677159   \n",
       "2      736.619983  852.877482  846.442556  880.032954  ...  0.176051   \n",
       "3      791.982323  778.923616  763.872372  766.167084  ... -1.031820   \n",
       "4      878.300632  854.687077  857.886939  843.664033  ... -0.445534   \n",
       "...           ...         ...         ...         ...  ...       ...   \n",
       "24495  393.814677  434.337426  465.842248  483.735465  ...  2.183710   \n",
       "24496  617.272225  592.763865  581.986254  587.911558  ... -0.494557   \n",
       "24497  720.184004  675.370269  654.120020  669.209982  ...  0.621717   \n",
       "24498  394.452786  436.664631  446.643034  495.258518  ... -1.801749   \n",
       "24499  715.455100  674.985185  658.325907  679.605768  ...  1.746490   \n",
       "\n",
       "             89        90        91        92        93        94        95  \\\n",
       "0     -1.581549 -1.708760 -2.080869 -2.287379 -2.467120 -2.666997 -2.982241   \n",
       "1     -2.877730 -3.051501  2.738820  2.503722  2.256683  2.035376  1.737005   \n",
       "2     -0.063704 -0.251636 -0.776139 -1.023850 -1.275624 -1.535994 -1.899522   \n",
       "3     -1.194379 -1.338067 -1.666445 -1.878363 -2.100073 -2.369892 -2.666982   \n",
       "4     -0.593750 -0.783166 -1.090942 -1.286103 -1.497279 -1.784787 -2.076610   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24495  2.005190  1.826107  1.364322  1.150287  0.872978  0.573063  0.255977   \n",
       "24496 -0.782703 -1.053078 -1.631902 -1.969954 -2.328378 -2.758960  3.087458   \n",
       "24497  0.498336  0.356470 -0.131807 -0.426306 -0.748795 -1.031706 -1.339351   \n",
       "24498 -2.120022 -2.410138  3.138034  2.751942  2.306627  1.868269  1.432071   \n",
       "24499  1.586924  1.427713  1.118018  0.870126  0.650771  0.353386  0.054227   \n",
       "\n",
       "         96     97  \n",
       "0     -50.0  136.0  \n",
       "1     -49.0  148.0  \n",
       "2     -50.0  224.0  \n",
       "3     -51.0  136.0  \n",
       "4     -50.0  136.0  \n",
       "...     ...    ...  \n",
       "24495 -56.0  148.0  \n",
       "24496 -59.0  136.0  \n",
       "24497 -58.0  136.0  \n",
       "24498 -56.0  148.0  \n",
       "24499 -58.0  136.0  \n",
       "\n",
       "[24500 rows x 98 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi[\"Label\"] = rp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>893.573164</td>\n",
       "      <td>888.473973</td>\n",
       "      <td>871.023536</td>\n",
       "      <td>836.316328</td>\n",
       "      <td>841.770753</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.581549</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>-2.080869</td>\n",
       "      <td>-2.287379</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>672.521375</td>\n",
       "      <td>687.674342</td>\n",
       "      <td>780.946221</td>\n",
       "      <td>803.560203</td>\n",
       "      <td>863.103702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.877730</td>\n",
       "      <td>-3.051501</td>\n",
       "      <td>2.738820</td>\n",
       "      <td>2.503722</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>713.950278</td>\n",
       "      <td>736.619983</td>\n",
       "      <td>852.877482</td>\n",
       "      <td>846.442556</td>\n",
       "      <td>880.032954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.251636</td>\n",
       "      <td>-0.776139</td>\n",
       "      <td>-1.023850</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>800.870776</td>\n",
       "      <td>791.982323</td>\n",
       "      <td>778.923616</td>\n",
       "      <td>763.872372</td>\n",
       "      <td>766.167084</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.194379</td>\n",
       "      <td>-1.338067</td>\n",
       "      <td>-1.666445</td>\n",
       "      <td>-1.878363</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>875.416472</td>\n",
       "      <td>878.300632</td>\n",
       "      <td>854.687077</td>\n",
       "      <td>857.886939</td>\n",
       "      <td>843.664033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-1.090942</td>\n",
       "      <td>-1.286103</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>397.615392</td>\n",
       "      <td>416.889674</td>\n",
       "      <td>395.588170</td>\n",
       "      <td>413.019370</td>\n",
       "      <td>382.733589</td>\n",
       "      <td>381.136459</td>\n",
       "      <td>393.814677</td>\n",
       "      <td>434.337426</td>\n",
       "      <td>465.842248</td>\n",
       "      <td>483.735465</td>\n",
       "      <td>...</td>\n",
       "      <td>2.005190</td>\n",
       "      <td>1.826107</td>\n",
       "      <td>1.364322</td>\n",
       "      <td>1.150287</td>\n",
       "      <td>0.872978</td>\n",
       "      <td>0.573063</td>\n",
       "      <td>0.255977</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24496</th>\n",
       "      <td>643.708785</td>\n",
       "      <td>646.260010</td>\n",
       "      <td>627.624888</td>\n",
       "      <td>621.498994</td>\n",
       "      <td>605.152873</td>\n",
       "      <td>628.458431</td>\n",
       "      <td>617.272225</td>\n",
       "      <td>592.763865</td>\n",
       "      <td>581.986254</td>\n",
       "      <td>587.911558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.782703</td>\n",
       "      <td>-1.053078</td>\n",
       "      <td>-1.631902</td>\n",
       "      <td>-1.969954</td>\n",
       "      <td>-2.328378</td>\n",
       "      <td>-2.758960</td>\n",
       "      <td>3.087458</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24497</th>\n",
       "      <td>762.685387</td>\n",
       "      <td>745.553486</td>\n",
       "      <td>706.884007</td>\n",
       "      <td>695.708272</td>\n",
       "      <td>698.951357</td>\n",
       "      <td>710.169698</td>\n",
       "      <td>720.184004</td>\n",
       "      <td>675.370269</td>\n",
       "      <td>654.120020</td>\n",
       "      <td>669.209982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.356470</td>\n",
       "      <td>-0.131807</td>\n",
       "      <td>-0.426306</td>\n",
       "      <td>-0.748795</td>\n",
       "      <td>-1.031706</td>\n",
       "      <td>-1.339351</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>401.220638</td>\n",
       "      <td>416.688133</td>\n",
       "      <td>388.561449</td>\n",
       "      <td>390.436935</td>\n",
       "      <td>370.411933</td>\n",
       "      <td>382.498366</td>\n",
       "      <td>394.452786</td>\n",
       "      <td>436.664631</td>\n",
       "      <td>446.643034</td>\n",
       "      <td>495.258518</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.120022</td>\n",
       "      <td>-2.410138</td>\n",
       "      <td>3.138034</td>\n",
       "      <td>2.751942</td>\n",
       "      <td>2.306627</td>\n",
       "      <td>1.868269</td>\n",
       "      <td>1.432071</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>753.160010</td>\n",
       "      <td>739.957431</td>\n",
       "      <td>724.996552</td>\n",
       "      <td>704.264865</td>\n",
       "      <td>701.806954</td>\n",
       "      <td>717.674717</td>\n",
       "      <td>715.455100</td>\n",
       "      <td>674.985185</td>\n",
       "      <td>658.325907</td>\n",
       "      <td>679.605768</td>\n",
       "      <td>...</td>\n",
       "      <td>1.586924</td>\n",
       "      <td>1.427713</td>\n",
       "      <td>1.118018</td>\n",
       "      <td>0.870126</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24500 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "0      919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
       "1      698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
       "2      745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
       "3      830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
       "4      906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "24495  397.615392  416.889674  395.588170  413.019370  382.733589  381.136459   \n",
       "24496  643.708785  646.260010  627.624888  621.498994  605.152873  628.458431   \n",
       "24497  762.685387  745.553486  706.884007  695.708272  698.951357  710.169698   \n",
       "24498  401.220638  416.688133  388.561449  390.436935  370.411933  382.498366   \n",
       "24499  753.160010  739.957431  724.996552  704.264865  701.806954  717.674717   \n",
       "\n",
       "                6           7           8           9  ...        89  \\\n",
       "0      888.473973  871.023536  836.316328  841.770753  ... -1.581549   \n",
       "1      687.674342  780.946221  803.560203  863.103702  ... -2.877730   \n",
       "2      736.619983  852.877482  846.442556  880.032954  ... -0.063704   \n",
       "3      791.982323  778.923616  763.872372  766.167084  ... -1.194379   \n",
       "4      878.300632  854.687077  857.886939  843.664033  ... -0.593750   \n",
       "...           ...         ...         ...         ...  ...       ...   \n",
       "24495  393.814677  434.337426  465.842248  483.735465  ...  2.005190   \n",
       "24496  617.272225  592.763865  581.986254  587.911558  ... -0.782703   \n",
       "24497  720.184004  675.370269  654.120020  669.209982  ...  0.498336   \n",
       "24498  394.452786  436.664631  446.643034  495.258518  ... -2.120022   \n",
       "24499  715.455100  674.985185  658.325907  679.605768  ...  1.586924   \n",
       "\n",
       "             90        91        92        93        94        95    96  \\\n",
       "0     -1.708760 -2.080869 -2.287379 -2.467120 -2.666997 -2.982241 -50.0   \n",
       "1     -3.051501  2.738820  2.503722  2.256683  2.035376  1.737005 -49.0   \n",
       "2     -0.251636 -0.776139 -1.023850 -1.275624 -1.535994 -1.899522 -50.0   \n",
       "3     -1.338067 -1.666445 -1.878363 -2.100073 -2.369892 -2.666982 -51.0   \n",
       "4     -0.783166 -1.090942 -1.286103 -1.497279 -1.784787 -2.076610 -50.0   \n",
       "...         ...       ...       ...       ...       ...       ...   ...   \n",
       "24495  1.826107  1.364322  1.150287  0.872978  0.573063  0.255977 -56.0   \n",
       "24496 -1.053078 -1.631902 -1.969954 -2.328378 -2.758960  3.087458 -59.0   \n",
       "24497  0.356470 -0.131807 -0.426306 -0.748795 -1.031706 -1.339351 -58.0   \n",
       "24498 -2.410138  3.138034  2.751942  2.306627  1.868269  1.432071 -56.0   \n",
       "24499  1.427713  1.118018  0.870126  0.650771  0.353386  0.054227 -58.0   \n",
       "\n",
       "          97  Label  \n",
       "0      136.0      1  \n",
       "1      148.0      1  \n",
       "2      224.0      1  \n",
       "3      136.0      1  \n",
       "4      136.0      1  \n",
       "...      ...    ...  \n",
       "24495  148.0     49  \n",
       "24496  136.0     49  \n",
       "24497  136.0     49  \n",
       "24498  148.0     49  \n",
       "24499  136.0     49  \n",
       "\n",
       "[24500 rows x 99 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "csi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data RSSI\n",
    "\n",
    "label_to_coordinates_swapped = {\n",
    "    \"1-1\":  (0, 0),   \"1-2\":  (0, 0.6),   \"1-3\":  (0, 1.2),   \"1-4\":  (0, 1.8),\n",
    "    \"1-5\":  (0, 2.4), \"1-6\":  (0, 3.0),   \"1-7\":  (0, 3.6),   \"1-8\":  (0, 4.2),\n",
    "    \"1-9\":  (0, 4.8), \"1-10\": (0, 5.4),   \"1-11\": (0, 6.0),\n",
    "\n",
    "    \"2-1\":  (0.6, 0),  \"2-11\": (0.6, 6.0),\n",
    "\n",
    "    \"3-1\":  (1.2, 0),  \"3-11\": (1.2, 6.0),\n",
    "\n",
    "    \"4-1\":  (1.8, 0),  \"4-11\": (1.8, 6.0),\n",
    "\n",
    "    \"5-1\":  (2.4, 0),  \"5-11\": (2.4, 6.0),\n",
    "\n",
    "    \"6-1\":  (3.0, 0),  \"6-2\":  (3.0, 0.6),  \"6-3\":  (3.0, 1.2),  \"6-4\":  (3.0, 1.8),\n",
    "    \"6-5\":  (3.0, 2.4), \"6-6\":  (3.0, 3.0),  \"6-7\":  (3.0, 3.6),  \"6-8\":  (3.0, 4.2),\n",
    "    \"6-9\":  (3.0, 4.8), \"6-10\": (3.0, 5.4),  \"6-11\": (3.0, 6.0),\n",
    "\n",
    "    \"7-1\":  (3.6, 0),  \"7-11\": (3.6, 6.0),\n",
    "\n",
    "    \"8-1\":  (4.2, 0),  \"8-11\": (4.2, 6.0),\n",
    "\n",
    "    \"9-1\":  (4.8, 0),  \"9-11\": (4.8, 6.0),\n",
    "\n",
    "    \"10-1\": (5.4, 0),  \"10-11\": (5.4, 6.0),\n",
    "\n",
    "    \"11-1\": (6.0, 0),  \"11-2\": (6.0, 0.6), \"11-3\": (6.0, 1.2), \"11-4\": (6.0, 1.8),\n",
    "    \"11-5\": (6.0, 2.4),\"11-6\": (6.0, 3.0), \"11-7\": (6.0, 3.6), \"11-8\": (6.0, 4.2),\n",
    "    \"11-9\": (6.0, 4.8),\"11-10\":(6.0, 5.4), \"11-11\":(6.0, 6.0)\n",
    "}\n",
    "\n",
    "\n",
    "coordinates = {\n",
    "        1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "        36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "        2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "        6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "        12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "        16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "        20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "        22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "        26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "        41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "        44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "        47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "    }\n",
    "coordinate_to_label2 = {value: key for key, value in coordinates.items()}\n",
    "\n",
    "# label_to_coordinates:  \"1-1\" -> (0, 0)\n",
    "# coordinates:          1      -> (0, 0)\n",
    "\n",
    "# 1) 先建立一個「座標 -> 數字」的反轉字典\n",
    "coord_to_num = {value: key for key, value in coordinates.items()}\n",
    "# coord_to_num[(0,0)] = 1,  coord_to_num[(0.6,0)] = 40, ...\n",
    "\n",
    "# 2) 建立 \"1-1\" -> 整數標籤 的 map\n",
    "label_str_to_num = {}\n",
    "for str_label, coord in label_to_coordinates_swapped.items():\n",
    "    if coord in coord_to_num:  # 確保在座標字典中找得到\n",
    "        label_str_to_num[str_label] = coord_to_num[coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssi = pd.read_csv(rssi_path)\n",
    "\n",
    "rssi = rssi.drop(columns=['timeStemp', 'AP1_Distance (mm)', 'AP2_Distance (mm)', 'AP3_Distance (mm)', 'AP4_Distance (mm)', 'AP1_StdDev (mm)','AP2_StdDev (mm)', 'AP3_StdDev (mm)', 'AP4_StdDev (mm)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1-11\n",
       "1        1-11\n",
       "2        1-11\n",
       "3        1-11\n",
       "4        1-11\n",
       "         ... \n",
       "19644     7-1\n",
       "19645     7-1\n",
       "19646     7-1\n",
       "19647     7-1\n",
       "19648     7-1\n",
       "Name: Label, Length: 19649, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rssi[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssi[\"Label\"] = (\n",
    "    rssi[\"Label\"]\n",
    "    .map(label_to_coordinates_swapped)   # 由 \"1-1\" → (0,0)\n",
    "    .map(coordinate_to_label2)   # 由 (0,0) → 1 (或其他整數標籤)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>35</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>35</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>35</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>35</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>35</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19649 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
       "0         11     -60.0     -68.0     -61.0     -59.0\n",
       "1         11     -62.0     -70.0     -59.0     -59.0\n",
       "2         11     -61.0     -69.0     -62.0     -55.0\n",
       "3         11     -62.0     -69.0     -63.0     -55.0\n",
       "4         11     -61.0     -70.0     -62.0     -55.0\n",
       "...      ...       ...       ...       ...       ...\n",
       "19644     35     -64.0     -66.0     -61.0     -58.0\n",
       "19645     35     -67.0     -75.0     -62.0     -59.0\n",
       "19646     35     -64.0     -66.0     -61.0     -57.0\n",
       "19647     35     -66.0     -65.0     -59.0     -58.0\n",
       "19648     35     -68.0     -64.0     -61.0     -59.0\n",
       "\n",
       "[19649 rows x 5 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rssi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24500, 5)\n",
      "Label\n",
      "11    500\n",
      "10    500\n",
      "9     500\n",
      "8     500\n",
      "7     500\n",
      "6     500\n",
      "5     500\n",
      "4     500\n",
      "3     500\n",
      "2     500\n",
      "1     500\n",
      "40    500\n",
      "39    500\n",
      "38    500\n",
      "37    500\n",
      "36    500\n",
      "41    500\n",
      "42    500\n",
      "43    500\n",
      "44    500\n",
      "45    500\n",
      "46    500\n",
      "47    500\n",
      "48    500\n",
      "49    500\n",
      "16    500\n",
      "15    500\n",
      "14    500\n",
      "13    500\n",
      "12    500\n",
      "17    500\n",
      "18    500\n",
      "19    500\n",
      "20    500\n",
      "21    500\n",
      "22    500\n",
      "23    500\n",
      "24    500\n",
      "25    500\n",
      "26    500\n",
      "27    500\n",
      "28    500\n",
      "29    500\n",
      "30    500\n",
      "31    500\n",
      "32    500\n",
      "33    500\n",
      "34    500\n",
      "35    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 假設你的 RSSI DataFrame 名叫 df_rssi\n",
    "# 它至少包含欄位 [\"Label\", \"AP1_Rssi\", \"AP2_Rssi\", ...] 等資訊\n",
    "\n",
    "target_total = 500  # 每個 Label 期望擁有的筆數\n",
    "need_extra = 99     # 需要再抽幾筆 (500 - 401 = 99)\n",
    "\n",
    "all_label_groups = []  # 用來存放每個 label 擴充後的 DataFrame\n",
    "\n",
    "unique_labels = rssi[\"Label\"].unique()\n",
    "\n",
    "for label_value in unique_labels:\n",
    "    sub_df = rssi[rssi[\"Label\"] == label_value].reset_index(drop=True)\n",
    "    \n",
    "    # 假設這裡已知 sub_df 長度就是 401；若有需要，你可加檢查:\n",
    "    # if len(sub_df) != 401:\n",
    "    #     print(f\"Label {label_value} 不等於 401 筆，目前有 {len(sub_df)} 筆。\")\n",
    "    #     # 或視需求做別的處理\n",
    "\n",
    "    # 從 sub_df 再抽樣 99 筆 (有放回 replace=True)\n",
    "    extra = sub_df.sample(n=need_extra, replace=True)\n",
    "    \n",
    "    # 與原 sub_df 合併\n",
    "    expanded_sub_df = pd.concat([sub_df, extra], ignore_index=True)\n",
    "    \n",
    "    # 此時 expanded_sub_df 就有 401 + 99 = 500 筆\n",
    "    all_label_groups.append(expanded_sub_df)\n",
    "\n",
    "# 最後將所有 label 擴充後的 DataFrame 合併\n",
    "df_final = pd.concat(all_label_groups, ignore_index=True)\n",
    "\n",
    "print(df_final.shape)  # 應該是 (500 * label_count, ...)\n",
    "print(df_final[\"Label\"].value_counts())  # 檢查每個 Label 筆數是否都 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24500, 5)\n",
      "Label\n",
      "11    500\n",
      "10    500\n",
      "9     500\n",
      "8     500\n",
      "7     500\n",
      "6     500\n",
      "5     500\n",
      "4     500\n",
      "3     500\n",
      "2     500\n",
      "1     500\n",
      "40    500\n",
      "39    500\n",
      "38    500\n",
      "37    500\n",
      "36    500\n",
      "41    500\n",
      "42    500\n",
      "43    500\n",
      "44    500\n",
      "45    500\n",
      "46    500\n",
      "47    500\n",
      "48    500\n",
      "49    500\n",
      "16    500\n",
      "15    500\n",
      "14    500\n",
      "13    500\n",
      "12    500\n",
      "17    500\n",
      "18    500\n",
      "19    500\n",
      "20    500\n",
      "21    500\n",
      "22    500\n",
      "23    500\n",
      "24    500\n",
      "25    500\n",
      "26    500\n",
      "27    500\n",
      "28    500\n",
      "29    500\n",
      "30    500\n",
      "31    500\n",
      "32    500\n",
      "33    500\n",
      "34    500\n",
      "35    500\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 最後將所有 label 擴充後的 DataFrame 合併\n",
    "# df_final = pd.concat(all_label_groups, ignore_index=True)\n",
    "\n",
    "# 儲存為 CSV\n",
    "# df_final.to_csv(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/rssi_for_4AP.csv\", index=False)\n",
    "\n",
    "# 顯示資訊\n",
    "# print(df_final.shape)  # 應該是 (500 * label_count, ...)\n",
    "# print(df_final[\"Label\"].value_counts())  # 檢查每個 Label 筆數是否都 500\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_rssi labels: [11 10  9  8  7  6  5  4  3  2  1 40 39 38 37 36 41 42 43 44 45 46 47 48\n",
      " 49 16 15 14 13 12 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35]\n",
      "df_other labels: [ 1 40 39 38 37 36 35 34 33 32 31  2  3  4  5  6  7  8  9 10 11 12 13 14\n",
      " 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 41 42 43 44 45 46 47 48\n",
      " 49]\n",
      "df_rssi labels len: 49\n",
      "df_other labels len: 49\n",
      "Label=1, df_rssi count=401, df_other count=500\n",
      "Label=2, df_rssi count=401, df_other count=500\n",
      "Label=3, df_rssi count=401, df_other count=500\n",
      "Label=4, df_rssi count=401, df_other count=500\n",
      "Label=5, df_rssi count=401, df_other count=500\n",
      "Label=6, df_rssi count=401, df_other count=500\n",
      "Label=7, df_rssi count=401, df_other count=500\n",
      "Label=8, df_rssi count=401, df_other count=500\n",
      "Label=9, df_rssi count=401, df_other count=500\n",
      "Label=10, df_rssi count=401, df_other count=500\n",
      "Label=11, df_rssi count=401, df_other count=500\n",
      "Label=12, df_rssi count=401, df_other count=500\n",
      "Label=13, df_rssi count=401, df_other count=500\n",
      "Label=14, df_rssi count=401, df_other count=500\n",
      "Label=15, df_rssi count=401, df_other count=500\n",
      "Label=16, df_rssi count=401, df_other count=500\n",
      "Label=17, df_rssi count=401, df_other count=500\n",
      "Label=18, df_rssi count=401, df_other count=500\n",
      "Label=19, df_rssi count=401, df_other count=500\n",
      "Label=20, df_rssi count=401, df_other count=500\n",
      "Label=21, df_rssi count=401, df_other count=500\n",
      "Label=22, df_rssi count=401, df_other count=500\n",
      "Label=23, df_rssi count=401, df_other count=500\n",
      "Label=24, df_rssi count=401, df_other count=500\n",
      "Label=25, df_rssi count=401, df_other count=500\n",
      "Label=26, df_rssi count=401, df_other count=500\n",
      "Label=27, df_rssi count=401, df_other count=500\n",
      "Label=28, df_rssi count=401, df_other count=500\n",
      "Label=29, df_rssi count=401, df_other count=500\n",
      "Label=30, df_rssi count=401, df_other count=500\n",
      "Label=31, df_rssi count=401, df_other count=500\n",
      "Label=32, df_rssi count=401, df_other count=500\n",
      "Label=33, df_rssi count=401, df_other count=500\n",
      "Label=34, df_rssi count=401, df_other count=500\n",
      "Label=35, df_rssi count=401, df_other count=500\n",
      "Label=36, df_rssi count=401, df_other count=500\n",
      "Label=37, df_rssi count=401, df_other count=500\n",
      "Label=38, df_rssi count=401, df_other count=500\n",
      "Label=39, df_rssi count=401, df_other count=500\n",
      "Label=40, df_rssi count=401, df_other count=500\n",
      "Label=41, df_rssi count=401, df_other count=500\n",
      "Label=42, df_rssi count=401, df_other count=500\n",
      "Label=43, df_rssi count=401, df_other count=500\n",
      "Label=44, df_rssi count=401, df_other count=500\n",
      "Label=45, df_rssi count=401, df_other count=500\n",
      "Label=46, df_rssi count=401, df_other count=500\n",
      "Label=47, df_rssi count=401, df_other count=500\n",
      "Label=48, df_rssi count=401, df_other count=500\n",
      "Label=49, df_rssi count=401, df_other count=500\n"
     ]
    }
   ],
   "source": [
    "labels_rssi = df_final[\"Label\"].unique()\n",
    "labels_other = csi[\"Label\"].unique()\n",
    "\n",
    "print(\"df_rssi labels:\", labels_rssi)\n",
    "print(\"df_other labels:\", labels_other)\n",
    "\n",
    "\n",
    "print(\"df_rssi labels len:\", len(labels_rssi))\n",
    "print(\"df_other labels len:\", len(labels_other))\n",
    "\n",
    "common_labels = sorted(set(labels_rssi).intersection(labels_other))\n",
    "\n",
    "for lb in common_labels:\n",
    "    count_rssi = len(rssi[rssi[\"Label\"] == lb])\n",
    "    count_other = len(csi[csi[\"Label\"] == lb])\n",
    "    #if count_rssi == 0 or count_other == 0:\n",
    "    print(f\"Label={lb}, df_rssi count={count_rssi}, df_other count={count_other}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(24500, 103)\n",
      "   Label  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi           0           1  \\\n",
      "0      1     -62.0     -72.0     -65.0     -54.0  919.592301  929.595611   \n",
      "1      1     -61.0     -73.0     -66.0     -56.0  698.204841  732.963846   \n",
      "2      1     -62.0     -72.0     -66.0     -55.0  745.268408  768.188128   \n",
      "3      1     -61.0     -73.0     -66.0     -55.0  830.278267  829.860832   \n",
      "4      1     -54.0     -72.0     -67.0     -55.0  906.099884  903.507056   \n",
      "\n",
      "            2           3           4  ...        88        89        90  \\\n",
      "0  877.760787  898.203763  851.400023  ... -1.387518 -1.581549 -1.708760   \n",
      "1  684.079674  694.257877  651.079872  ... -2.677159 -2.877730 -3.051501   \n",
      "2  706.606680  713.252410  681.482208  ...  0.176051 -0.063704 -0.251636   \n",
      "3  804.005597  796.492310  786.787138  ... -1.031820 -1.194379 -1.338067   \n",
      "4  910.843565  896.688352  892.235955  ... -0.445534 -0.593750 -0.783166   \n",
      "\n",
      "         91        92        93        94        95    96     97  \n",
      "0 -2.080869 -2.287379 -2.467120 -2.666997 -2.982241 -50.0  136.0  \n",
      "1  2.738820  2.503722  2.256683  2.035376  1.737005 -49.0  148.0  \n",
      "2 -0.776139 -1.023850 -1.275624 -1.535994 -1.899522 -50.0  224.0  \n",
      "3 -1.666445 -1.878363 -2.100073 -2.369892 -2.666982 -51.0  136.0  \n",
      "4 -1.090942 -1.286103 -1.497279 -1.784787 -2.076610 -50.0  136.0  \n",
      "\n",
      "[5 rows x 103 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# === 假設以下兩個 DataFrame 已經就緒 ===\n",
    "# df_rssi: 包含 [\"Label\", ...] + RSSI 欄位\n",
    "# df_csi:  包含 [\"Label\", ...] + CSI 欄位\n",
    "\n",
    "# 取得兩邊都出現的所有 Label\n",
    "common_labels = sorted(set(df_final[\"Label\"].unique()) & set(csi[\"Label\"].unique()))\n",
    "\n",
    "merged_list = []\n",
    "\n",
    "for lb in common_labels:\n",
    "    # 1. 取出該 Label 的子集\n",
    "    sub_rssi = df_final[df_final[\"Label\"] == lb].reset_index(drop=True)\n",
    "    sub_csi  = csi[csi[\"Label\"] == lb].reset_index(drop=True)\n",
    "    \n",
    "    # 2. 檢查兩者長度是否一致\n",
    "    len_rssi = len(sub_rssi)\n",
    "    len_csi  = len(sub_csi)\n",
    "    \n",
    "    if len_rssi != len_csi:\n",
    "        print(f\"警告：Label={lb} 在 df_rssi 有 {len_rssi} 筆, df_csi 有 {len_csi} 筆，長度不同。\")\n",
    "        # 你可以選擇：略過、或僅取最短長度、或補齊…\n",
    "        # 這邊示範「取最小長度」來對齊\n",
    "        min_len = min(len_rssi, len_csi)\n",
    "        sub_rssi = sub_rssi.iloc[:min_len].reset_index(drop=True)\n",
    "        sub_csi  = sub_csi.iloc[:min_len].reset_index(drop=True)\n",
    "    \n",
    "    # 3. 逐列「貼合」兩個子 DataFrame (axis=1 即左右合併)\n",
    "    #    如果不想留重複的 Label 欄位，可以先丟掉 sub_csi 的 Label 欄位\n",
    "    sub_csi_dropped = sub_csi.drop(columns=[\"Label\"])\n",
    "    \n",
    "    # 合併\n",
    "    sub_merged = pd.concat([sub_rssi, sub_csi_dropped], axis=1)\n",
    "    \n",
    "    # 4. 把合併後的子 DataFrame 放進清單\n",
    "    merged_list.append(sub_merged)\n",
    "\n",
    "# 5. 最後把所有 Label 的結果上下堆起來\n",
    "df_merged_all = pd.concat(merged_list, ignore_index=True)\n",
    "\n",
    "# 檢查\n",
    "print(df_merged_all.shape)\n",
    "print(df_merged_all.head())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387518</td>\n",
       "      <td>-1.581549</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>-2.080869</td>\n",
       "      <td>-2.287379</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.677159</td>\n",
       "      <td>-2.877730</td>\n",
       "      <td>-3.051501</td>\n",
       "      <td>2.738820</td>\n",
       "      <td>2.503722</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176051</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.251636</td>\n",
       "      <td>-0.776139</td>\n",
       "      <td>-1.023850</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.031820</td>\n",
       "      <td>-1.194379</td>\n",
       "      <td>-1.338067</td>\n",
       "      <td>-1.666445</td>\n",
       "      <td>-1.878363</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445534</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-1.090942</td>\n",
       "      <td>-1.286103</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>49</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>397.615392</td>\n",
       "      <td>416.889674</td>\n",
       "      <td>395.588170</td>\n",
       "      <td>413.019370</td>\n",
       "      <td>382.733589</td>\n",
       "      <td>...</td>\n",
       "      <td>2.183710</td>\n",
       "      <td>2.005190</td>\n",
       "      <td>1.826107</td>\n",
       "      <td>1.364322</td>\n",
       "      <td>1.150287</td>\n",
       "      <td>0.872978</td>\n",
       "      <td>0.573063</td>\n",
       "      <td>0.255977</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24496</th>\n",
       "      <td>49</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>643.708785</td>\n",
       "      <td>646.260010</td>\n",
       "      <td>627.624888</td>\n",
       "      <td>621.498994</td>\n",
       "      <td>605.152873</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494557</td>\n",
       "      <td>-0.782703</td>\n",
       "      <td>-1.053078</td>\n",
       "      <td>-1.631902</td>\n",
       "      <td>-1.969954</td>\n",
       "      <td>-2.328378</td>\n",
       "      <td>-2.758960</td>\n",
       "      <td>3.087458</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24497</th>\n",
       "      <td>49</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>762.685387</td>\n",
       "      <td>745.553486</td>\n",
       "      <td>706.884007</td>\n",
       "      <td>695.708272</td>\n",
       "      <td>698.951357</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621717</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.356470</td>\n",
       "      <td>-0.131807</td>\n",
       "      <td>-0.426306</td>\n",
       "      <td>-0.748795</td>\n",
       "      <td>-1.031706</td>\n",
       "      <td>-1.339351</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>49</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>401.220638</td>\n",
       "      <td>416.688133</td>\n",
       "      <td>388.561449</td>\n",
       "      <td>390.436935</td>\n",
       "      <td>370.411933</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.801749</td>\n",
       "      <td>-2.120022</td>\n",
       "      <td>-2.410138</td>\n",
       "      <td>3.138034</td>\n",
       "      <td>2.751942</td>\n",
       "      <td>2.306627</td>\n",
       "      <td>1.868269</td>\n",
       "      <td>1.432071</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>49</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>753.160010</td>\n",
       "      <td>739.957431</td>\n",
       "      <td>724.996552</td>\n",
       "      <td>704.264865</td>\n",
       "      <td>701.806954</td>\n",
       "      <td>...</td>\n",
       "      <td>1.746490</td>\n",
       "      <td>1.586924</td>\n",
       "      <td>1.427713</td>\n",
       "      <td>1.118018</td>\n",
       "      <td>0.870126</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24500 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi           0           1  \\\n",
       "0          1     -62.0     -72.0     -65.0     -54.0  919.592301  929.595611   \n",
       "1          1     -61.0     -73.0     -66.0     -56.0  698.204841  732.963846   \n",
       "2          1     -62.0     -72.0     -66.0     -55.0  745.268408  768.188128   \n",
       "3          1     -61.0     -73.0     -66.0     -55.0  830.278267  829.860832   \n",
       "4          1     -54.0     -72.0     -67.0     -55.0  906.099884  903.507056   \n",
       "...      ...       ...       ...       ...       ...         ...         ...   \n",
       "24495     49     -54.0     -65.0     -55.0     -58.0  397.615392  416.889674   \n",
       "24496     49     -54.0     -64.0     -55.0     -59.0  643.708785  646.260010   \n",
       "24497     49     -54.0     -65.0     -53.0     -58.0  762.685387  745.553486   \n",
       "24498     49     -53.0     -64.0     -55.0     -59.0  401.220638  416.688133   \n",
       "24499     49     -53.0     -64.0     -55.0     -59.0  753.160010  739.957431   \n",
       "\n",
       "                2           3           4  ...        88        89        90  \\\n",
       "0      877.760787  898.203763  851.400023  ... -1.387518 -1.581549 -1.708760   \n",
       "1      684.079674  694.257877  651.079872  ... -2.677159 -2.877730 -3.051501   \n",
       "2      706.606680  713.252410  681.482208  ...  0.176051 -0.063704 -0.251636   \n",
       "3      804.005597  796.492310  786.787138  ... -1.031820 -1.194379 -1.338067   \n",
       "4      910.843565  896.688352  892.235955  ... -0.445534 -0.593750 -0.783166   \n",
       "...           ...         ...         ...  ...       ...       ...       ...   \n",
       "24495  395.588170  413.019370  382.733589  ...  2.183710  2.005190  1.826107   \n",
       "24496  627.624888  621.498994  605.152873  ... -0.494557 -0.782703 -1.053078   \n",
       "24497  706.884007  695.708272  698.951357  ...  0.621717  0.498336  0.356470   \n",
       "24498  388.561449  390.436935  370.411933  ... -1.801749 -2.120022 -2.410138   \n",
       "24499  724.996552  704.264865  701.806954  ...  1.746490  1.586924  1.427713   \n",
       "\n",
       "             91        92        93        94        95    96     97  \n",
       "0     -2.080869 -2.287379 -2.467120 -2.666997 -2.982241 -50.0  136.0  \n",
       "1      2.738820  2.503722  2.256683  2.035376  1.737005 -49.0  148.0  \n",
       "2     -0.776139 -1.023850 -1.275624 -1.535994 -1.899522 -50.0  224.0  \n",
       "3     -1.666445 -1.878363 -2.100073 -2.369892 -2.666982 -51.0  136.0  \n",
       "4     -1.090942 -1.286103 -1.497279 -1.784787 -2.076610 -50.0  136.0  \n",
       "...         ...       ...       ...       ...       ...   ...    ...  \n",
       "24495  1.364322  1.150287  0.872978  0.573063  0.255977 -56.0  148.0  \n",
       "24496 -1.631902 -1.969954 -2.328378 -2.758960  3.087458 -59.0  136.0  \n",
       "24497 -0.131807 -0.426306 -0.748795 -1.031706 -1.339351 -58.0  136.0  \n",
       "24498  3.138034  2.751942  2.306627  1.868269  1.432071 -56.0  148.0  \n",
       "24499  1.118018  0.870126  0.650771  0.353386  0.054227 -58.0  136.0  \n",
       "\n",
       "[24500 rows x 103 columns]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_merged_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssi_columns = ['AP1_Rssi', 'AP2_Rssi', 'AP3_Rssi', 'AP4_Rssi']\n",
    "df_merged_all[rssi_columns] = df_merged_all[rssi_columns].fillna(0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_labels = df_merged_all['Label']\n",
    "csi = df_merged_all.drop(columns=['Label', 'AP1_Rssi', 'AP2_Rssi', 'AP3_Rssi', 'AP4_Rssi'])\n",
    "amp = csi.iloc[:,:48]\n",
    "phase = csi.iloc[:,48:-2]\n",
    "rssi = df_merged_all[['AP1_Rssi', 'AP2_Rssi', 'AP3_Rssi', 'AP4_Rssi']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csi shape: (24500, 4)\n",
      "amp shape: (24500, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"csi shape:\", rssi.shape)\n",
    "print(\"amp shape:\", amp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "type(amp)\n",
    "amp1 = amp.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_d = denoise.preprocess_csi_for_fingerprint2(np.array(amp)) # 前處理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_d = pd.DataFrame(amp_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 取得原始的 index\n",
    "indices = amp.index\n",
    "\n",
    "# 先切分 train 與 temp (train: 70%, temp: 30%)\n",
    "train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=42)\n",
    "\n",
    "# 接著切分 temp 為 validation 與 test (各 10% 與 20%，因為 1/3 的 30% 為 10%)\n",
    "val_idx, test_idx = train_test_split(temp_idx, test_size=1/3, random_state=42)\n",
    "\n",
    "# 使用 .loc 根據切分的 index 取得各部分資料\n",
    "amp_train = amp.loc[train_idx]\n",
    "amp_val   = amp.loc[val_idx]\n",
    "amp_test  = amp.loc[test_idx]\n",
    "\n",
    "rssi_train = rssi.loc[train_idx]\n",
    "rssi_val   = rssi.loc[val_idx]\n",
    "rssi_test  = rssi.loc[test_idx]\n",
    "\n",
    "# 假設 one_hot_labels 也是與 amp 同 index 的 Series 或 DataFrame\n",
    "y_train = rp_labels.loc[train_idx]\n",
    "y_val   = rp_labels.loc[val_idx]\n",
    "y_test  = rp_labels.loc[test_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7109</th>\n",
       "      <td>-53.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-60.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>-67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11025</th>\n",
       "      <td>-64.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-48.0</td>\n",
       "      <td>-61.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14360</th>\n",
       "      <td>-68.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>-62.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18985</th>\n",
       "      <td>-61.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-49.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8279</th>\n",
       "      <td>-55.0</td>\n",
       "      <td>-76.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-64.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21198</th>\n",
       "      <td>-54.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-51.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6648</th>\n",
       "      <td>-55.0</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9517</th>\n",
       "      <td>-58.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19386</th>\n",
       "      <td>-61.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-42.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2450 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
       "7109      -53.0     -69.0     -55.0     -60.0\n",
       "13037     -68.0     -64.0     -57.0     -67.0\n",
       "11025     -64.0     -60.0     -48.0     -61.0\n",
       "14360     -68.0     -54.0     -50.0     -62.0\n",
       "18985     -61.0     -70.0     -60.0     -49.0\n",
       "...         ...       ...       ...       ...\n",
       "8279      -55.0     -76.0     -54.0     -64.0\n",
       "21198     -54.0     -68.0     -56.0     -51.0\n",
       "6648      -55.0     -80.0     -54.0       0.0\n",
       "9517      -58.0     -73.0     -55.0     -67.0\n",
       "19386     -61.0     -69.0     -62.0     -42.0\n",
       "\n",
       "[2450 rows x 4 columns]"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rssi_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>38</th>\n",
       "      <th>39</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7109</th>\n",
       "      <td>404.526884</td>\n",
       "      <td>420.896662</td>\n",
       "      <td>428.590714</td>\n",
       "      <td>432.852169</td>\n",
       "      <td>432.677709</td>\n",
       "      <td>438.553303</td>\n",
       "      <td>474.030590</td>\n",
       "      <td>546.854642</td>\n",
       "      <td>604.797487</td>\n",
       "      <td>640.063278</td>\n",
       "      <td>...</td>\n",
       "      <td>439.018223</td>\n",
       "      <td>416.914859</td>\n",
       "      <td>366.012295</td>\n",
       "      <td>314.771346</td>\n",
       "      <td>287.892341</td>\n",
       "      <td>274.448538</td>\n",
       "      <td>279.358193</td>\n",
       "      <td>265.646381</td>\n",
       "      <td>248.080632</td>\n",
       "      <td>238.924674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13037</th>\n",
       "      <td>587.804389</td>\n",
       "      <td>613.921819</td>\n",
       "      <td>609.186343</td>\n",
       "      <td>633.432712</td>\n",
       "      <td>619.067848</td>\n",
       "      <td>634.589631</td>\n",
       "      <td>654.599114</td>\n",
       "      <td>646.437159</td>\n",
       "      <td>671.503537</td>\n",
       "      <td>677.268780</td>\n",
       "      <td>...</td>\n",
       "      <td>574.652939</td>\n",
       "      <td>563.056835</td>\n",
       "      <td>560.528322</td>\n",
       "      <td>555.038737</td>\n",
       "      <td>540.792936</td>\n",
       "      <td>518.322294</td>\n",
       "      <td>517.571251</td>\n",
       "      <td>507.771602</td>\n",
       "      <td>510.169580</td>\n",
       "      <td>499.961999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11025</th>\n",
       "      <td>303.065999</td>\n",
       "      <td>304.607945</td>\n",
       "      <td>328.803893</td>\n",
       "      <td>345.418587</td>\n",
       "      <td>350.891721</td>\n",
       "      <td>366.525579</td>\n",
       "      <td>378.926114</td>\n",
       "      <td>386.046629</td>\n",
       "      <td>405.616814</td>\n",
       "      <td>422.291369</td>\n",
       "      <td>...</td>\n",
       "      <td>638.063476</td>\n",
       "      <td>644.609184</td>\n",
       "      <td>648.694073</td>\n",
       "      <td>661.048410</td>\n",
       "      <td>660.878960</td>\n",
       "      <td>667.176139</td>\n",
       "      <td>674.104591</td>\n",
       "      <td>678.412854</td>\n",
       "      <td>682.472710</td>\n",
       "      <td>700.383466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14360</th>\n",
       "      <td>620.918674</td>\n",
       "      <td>604.050495</td>\n",
       "      <td>560.142839</td>\n",
       "      <td>541.587481</td>\n",
       "      <td>549.263143</td>\n",
       "      <td>610.666030</td>\n",
       "      <td>655.861266</td>\n",
       "      <td>668.763785</td>\n",
       "      <td>675.018518</td>\n",
       "      <td>667.616656</td>\n",
       "      <td>...</td>\n",
       "      <td>781.424980</td>\n",
       "      <td>802.419466</td>\n",
       "      <td>827.589874</td>\n",
       "      <td>894.964804</td>\n",
       "      <td>869.517682</td>\n",
       "      <td>864.155657</td>\n",
       "      <td>945.052909</td>\n",
       "      <td>929.709632</td>\n",
       "      <td>986.918943</td>\n",
       "      <td>1006.123750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18985</th>\n",
       "      <td>614.493287</td>\n",
       "      <td>620.104830</td>\n",
       "      <td>631.484758</td>\n",
       "      <td>630.834368</td>\n",
       "      <td>638.200595</td>\n",
       "      <td>655.903194</td>\n",
       "      <td>673.683902</td>\n",
       "      <td>667.487828</td>\n",
       "      <td>688.723457</td>\n",
       "      <td>689.882599</td>\n",
       "      <td>...</td>\n",
       "      <td>615.978084</td>\n",
       "      <td>628.199013</td>\n",
       "      <td>615.753197</td>\n",
       "      <td>615.233289</td>\n",
       "      <td>617.285995</td>\n",
       "      <td>601.312731</td>\n",
       "      <td>598.752035</td>\n",
       "      <td>584.370602</td>\n",
       "      <td>589.598168</td>\n",
       "      <td>576.400035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8279</th>\n",
       "      <td>378.343759</td>\n",
       "      <td>385.607313</td>\n",
       "      <td>403.803170</td>\n",
       "      <td>415.380548</td>\n",
       "      <td>414.849370</td>\n",
       "      <td>411.273632</td>\n",
       "      <td>430.238306</td>\n",
       "      <td>442.899537</td>\n",
       "      <td>469.945742</td>\n",
       "      <td>477.168733</td>\n",
       "      <td>...</td>\n",
       "      <td>697.097554</td>\n",
       "      <td>712.573505</td>\n",
       "      <td>696.347614</td>\n",
       "      <td>708.837076</td>\n",
       "      <td>701.501247</td>\n",
       "      <td>699.451213</td>\n",
       "      <td>711.373320</td>\n",
       "      <td>687.262686</td>\n",
       "      <td>678.457810</td>\n",
       "      <td>639.094672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21198</th>\n",
       "      <td>356.388833</td>\n",
       "      <td>334.611715</td>\n",
       "      <td>316.483807</td>\n",
       "      <td>322.332127</td>\n",
       "      <td>307.312544</td>\n",
       "      <td>330.122704</td>\n",
       "      <td>347.634866</td>\n",
       "      <td>356.609030</td>\n",
       "      <td>361.773410</td>\n",
       "      <td>388.856014</td>\n",
       "      <td>...</td>\n",
       "      <td>878.143496</td>\n",
       "      <td>859.098365</td>\n",
       "      <td>867.551151</td>\n",
       "      <td>880.190888</td>\n",
       "      <td>867.426654</td>\n",
       "      <td>823.119068</td>\n",
       "      <td>835.380153</td>\n",
       "      <td>815.247202</td>\n",
       "      <td>805.993176</td>\n",
       "      <td>778.740650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6648</th>\n",
       "      <td>512.164036</td>\n",
       "      <td>544.614543</td>\n",
       "      <td>529.801850</td>\n",
       "      <td>543.658900</td>\n",
       "      <td>516.034883</td>\n",
       "      <td>549.815424</td>\n",
       "      <td>566.681568</td>\n",
       "      <td>644.287203</td>\n",
       "      <td>678.849762</td>\n",
       "      <td>727.402227</td>\n",
       "      <td>...</td>\n",
       "      <td>421.825793</td>\n",
       "      <td>373.363094</td>\n",
       "      <td>357.012605</td>\n",
       "      <td>306.400065</td>\n",
       "      <td>283.637092</td>\n",
       "      <td>255.548430</td>\n",
       "      <td>273.426407</td>\n",
       "      <td>280.203497</td>\n",
       "      <td>298.931430</td>\n",
       "      <td>298.810977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9517</th>\n",
       "      <td>894.724539</td>\n",
       "      <td>903.816353</td>\n",
       "      <td>923.693672</td>\n",
       "      <td>902.906418</td>\n",
       "      <td>924.313800</td>\n",
       "      <td>931.262584</td>\n",
       "      <td>957.439293</td>\n",
       "      <td>937.632124</td>\n",
       "      <td>966.041407</td>\n",
       "      <td>974.030800</td>\n",
       "      <td>...</td>\n",
       "      <td>273.715180</td>\n",
       "      <td>260.848615</td>\n",
       "      <td>233.259512</td>\n",
       "      <td>227.105702</td>\n",
       "      <td>214.347848</td>\n",
       "      <td>182.537667</td>\n",
       "      <td>182.156526</td>\n",
       "      <td>158.521292</td>\n",
       "      <td>123.858791</td>\n",
       "      <td>110.222502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19386</th>\n",
       "      <td>842.881961</td>\n",
       "      <td>802.100368</td>\n",
       "      <td>820.015244</td>\n",
       "      <td>754.725116</td>\n",
       "      <td>781.449934</td>\n",
       "      <td>783.035759</td>\n",
       "      <td>808.392232</td>\n",
       "      <td>817.397088</td>\n",
       "      <td>851.840361</td>\n",
       "      <td>827.543957</td>\n",
       "      <td>...</td>\n",
       "      <td>252.733061</td>\n",
       "      <td>273.402633</td>\n",
       "      <td>255.039213</td>\n",
       "      <td>265.369554</td>\n",
       "      <td>267.619132</td>\n",
       "      <td>288.922135</td>\n",
       "      <td>297.833846</td>\n",
       "      <td>294.090122</td>\n",
       "      <td>299.933326</td>\n",
       "      <td>279.728440</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2450 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "7109   404.526884  420.896662  428.590714  432.852169  432.677709  438.553303   \n",
       "13037  587.804389  613.921819  609.186343  633.432712  619.067848  634.589631   \n",
       "11025  303.065999  304.607945  328.803893  345.418587  350.891721  366.525579   \n",
       "14360  620.918674  604.050495  560.142839  541.587481  549.263143  610.666030   \n",
       "18985  614.493287  620.104830  631.484758  630.834368  638.200595  655.903194   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "8279   378.343759  385.607313  403.803170  415.380548  414.849370  411.273632   \n",
       "21198  356.388833  334.611715  316.483807  322.332127  307.312544  330.122704   \n",
       "6648   512.164036  544.614543  529.801850  543.658900  516.034883  549.815424   \n",
       "9517   894.724539  903.816353  923.693672  902.906418  924.313800  931.262584   \n",
       "19386  842.881961  802.100368  820.015244  754.725116  781.449934  783.035759   \n",
       "\n",
       "               6           7           8           9   ...          38  \\\n",
       "7109   474.030590  546.854642  604.797487  640.063278  ...  439.018223   \n",
       "13037  654.599114  646.437159  671.503537  677.268780  ...  574.652939   \n",
       "11025  378.926114  386.046629  405.616814  422.291369  ...  638.063476   \n",
       "14360  655.861266  668.763785  675.018518  667.616656  ...  781.424980   \n",
       "18985  673.683902  667.487828  688.723457  689.882599  ...  615.978084   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "8279   430.238306  442.899537  469.945742  477.168733  ...  697.097554   \n",
       "21198  347.634866  356.609030  361.773410  388.856014  ...  878.143496   \n",
       "6648   566.681568  644.287203  678.849762  727.402227  ...  421.825793   \n",
       "9517   957.439293  937.632124  966.041407  974.030800  ...  273.715180   \n",
       "19386  808.392232  817.397088  851.840361  827.543957  ...  252.733061   \n",
       "\n",
       "               39          40          41          42          43          44  \\\n",
       "7109   416.914859  366.012295  314.771346  287.892341  274.448538  279.358193   \n",
       "13037  563.056835  560.528322  555.038737  540.792936  518.322294  517.571251   \n",
       "11025  644.609184  648.694073  661.048410  660.878960  667.176139  674.104591   \n",
       "14360  802.419466  827.589874  894.964804  869.517682  864.155657  945.052909   \n",
       "18985  628.199013  615.753197  615.233289  617.285995  601.312731  598.752035   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "8279   712.573505  696.347614  708.837076  701.501247  699.451213  711.373320   \n",
       "21198  859.098365  867.551151  880.190888  867.426654  823.119068  835.380153   \n",
       "6648   373.363094  357.012605  306.400065  283.637092  255.548430  273.426407   \n",
       "9517   260.848615  233.259512  227.105702  214.347848  182.537667  182.156526   \n",
       "19386  273.402633  255.039213  265.369554  267.619132  288.922135  297.833846   \n",
       "\n",
       "               45          46           47  \n",
       "7109   265.646381  248.080632   238.924674  \n",
       "13037  507.771602  510.169580   499.961999  \n",
       "11025  678.412854  682.472710   700.383466  \n",
       "14360  929.709632  986.918943  1006.123750  \n",
       "18985  584.370602  589.598168   576.400035  \n",
       "...           ...         ...          ...  \n",
       "8279   687.262686  678.457810   639.094672  \n",
       "21198  815.247202  805.993176   778.740650  \n",
       "6648   280.203497  298.931430   298.810977  \n",
       "9517   158.521292  123.858791   110.222502  \n",
       "19386  294.090122  299.933326   279.728440  \n",
       "\n",
       "[2450 rows x 48 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp_train shape: (17150, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"amp_train shape:\", amp_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_oh = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_val_oh = encoder.transform(np.array(y_val).reshape(-1, 1))\n",
    "y_test_oh = encoder.transform(np.array(y_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_oh shape: (17150, 49)\n",
      "y_val_oh shape: (4900, 49)\n",
      "y_test_oh shape: (2450, 49)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train_oh shape:\", y_train_oh.shape)\n",
    "print(\"y_val_oh shape:\", y_val_oh.shape)\n",
    "print(\"y_test_oh shape:\", y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 Dataset 和 DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CSIRSSIDataset(np.array(amp_train), np.array(rssi_train), y_train_oh, augment=True, csi_noise_std=0.00, rssi_mask_prob=0.05)\n",
    "val_dataset = CSIRSSIDataset(np.array(amp_val),np.array(rssi_val), y_val_oh)\n",
    "test_dataset = CSIRSSIDataset(np.array(amp_test), np.array(rssi_test), y_test_oh)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Output shape: torch.Size([1, 49])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CSIRSSIClassifier                        [1, 49]                   --\n",
       "├─Conv1d: 1-1                            [1, 64, 48]               256\n",
       "├─BatchNorm1d: 1-2                       [1, 64, 48]               128\n",
       "├─MaxPool1d: 1-3                         [1, 64, 24]               --\n",
       "├─Conv1d: 1-4                            [1, 128, 24]              24,704\n",
       "├─BatchNorm1d: 1-5                       [1, 128, 24]              256\n",
       "├─MaxPool1d: 1-6                         [1, 128, 12]              --\n",
       "├─Linear: 1-7                            [1, 128]                  196,736\n",
       "├─Dropout: 1-8                           [1, 128]                  --\n",
       "├─Linear: 1-9                            [1, 64]                   8,256\n",
       "├─Dropout: 1-10                          [1, 64]                   --\n",
       "├─Linear: 1-11                           [1, 32]                   160\n",
       "├─Dropout: 1-12                          [1, 32]                   --\n",
       "├─Linear: 1-13                           [1, 32]                   1,056\n",
       "├─Dropout: 1-14                          [1, 32]                   --\n",
       "├─Linear: 1-15                           [1, 64]                   6,208\n",
       "├─Dropout: 1-16                          [1, 64]                   --\n",
       "├─Linear: 1-17                           [1, 49]                   3,185\n",
       "==========================================================================================\n",
       "Total params: 240,945\n",
       "Trainable params: 240,945\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.96\n",
       "Estimated Total Size (MB): 1.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class CSIRSSIClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=49, rssi_dim=4):\n",
    "        super(CSIRSSIClassifier, self).__init__()\n",
    "        # ========= CSI 分支 =========\n",
    "        # 參照你原本的 CSI CNN 架構 (輸入 shape: (batch, 1, 48))\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 輸入長度 48 經過兩次 pool 後變 48/2 -> 24，再 24/2 -> 12\n",
    "        # 輸出 channels = 128, 故 flatten_dim = 128 * 12\n",
    "        self.flatten_dim = 128 * 12\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.flatten_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2   = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        # 最後 CSI 特徵維度為 64\n",
    "        \n",
    "        # ========= RSSI 分支 =========\n",
    "        # 針對 RSSI 輸入 (假設維度為 rssi_dim, 如 4)\n",
    "        self.fc_rssi1 = nn.Linear(rssi_dim, 32)\n",
    "        self.dropout_rssi1 = nn.Dropout(0.5)\n",
    "        self.fc_rssi2 = nn.Linear(32, 32)\n",
    "        self.dropout_rssi2 = nn.Dropout(0.5)\n",
    "        # 最後 RSSI 特徵維度為 32\n",
    "        \n",
    "        # ========= 融合後分類 =========\n",
    "        # 將 CSI (64-d) 與 RSSI (32-d) 融合後為 96-d，再接全連接層\n",
    "        self.fc_merge1 = nn.Linear(64 + 32, 64)\n",
    "        self.dropout_merge = nn.Dropout(0.5)\n",
    "        self.fc_merge2 = nn.Linear(64, num_classes)\n",
    "    \n",
    "    def forward(self, csi_input, rssi_input):\n",
    "        # ---- CSI 分支 ----\n",
    "        # csi_input 預期 shape 為 (batch, 1, 48)\n",
    "        if csi_input.dim() == 2:\n",
    "            csi_input = csi_input.unsqueeze(1)\n",
    "        x = F.relu(self.bn1(self.conv1(csi_input)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten 成 (batch, flatten_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        csi_feat = F.relu(self.fc2(x))\n",
    "        csi_feat = self.dropout2(csi_feat)\n",
    "        \n",
    "        # ---- RSSI 分支 ----\n",
    "        # rssi_input 預期 shape 為 (batch, rssi_dim)\n",
    "        rssi_feat = F.relu(self.fc_rssi1(rssi_input))\n",
    "        rssi_feat = self.dropout_rssi1(rssi_feat)\n",
    "        rssi_feat = F.relu(self.fc_rssi2(rssi_feat))\n",
    "        rssi_feat = self.dropout_rssi2(rssi_feat)\n",
    "        \n",
    "        # ---- 融合特徵 ----\n",
    "        # 將 CSI 與 RSSI 特徵在特徵維度上做 concat (dim=1)\n",
    "        fusion = torch.cat((csi_feat, rssi_feat), dim=1)\n",
    "        fusion = F.relu(self.fc_merge1(fusion))\n",
    "        fusion = self.dropout_merge(fusion)\n",
    "        out = self.fc_merge2(fusion)\n",
    "        return out\n",
    "\n",
    "# 初始化模型\n",
    "model = CSIRSSIClassifier(num_classes=49, rssi_dim=4).to(device)\n",
    "\n",
    "# 測試：建立 dummy 輸入並做前向傳播\n",
    "dummy_csi = torch.randn(1, 1, 48).to(device)  # CSI 輸入: (batch=1, channels=1, length=48)\n",
    "dummy_rssi = torch.randn(1, 4).to(device)       # RSSI 輸入: (batch=1, 4-d)\n",
    "output = model(dummy_csi, dummy_rssi)\n",
    "print(\"Output shape:\", output.shape)  # 預期: (1, 49)\n",
    "\n",
    "# 安裝 torchinfo (如果還沒安裝)\n",
    "# pip install torchinfo\n",
    "\n",
    "from torchinfo import summary\n",
    "\n",
    "dummy_csi = torch.randn(1, 1, 48).to(device)\n",
    "dummy_rssi = torch.randn(1, 4).to(device)\n",
    "summary(model, input_data=(dummy_csi, dummy_rssi))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 損失函數\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 優化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 學習率調整器\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Train Loss: 3.9472 | Train Acc: 2.70% | Val Loss: 3.5648 | Val Acc: 4.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 3.5648) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [2/200] | Train Loss: 3.2707 | Train Acc: 8.99% | Val Loss: 2.3278 | Val Acc: 25.65%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.3278) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [3/200] | Train Loss: 2.5233 | Train Acc: 18.52% | Val Loss: 1.7800 | Val Acc: 43.31%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.7800) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [4/200] | Train Loss: 2.2168 | Train Acc: 24.87% | Val Loss: 1.5716 | Val Acc: 48.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.5716) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [5/200] | Train Loss: 2.0284 | Train Acc: 30.15% | Val Loss: 1.3340 | Val Acc: 59.20%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.3340) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [6/200] | Train Loss: 1.8740 | Train Acc: 35.52% | Val Loss: 1.1669 | Val Acc: 70.94%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.1669) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [7/200] | Train Loss: 1.7448 | Train Acc: 39.94% | Val Loss: 0.9617 | Val Acc: 77.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9617) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [8/200] | Train Loss: 1.6261 | Train Acc: 44.41% | Val Loss: 0.8232 | Val Acc: 81.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.8232) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [9/200] | Train Loss: 1.5625 | Train Acc: 47.75% | Val Loss: 0.7611 | Val Acc: 86.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7611) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [10/200] | Train Loss: 1.4809 | Train Acc: 49.98% | Val Loss: 0.6519 | Val Acc: 85.55%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6519) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [11/200] | Train Loss: 1.4003 | Train Acc: 52.55% | Val Loss: 0.5823 | Val Acc: 89.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5823) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [12/200] | Train Loss: 1.3727 | Train Acc: 53.62% | Val Loss: 0.5419 | Val Acc: 89.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5419) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [13/200] | Train Loss: 1.3060 | Train Acc: 56.33% | Val Loss: 0.5231 | Val Acc: 88.84%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5231) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [14/200] | Train Loss: 1.2396 | Train Acc: 58.64% | Val Loss: 0.4447 | Val Acc: 89.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4447) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [15/200] | Train Loss: 1.2240 | Train Acc: 58.85% | Val Loss: 0.4648 | Val Acc: 91.06%\n",
      "Epoch [16/200] | Train Loss: 1.1794 | Train Acc: 60.21% | Val Loss: 0.3727 | Val Acc: 93.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3727) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [17/200] | Train Loss: 1.1622 | Train Acc: 61.21% | Val Loss: 0.3955 | Val Acc: 92.22%\n",
      "Epoch [18/200] | Train Loss: 1.1372 | Train Acc: 62.13% | Val Loss: 0.3637 | Val Acc: 93.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3637) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [19/200] | Train Loss: 1.1014 | Train Acc: 63.43% | Val Loss: 0.3494 | Val Acc: 91.18%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3494) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [20/200] | Train Loss: 1.0821 | Train Acc: 63.81% | Val Loss: 0.3347 | Val Acc: 92.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3347) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [21/200] | Train Loss: 1.0729 | Train Acc: 64.60% | Val Loss: 0.3079 | Val Acc: 96.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3079) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [22/200] | Train Loss: 1.0524 | Train Acc: 65.60% | Val Loss: 0.3064 | Val Acc: 93.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3064) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [23/200] | Train Loss: 1.0287 | Train Acc: 66.29% | Val Loss: 0.2800 | Val Acc: 92.80%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2800) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [24/200] | Train Loss: 1.0106 | Train Acc: 67.48% | Val Loss: 0.2693 | Val Acc: 93.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2693) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [25/200] | Train Loss: 0.9909 | Train Acc: 67.85% | Val Loss: 0.2667 | Val Acc: 93.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2667) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [26/200] | Train Loss: 0.9849 | Train Acc: 67.60% | Val Loss: 0.2529 | Val Acc: 94.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2529) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [27/200] | Train Loss: 0.9642 | Train Acc: 68.10% | Val Loss: 0.2725 | Val Acc: 92.86%\n",
      "Epoch [28/200] | Train Loss: 0.9578 | Train Acc: 68.62% | Val Loss: 0.2418 | Val Acc: 95.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2418) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [29/200] | Train Loss: 0.9580 | Train Acc: 68.83% | Val Loss: 0.2580 | Val Acc: 92.33%\n",
      "Epoch [30/200] | Train Loss: 0.9264 | Train Acc: 69.52% | Val Loss: 0.2306 | Val Acc: 95.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2306) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [31/200] | Train Loss: 0.9511 | Train Acc: 68.86% | Val Loss: 0.2416 | Val Acc: 94.92%\n",
      "Epoch [32/200] | Train Loss: 0.9257 | Train Acc: 69.60% | Val Loss: 0.2254 | Val Acc: 93.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2254) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [33/200] | Train Loss: 0.9196 | Train Acc: 70.19% | Val Loss: 0.2283 | Val Acc: 95.37%\n",
      "Epoch [34/200] | Train Loss: 0.9042 | Train Acc: 70.29% | Val Loss: 0.2247 | Val Acc: 94.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2247) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [35/200] | Train Loss: 0.8842 | Train Acc: 71.04% | Val Loss: 0.2024 | Val Acc: 93.82%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2024) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [36/200] | Train Loss: 0.8745 | Train Acc: 71.38% | Val Loss: 0.2071 | Val Acc: 95.22%\n",
      "Epoch [37/200] | Train Loss: 0.8804 | Train Acc: 71.17% | Val Loss: 0.2084 | Val Acc: 95.35%\n",
      "Epoch [38/200] | Train Loss: 0.8647 | Train Acc: 71.60% | Val Loss: 0.1932 | Val Acc: 95.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1932) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [39/200] | Train Loss: 0.8746 | Train Acc: 71.36% | Val Loss: 0.1904 | Val Acc: 94.78%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1904) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [40/200] | Train Loss: 0.8717 | Train Acc: 71.64% | Val Loss: 0.2150 | Val Acc: 94.04%\n",
      "Epoch [41/200] | Train Loss: 0.8347 | Train Acc: 73.00% | Val Loss: 0.1846 | Val Acc: 95.35%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1846) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [42/200] | Train Loss: 0.8436 | Train Acc: 72.28% | Val Loss: 0.1901 | Val Acc: 94.39%\n",
      "Epoch [43/200] | Train Loss: 0.8325 | Train Acc: 72.60% | Val Loss: 0.1834 | Val Acc: 95.04%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1834) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [44/200] | Train Loss: 0.8285 | Train Acc: 73.04% | Val Loss: 0.1883 | Val Acc: 95.08%\n",
      "Epoch [45/200] | Train Loss: 0.8275 | Train Acc: 73.54% | Val Loss: 0.1827 | Val Acc: 96.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1827) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [46/200] | Train Loss: 0.8357 | Train Acc: 72.92% | Val Loss: 0.1859 | Val Acc: 95.80%\n",
      "Epoch [47/200] | Train Loss: 0.8187 | Train Acc: 72.94% | Val Loss: 0.1823 | Val Acc: 95.96%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1823) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [48/200] | Train Loss: 0.7787 | Train Acc: 74.41% | Val Loss: 0.1748 | Val Acc: 95.76%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1748) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [49/200] | Train Loss: 0.8155 | Train Acc: 73.76% | Val Loss: 0.2007 | Val Acc: 94.16%\n",
      "Epoch [50/200] | Train Loss: 0.8023 | Train Acc: 74.25% | Val Loss: 0.1741 | Val Acc: 96.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1741) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [51/200] | Train Loss: 0.8089 | Train Acc: 74.13% | Val Loss: 0.1660 | Val Acc: 96.04%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1660) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [52/200] | Train Loss: 0.7924 | Train Acc: 74.42% | Val Loss: 0.1648 | Val Acc: 96.67%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1648) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [53/200] | Train Loss: 0.7884 | Train Acc: 74.09% | Val Loss: 0.1618 | Val Acc: 96.20%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1618) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [54/200] | Train Loss: 0.7776 | Train Acc: 74.38% | Val Loss: 0.1544 | Val Acc: 97.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1544) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [55/200] | Train Loss: 0.7526 | Train Acc: 75.29% | Val Loss: 0.2007 | Val Acc: 95.45%\n",
      "Epoch [56/200] | Train Loss: 0.7768 | Train Acc: 74.60% | Val Loss: 0.1786 | Val Acc: 95.59%\n",
      "Epoch [57/200] | Train Loss: 0.7705 | Train Acc: 75.18% | Val Loss: 0.1887 | Val Acc: 96.45%\n",
      "Epoch [58/200] | Train Loss: 0.7556 | Train Acc: 75.26% | Val Loss: 0.1590 | Val Acc: 96.43%\n",
      "Epoch [59/200] | Train Loss: 0.7532 | Train Acc: 75.57% | Val Loss: 0.1658 | Val Acc: 95.57%\n",
      "Epoch [60/200] | Train Loss: 0.7395 | Train Acc: 76.23% | Val Loss: 0.1454 | Val Acc: 96.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1454) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [61/200] | Train Loss: 0.7569 | Train Acc: 76.10% | Val Loss: 0.1479 | Val Acc: 96.76%\n",
      "Epoch [62/200] | Train Loss: 0.7366 | Train Acc: 76.20% | Val Loss: 0.1444 | Val Acc: 98.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1444) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [63/200] | Train Loss: 0.7319 | Train Acc: 76.23% | Val Loss: 0.1550 | Val Acc: 96.53%\n",
      "Epoch [64/200] | Train Loss: 0.7304 | Train Acc: 76.36% | Val Loss: 0.1496 | Val Acc: 97.47%\n",
      "Epoch [65/200] | Train Loss: 0.7192 | Train Acc: 76.66% | Val Loss: 0.1346 | Val Acc: 96.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1346) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [66/200] | Train Loss: 0.7265 | Train Acc: 76.60% | Val Loss: 0.1399 | Val Acc: 97.43%\n",
      "Epoch [67/200] | Train Loss: 0.7288 | Train Acc: 76.85% | Val Loss: 0.1279 | Val Acc: 97.35%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1279) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [68/200] | Train Loss: 0.7043 | Train Acc: 77.42% | Val Loss: 0.1534 | Val Acc: 96.51%\n",
      "Epoch [69/200] | Train Loss: 0.7193 | Train Acc: 77.28% | Val Loss: 0.1346 | Val Acc: 96.57%\n",
      "Epoch [70/200] | Train Loss: 0.7260 | Train Acc: 76.72% | Val Loss: 0.1371 | Val Acc: 97.67%\n",
      "Epoch [71/200] | Train Loss: 0.6883 | Train Acc: 78.31% | Val Loss: 0.1285 | Val Acc: 96.90%\n",
      "Epoch [72/200] | Train Loss: 0.7098 | Train Acc: 77.40% | Val Loss: 0.1376 | Val Acc: 97.43%\n",
      "Epoch [73/200] | Train Loss: 0.7011 | Train Acc: 77.35% | Val Loss: 0.1320 | Val Acc: 97.12%\n",
      "Epoch [74/200] | Train Loss: 0.7117 | Train Acc: 77.38% | Val Loss: 0.1444 | Val Acc: 96.59%\n",
      "Epoch [75/200] | Train Loss: 0.6925 | Train Acc: 77.85% | Val Loss: 0.1237 | Val Acc: 97.20%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1237) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [76/200] | Train Loss: 0.6835 | Train Acc: 78.08% | Val Loss: 0.1336 | Val Acc: 97.57%\n",
      "Epoch [77/200] | Train Loss: 0.7004 | Train Acc: 77.91% | Val Loss: 0.1416 | Val Acc: 97.43%\n",
      "Epoch [78/200] | Train Loss: 0.7005 | Train Acc: 78.02% | Val Loss: 0.1229 | Val Acc: 96.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1229) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [79/200] | Train Loss: 0.6836 | Train Acc: 78.12% | Val Loss: 0.1252 | Val Acc: 97.80%\n",
      "Epoch [80/200] | Train Loss: 0.6940 | Train Acc: 77.78% | Val Loss: 0.1262 | Val Acc: 98.41%\n",
      "Epoch [81/200] | Train Loss: 0.6968 | Train Acc: 77.67% | Val Loss: 0.1310 | Val Acc: 98.02%\n",
      "Epoch [82/200] | Train Loss: 0.7038 | Train Acc: 77.59% | Val Loss: 0.1530 | Val Acc: 96.57%\n",
      "Epoch [83/200] | Train Loss: 0.6735 | Train Acc: 78.55% | Val Loss: 0.1234 | Val Acc: 96.69%\n",
      "Epoch [84/200] | Train Loss: 0.6860 | Train Acc: 78.24% | Val Loss: 0.1244 | Val Acc: 98.33%\n",
      "Epoch [85/200] | Train Loss: 0.6721 | Train Acc: 78.37% | Val Loss: 0.1251 | Val Acc: 96.94%\n",
      "Epoch [86/200] | Train Loss: 0.6812 | Train Acc: 78.59% | Val Loss: 0.1133 | Val Acc: 97.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1133) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [87/200] | Train Loss: 0.6851 | Train Acc: 78.11% | Val Loss: 0.1360 | Val Acc: 97.65%\n",
      "Epoch [88/200] | Train Loss: 0.7001 | Train Acc: 78.16% | Val Loss: 0.1102 | Val Acc: 98.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1102) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [89/200] | Train Loss: 0.6669 | Train Acc: 79.29% | Val Loss: 0.1097 | Val Acc: 97.84%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1097) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [90/200] | Train Loss: 0.6692 | Train Acc: 79.29% | Val Loss: 0.1046 | Val Acc: 98.18%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1046) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [91/200] | Train Loss: 0.6627 | Train Acc: 78.99% | Val Loss: 0.1048 | Val Acc: 98.18%\n",
      "Epoch [92/200] | Train Loss: 0.6790 | Train Acc: 78.69% | Val Loss: 0.1292 | Val Acc: 97.88%\n",
      "Epoch [93/200] | Train Loss: 0.6627 | Train Acc: 79.23% | Val Loss: 0.1149 | Val Acc: 98.43%\n",
      "Epoch [94/200] | Train Loss: 0.6591 | Train Acc: 78.75% | Val Loss: 0.1146 | Val Acc: 97.92%\n",
      "Epoch [95/200] | Train Loss: 0.6702 | Train Acc: 78.93% | Val Loss: 0.1135 | Val Acc: 97.63%\n",
      "Epoch [96/200] | Train Loss: 0.6739 | Train Acc: 78.82% | Val Loss: 0.0996 | Val Acc: 98.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0996) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [97/200] | Train Loss: 0.6631 | Train Acc: 78.94% | Val Loss: 0.1043 | Val Acc: 98.37%\n",
      "Epoch [98/200] | Train Loss: 0.6628 | Train Acc: 78.62% | Val Loss: 0.1167 | Val Acc: 97.96%\n",
      "Epoch [99/200] | Train Loss: 0.6400 | Train Acc: 79.83% | Val Loss: 0.1024 | Val Acc: 98.57%\n",
      "Epoch [100/200] | Train Loss: 0.6410 | Train Acc: 79.96% | Val Loss: 0.1282 | Val Acc: 98.02%\n",
      "Epoch [101/200] | Train Loss: 0.6492 | Train Acc: 79.45% | Val Loss: 0.1008 | Val Acc: 98.59%\n",
      "Epoch [102/200] | Train Loss: 0.6451 | Train Acc: 79.80% | Val Loss: 0.1206 | Val Acc: 98.14%\n",
      "Epoch [103/200] | Train Loss: 0.6518 | Train Acc: 79.24% | Val Loss: 0.1073 | Val Acc: 98.43%\n",
      "Epoch [104/200] | Train Loss: 0.6420 | Train Acc: 79.94% | Val Loss: 0.1072 | Val Acc: 97.61%\n",
      "Epoch [105/200] | Train Loss: 0.6524 | Train Acc: 79.18% | Val Loss: 0.1228 | Val Acc: 98.12%\n",
      "Epoch [106/200] | Train Loss: 0.6421 | Train Acc: 80.31% | Val Loss: 0.0986 | Val Acc: 98.55%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0986) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [107/200] | Train Loss: 0.6518 | Train Acc: 79.59% | Val Loss: 0.1095 | Val Acc: 98.16%\n",
      "Epoch [108/200] | Train Loss: 0.6632 | Train Acc: 79.35% | Val Loss: 0.1185 | Val Acc: 98.45%\n",
      "Epoch [109/200] | Train Loss: 0.6301 | Train Acc: 80.21% | Val Loss: 0.1076 | Val Acc: 98.73%\n",
      "Epoch [110/200] | Train Loss: 0.6357 | Train Acc: 80.16% | Val Loss: 0.1077 | Val Acc: 97.88%\n",
      "Epoch [111/200] | Train Loss: 0.6314 | Train Acc: 79.97% | Val Loss: 0.1686 | Val Acc: 96.08%\n",
      "Epoch [112/200] | Train Loss: 0.6461 | Train Acc: 79.27% | Val Loss: 0.1128 | Val Acc: 98.33%\n",
      "Epoch [113/200] | Train Loss: 0.6568 | Train Acc: 79.60% | Val Loss: 0.1095 | Val Acc: 97.57%\n",
      "Epoch [114/200] | Train Loss: 0.6272 | Train Acc: 80.29% | Val Loss: 0.1046 | Val Acc: 98.73%\n",
      "Epoch [115/200] | Train Loss: 0.6559 | Train Acc: 79.61% | Val Loss: 0.0969 | Val Acc: 98.98%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0969) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [116/200] | Train Loss: 0.6262 | Train Acc: 80.47% | Val Loss: 0.1061 | Val Acc: 98.49%\n",
      "Epoch [117/200] | Train Loss: 0.6628 | Train Acc: 79.48% | Val Loss: 0.1145 | Val Acc: 98.45%\n",
      "Epoch [118/200] | Train Loss: 0.6203 | Train Acc: 80.39% | Val Loss: 0.1056 | Val Acc: 98.78%\n",
      "Epoch [119/200] | Train Loss: 0.6175 | Train Acc: 80.51% | Val Loss: 0.0970 | Val Acc: 98.84%\n",
      "Epoch [120/200] | Train Loss: 0.6310 | Train Acc: 80.20% | Val Loss: 0.1011 | Val Acc: 97.98%\n",
      "Epoch [121/200] | Train Loss: 0.6445 | Train Acc: 79.80% | Val Loss: 0.1039 | Val Acc: 98.71%\n",
      "Epoch [122/200] | Train Loss: 0.6156 | Train Acc: 80.80% | Val Loss: 0.0958 | Val Acc: 98.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0958) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [123/200] | Train Loss: 0.6172 | Train Acc: 80.85% | Val Loss: 0.1075 | Val Acc: 98.41%\n",
      "Epoch [124/200] | Train Loss: 0.6109 | Train Acc: 80.49% | Val Loss: 0.0950 | Val Acc: 98.76%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0950) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [125/200] | Train Loss: 0.6150 | Train Acc: 80.95% | Val Loss: 0.0990 | Val Acc: 98.76%\n",
      "Epoch [126/200] | Train Loss: 0.6311 | Train Acc: 80.64% | Val Loss: 0.0976 | Val Acc: 98.71%\n",
      "Epoch [127/200] | Train Loss: 0.6371 | Train Acc: 80.47% | Val Loss: 0.1077 | Val Acc: 98.76%\n",
      "Epoch [128/200] | Train Loss: 0.6298 | Train Acc: 80.85% | Val Loss: 0.1114 | Val Acc: 98.02%\n",
      "Epoch [129/200] | Train Loss: 0.6218 | Train Acc: 80.95% | Val Loss: 0.0957 | Val Acc: 98.71%\n",
      "Epoch [130/200] | Train Loss: 0.6414 | Train Acc: 80.10% | Val Loss: 0.1079 | Val Acc: 98.12%\n",
      "Epoch [131/200] | Train Loss: 0.6365 | Train Acc: 80.22% | Val Loss: 0.0934 | Val Acc: 98.82%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0934) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [132/200] | Train Loss: 0.5875 | Train Acc: 81.29% | Val Loss: 0.0871 | Val Acc: 99.02%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0871) 至 ./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\n",
      "Epoch [133/200] | Train Loss: 0.6022 | Train Acc: 81.18% | Val Loss: 0.0943 | Val Acc: 98.76%\n",
      "Epoch [134/200] | Train Loss: 0.5978 | Train Acc: 81.60% | Val Loss: 0.0959 | Val Acc: 99.00%\n",
      "Epoch [135/200] | Train Loss: 0.6354 | Train Acc: 80.64% | Val Loss: 0.0957 | Val Acc: 98.82%\n",
      "Epoch [136/200] | Train Loss: 0.6137 | Train Acc: 80.83% | Val Loss: 0.1445 | Val Acc: 97.35%\n",
      "Epoch [137/200] | Train Loss: 0.6045 | Train Acc: 80.82% | Val Loss: 0.1548 | Val Acc: 96.57%\n",
      "Epoch [138/200] | Train Loss: 0.6285 | Train Acc: 80.53% | Val Loss: 0.0964 | Val Acc: 98.92%\n",
      "Epoch [139/200] | Train Loss: 0.6151 | Train Acc: 80.97% | Val Loss: 0.1420 | Val Acc: 97.08%\n",
      "Epoch [140/200] | Train Loss: 0.6218 | Train Acc: 81.14% | Val Loss: 0.0978 | Val Acc: 98.71%\n",
      "Epoch [141/200] | Train Loss: 0.6023 | Train Acc: 80.87% | Val Loss: 0.1022 | Val Acc: 98.59%\n",
      "Epoch [142/200] | Train Loss: 0.6246 | Train Acc: 80.84% | Val Loss: 0.1030 | Val Acc: 99.02%\n",
      "Epoch [143/200] | Train Loss: 0.6005 | Train Acc: 81.25% | Val Loss: 0.0877 | Val Acc: 99.04%\n",
      "Epoch [144/200] | Train Loss: 0.6083 | Train Acc: 81.24% | Val Loss: 0.0959 | Val Acc: 98.86%\n",
      "Epoch [145/200] | Train Loss: 0.5987 | Train Acc: 81.12% | Val Loss: 0.1530 | Val Acc: 96.33%\n",
      "Epoch [146/200] | Train Loss: 0.6043 | Train Acc: 81.31% | Val Loss: 0.1000 | Val Acc: 98.49%\n",
      "Epoch [147/200] | Train Loss: 0.6319 | Train Acc: 80.79% | Val Loss: 0.0935 | Val Acc: 99.00%\n",
      "Epoch [148/200] | Train Loss: 0.6324 | Train Acc: 80.62% | Val Loss: 0.1092 | Val Acc: 97.92%\n",
      "Epoch [149/200] | Train Loss: 0.6113 | Train Acc: 80.72% | Val Loss: 0.1086 | Val Acc: 98.41%\n",
      "Epoch [150/200] | Train Loss: 0.6064 | Train Acc: 81.16% | Val Loss: 0.1107 | Val Acc: 98.43%\n",
      "Epoch [151/200] | Train Loss: 0.5968 | Train Acc: 81.43% | Val Loss: 0.1063 | Val Acc: 98.27%\n",
      "Epoch [152/200] | Train Loss: 0.6088 | Train Acc: 81.17% | Val Loss: 0.0931 | Val Acc: 98.88%\n",
      "Early stop at epoch 152\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 儲存最佳模型相關設定\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\"\n",
    "\n",
    "# 訓練參數\n",
    "epochs = 200\n",
    "\n",
    "# Early Stopping 參數\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "# 紀錄訓練過程中的 loss 和 accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- 訓練階段 ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 注意：這裡每個 batch 返回三個項目：CSI (amp)、RSSI 與 labels\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        # 將資料移到 device 上\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 傳入兩個輸入到模型 (CSI 與 RSSI)\n",
    "        outputs = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # CrossEntropyLoss 需要 target 為 class index\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新訓練 loss 與正確數\n",
    "        train_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        train_correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    # ---- 驗證階段 ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(amp_inputs, rssi_inputs)\n",
    "            loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "            val_loss += loss.item() * amp_inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            val_correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    # 紀錄每個 epoch 的數值\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # 輸出當前 epoch 的結果\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # 儲存最佳模型\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ---- loss ----\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# ---- accuracy ----\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 測試損失: 0.4855, 測試準確率: 99.02%\n"
     ]
    }
   ],
   "source": [
    "# 載入最佳模型\n",
    "model.load_state_dict(torch.load(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN/models_save/rssi_csi_best_model_0522_no_std_rssi32_aug_005.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 測試模型\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for amp_inputs, rssi_input, labels in test_loader:\n",
    "        amp_inputs, rssi_inputs, labels = amp_inputs.to(device), rssi_input.to(device), labels.to(device)\n",
    "        outputs = model(amp_inputs, rssi_inputs)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "        test_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "        # 儲存真實標籤與預測標籤\n",
    "        all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"📊 測試損失: {test_loss:.4f}, 測試準確率: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance error: 0.030768533821913406\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adjusted_labels = np.array(all_labels) + 1\n",
    "adjusted_predictions = np.array(all_predictions) + 1\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # 取出對應的座標\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "            print(f\"Warning: Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # 計算歐氏距離\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) if errors else None\n",
    "\n",
    "COORDINATES = {\n",
    "    # 下邊界 (1-10 和 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "\n",
    "    # 左邊界 (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "\n",
    "    # 上邊界 (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "\n",
    "    # 右邊界 (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "\n",
    "    # 中間點 (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "mean_error = compute_mean_distance_error(adjusted_labels, adjusted_predictions, COORDINATES)\n",
    "print(\"Mean distance error:\", mean_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CSIRSSIClassifierGated(nn.Module):\n",
    "    def __init__(self, num_classes=49, rssi_dim=4):\n",
    "        super(CSIRSSIClassifierGated, self).__init__()\n",
    "        # --- CSI 分支 ---\n",
    "        self.conv1 = nn.Conv1d(1, 64, 3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(2)\n",
    "        self.conv2 = nn.Conv1d(64, 128, 3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(2)\n",
    "        self.fc1 = nn.Linear(128 * 12, 128)\n",
    "        self.dropout1 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # --- RSSI 分支 ---\n",
    "        self.fc_rssi1 = nn.Linear(rssi_dim, 32)\n",
    "        self.dropout_rssi1 = nn.Dropout(0.5)\n",
    "        self.fc_rssi2 = nn.Linear(32, 64)\n",
    "        self.dropout_rssi2 = nn.Dropout(0.5)\n",
    "\n",
    "        # --- 門控融合 ---\n",
    "        self.gate_fc = nn.Sequential(\n",
    "            nn.Linear(64 + 64, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # --- 分類層 ---\n",
    "        self.fc_final1 = nn.Linear(64, 64)\n",
    "        self.dropout_merge = nn.Dropout(0.5)\n",
    "        self.fc_final2 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, csi_input, rssi_input):\n",
    "        if csi_input.dim() == 2:\n",
    "            csi_input = csi_input.unsqueeze(1)\n",
    "        x = self.pool1(F.relu(self.bn1(self.conv1(csi_input))))\n",
    "        x = self.pool2(F.relu(self.bn2(self.conv2(x))))\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        csi_feat = self.dropout2(F.relu(self.fc2(x)))  # shape: (batch, 64)\n",
    "\n",
    "        rssi_feat = self.dropout_rssi1(F.relu(self.fc_rssi1(rssi_input)))\n",
    "        rssi_feat = self.dropout_rssi2(F.relu(self.fc_rssi2(rssi_feat)))  # shape: (batch, 8)\n",
    "\n",
    "        fusion = torch.cat((csi_feat, rssi_feat), dim=1)  # shape: (batch, 72)\n",
    "        gate = self.gate_fc(fusion)                      # shape: (batch, 1)\n",
    "        gated_fusion = gate * csi_feat + (1 - gate) * rssi_feat.repeat(1, 8)  # 將 8 維 rssi 擴展成 64 維\n",
    "\n",
    "        x = self.dropout_merge(F.relu(self.fc_final1(gated_fusion)))\n",
    "        out = self.fc_final2(x)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 損失函數\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 優化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 學習率調整器\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Train Loss: 3.6115 | Train Acc: 5.94% | Val Loss: 2.6704 | Val Acc: 22.08%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.6704) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [2/200] | Train Loss: 2.6853 | Train Acc: 16.14% | Val Loss: 1.8979 | Val Acc: 41.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.8979) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [3/200] | Train Loss: 2.2674 | Train Acc: 23.84% | Val Loss: 1.5910 | Val Acc: 49.10%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.5910) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [4/200] | Train Loss: 1.9910 | Train Acc: 30.49% | Val Loss: 1.2306 | Val Acc: 65.29%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.2306) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [5/200] | Train Loss: 1.8429 | Train Acc: 35.31% | Val Loss: 1.2417 | Val Acc: 65.06%\n",
      "Epoch [6/200] | Train Loss: 1.7404 | Train Acc: 37.89% | Val Loss: 1.0315 | Val Acc: 69.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.0315) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [7/200] | Train Loss: 1.6497 | Train Acc: 41.21% | Val Loss: 0.9463 | Val Acc: 73.41%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9463) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [8/200] | Train Loss: 1.5896 | Train Acc: 43.32% | Val Loss: 0.9094 | Val Acc: 73.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9094) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [9/200] | Train Loss: 1.5306 | Train Acc: 45.21% | Val Loss: 0.7752 | Val Acc: 78.82%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7752) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [10/200] | Train Loss: 1.4845 | Train Acc: 46.77% | Val Loss: 0.8038 | Val Acc: 73.41%\n",
      "Epoch [11/200] | Train Loss: 1.4366 | Train Acc: 48.75% | Val Loss: 0.7034 | Val Acc: 81.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7034) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [12/200] | Train Loss: 1.3798 | Train Acc: 50.13% | Val Loss: 0.6688 | Val Acc: 80.80%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6688) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [13/200] | Train Loss: 1.3605 | Train Acc: 50.69% | Val Loss: 0.6338 | Val Acc: 83.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6338) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [14/200] | Train Loss: 1.3260 | Train Acc: 52.20% | Val Loss: 0.7442 | Val Acc: 80.53%\n",
      "Epoch [15/200] | Train Loss: 1.3100 | Train Acc: 53.29% | Val Loss: 0.5841 | Val Acc: 83.47%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5841) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [16/200] | Train Loss: 1.2627 | Train Acc: 54.22% | Val Loss: 0.5494 | Val Acc: 84.92%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5494) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [17/200] | Train Loss: 1.2471 | Train Acc: 55.76% | Val Loss: 0.5495 | Val Acc: 83.20%\n",
      "Epoch [18/200] | Train Loss: 1.2300 | Train Acc: 56.47% | Val Loss: 0.5327 | Val Acc: 85.49%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5327) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [19/200] | Train Loss: 1.2165 | Train Acc: 56.63% | Val Loss: 0.5234 | Val Acc: 87.51%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5234) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [20/200] | Train Loss: 1.1911 | Train Acc: 57.72% | Val Loss: 0.5326 | Val Acc: 84.27%\n",
      "Epoch [21/200] | Train Loss: 1.1833 | Train Acc: 58.48% | Val Loss: 0.5214 | Val Acc: 83.29%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5214) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [22/200] | Train Loss: 1.1443 | Train Acc: 58.99% | Val Loss: 0.4489 | Val Acc: 87.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4489) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [23/200] | Train Loss: 1.1201 | Train Acc: 60.15% | Val Loss: 0.4405 | Val Acc: 86.94%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4405) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [24/200] | Train Loss: 1.1132 | Train Acc: 60.86% | Val Loss: 0.4481 | Val Acc: 90.65%\n",
      "Epoch [25/200] | Train Loss: 1.0951 | Train Acc: 61.22% | Val Loss: 0.3991 | Val Acc: 90.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3991) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [26/200] | Train Loss: 1.0872 | Train Acc: 61.94% | Val Loss: 0.4133 | Val Acc: 87.51%\n",
      "Epoch [27/200] | Train Loss: 1.0747 | Train Acc: 62.41% | Val Loss: 0.3965 | Val Acc: 89.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3965) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [28/200] | Train Loss: 1.0666 | Train Acc: 62.08% | Val Loss: 0.3946 | Val Acc: 89.49%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3946) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [29/200] | Train Loss: 1.0562 | Train Acc: 62.96% | Val Loss: 0.4079 | Val Acc: 90.22%\n",
      "Epoch [30/200] | Train Loss: 1.0390 | Train Acc: 63.47% | Val Loss: 0.3911 | Val Acc: 90.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3911) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [31/200] | Train Loss: 1.0166 | Train Acc: 64.57% | Val Loss: 0.6014 | Val Acc: 77.29%\n",
      "Epoch [32/200] | Train Loss: 1.0011 | Train Acc: 65.28% | Val Loss: 0.6082 | Val Acc: 78.67%\n",
      "Epoch [33/200] | Train Loss: 1.0011 | Train Acc: 64.86% | Val Loss: 0.3544 | Val Acc: 90.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3544) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [34/200] | Train Loss: 0.9980 | Train Acc: 65.57% | Val Loss: 0.3519 | Val Acc: 91.55%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3519) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [35/200] | Train Loss: 0.9829 | Train Acc: 65.68% | Val Loss: 0.3712 | Val Acc: 88.69%\n",
      "Epoch [36/200] | Train Loss: 0.9782 | Train Acc: 65.78% | Val Loss: 0.3437 | Val Acc: 89.94%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3437) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [37/200] | Train Loss: 0.9658 | Train Acc: 66.48% | Val Loss: 0.3249 | Val Acc: 92.35%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3249) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [38/200] | Train Loss: 0.9610 | Train Acc: 66.70% | Val Loss: 0.3329 | Val Acc: 89.59%\n",
      "Epoch [39/200] | Train Loss: 0.9512 | Train Acc: 67.76% | Val Loss: 0.3572 | Val Acc: 91.41%\n",
      "Epoch [40/200] | Train Loss: 0.9411 | Train Acc: 67.48% | Val Loss: 0.3296 | Val Acc: 89.59%\n",
      "Epoch [41/200] | Train Loss: 0.9165 | Train Acc: 68.43% | Val Loss: 0.2951 | Val Acc: 90.67%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2951) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [42/200] | Train Loss: 0.9517 | Train Acc: 67.71% | Val Loss: 0.3311 | Val Acc: 92.73%\n",
      "Epoch [43/200] | Train Loss: 0.9023 | Train Acc: 68.70% | Val Loss: 0.2977 | Val Acc: 92.16%\n",
      "Epoch [44/200] | Train Loss: 0.9133 | Train Acc: 68.69% | Val Loss: 0.2875 | Val Acc: 90.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2875) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [45/200] | Train Loss: 0.8993 | Train Acc: 69.12% | Val Loss: 0.2973 | Val Acc: 92.22%\n",
      "Epoch [46/200] | Train Loss: 0.8879 | Train Acc: 69.41% | Val Loss: 0.3078 | Val Acc: 90.49%\n",
      "Epoch [47/200] | Train Loss: 0.9074 | Train Acc: 69.39% | Val Loss: 0.3082 | Val Acc: 91.12%\n",
      "Epoch [48/200] | Train Loss: 0.8667 | Train Acc: 70.08% | Val Loss: 0.4070 | Val Acc: 85.12%\n",
      "Epoch [49/200] | Train Loss: 0.8975 | Train Acc: 70.05% | Val Loss: 0.2982 | Val Acc: 92.02%\n",
      "Epoch [50/200] | Train Loss: 0.8751 | Train Acc: 69.66% | Val Loss: 0.2807 | Val Acc: 92.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2807) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [51/200] | Train Loss: 0.8688 | Train Acc: 70.81% | Val Loss: 0.2765 | Val Acc: 91.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2765) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [52/200] | Train Loss: 0.8653 | Train Acc: 70.58% | Val Loss: 0.3319 | Val Acc: 90.22%\n",
      "Epoch [53/200] | Train Loss: 0.8577 | Train Acc: 71.10% | Val Loss: 0.2648 | Val Acc: 91.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2648) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [54/200] | Train Loss: 0.8489 | Train Acc: 70.65% | Val Loss: 0.2717 | Val Acc: 91.73%\n",
      "Epoch [55/200] | Train Loss: 0.8531 | Train Acc: 70.54% | Val Loss: 0.2745 | Val Acc: 91.96%\n",
      "Epoch [56/200] | Train Loss: 0.8320 | Train Acc: 72.02% | Val Loss: 0.2835 | Val Acc: 90.37%\n",
      "Epoch [57/200] | Train Loss: 0.8402 | Train Acc: 71.43% | Val Loss: 0.4315 | Val Acc: 85.24%\n",
      "Epoch [58/200] | Train Loss: 0.8263 | Train Acc: 71.83% | Val Loss: 0.2941 | Val Acc: 92.45%\n",
      "Epoch [59/200] | Train Loss: 0.8334 | Train Acc: 72.11% | Val Loss: 0.2973 | Val Acc: 93.59%\n",
      "Epoch [60/200] | Train Loss: 0.8156 | Train Acc: 72.38% | Val Loss: 0.3711 | Val Acc: 87.59%\n",
      "Epoch [61/200] | Train Loss: 0.8172 | Train Acc: 72.47% | Val Loss: 0.2717 | Val Acc: 92.59%\n",
      "Epoch [62/200] | Train Loss: 0.8116 | Train Acc: 72.17% | Val Loss: 0.2848 | Val Acc: 91.69%\n",
      "Epoch [63/200] | Train Loss: 0.7990 | Train Acc: 73.12% | Val Loss: 0.2893 | Val Acc: 92.33%\n",
      "Epoch [64/200] | Train Loss: 0.8007 | Train Acc: 73.29% | Val Loss: 0.2729 | Val Acc: 93.61%\n",
      "Epoch [65/200] | Train Loss: 0.7905 | Train Acc: 73.52% | Val Loss: 0.2831 | Val Acc: 91.24%\n",
      "Epoch [66/200] | Train Loss: 0.7988 | Train Acc: 72.61% | Val Loss: 0.2679 | Val Acc: 94.35%\n",
      "Epoch [67/200] | Train Loss: 0.7918 | Train Acc: 73.34% | Val Loss: 0.3026 | Val Acc: 92.57%\n",
      "Epoch [68/200] | Train Loss: 0.7846 | Train Acc: 74.32% | Val Loss: 0.2535 | Val Acc: 94.96%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2535) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [69/200] | Train Loss: 0.7726 | Train Acc: 74.49% | Val Loss: 0.2668 | Val Acc: 93.86%\n",
      "Epoch [70/200] | Train Loss: 0.7800 | Train Acc: 74.14% | Val Loss: 0.2612 | Val Acc: 94.27%\n",
      "Epoch [71/200] | Train Loss: 0.7776 | Train Acc: 74.47% | Val Loss: 0.2866 | Val Acc: 94.27%\n",
      "Epoch [72/200] | Train Loss: 0.7856 | Train Acc: 74.27% | Val Loss: 0.2356 | Val Acc: 95.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2356) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [73/200] | Train Loss: 0.7694 | Train Acc: 74.44% | Val Loss: 0.2563 | Val Acc: 94.39%\n",
      "Epoch [74/200] | Train Loss: 0.7497 | Train Acc: 74.76% | Val Loss: 0.2493 | Val Acc: 96.18%\n",
      "Epoch [75/200] | Train Loss: 0.7759 | Train Acc: 74.61% | Val Loss: 0.2778 | Val Acc: 95.18%\n",
      "Epoch [76/200] | Train Loss: 0.7709 | Train Acc: 74.53% | Val Loss: 0.2561 | Val Acc: 93.59%\n",
      "Epoch [77/200] | Train Loss: 0.7647 | Train Acc: 74.59% | Val Loss: 0.2650 | Val Acc: 93.47%\n",
      "Epoch [78/200] | Train Loss: 0.7685 | Train Acc: 74.65% | Val Loss: 0.2834 | Val Acc: 93.84%\n",
      "Epoch [79/200] | Train Loss: 0.7490 | Train Acc: 75.36% | Val Loss: 0.2408 | Val Acc: 95.02%\n",
      "Epoch [80/200] | Train Loss: 0.7379 | Train Acc: 75.73% | Val Loss: 0.2805 | Val Acc: 94.04%\n",
      "Epoch [81/200] | Train Loss: 0.7405 | Train Acc: 75.59% | Val Loss: 0.2786 | Val Acc: 94.16%\n",
      "Epoch [82/200] | Train Loss: 0.7306 | Train Acc: 76.10% | Val Loss: 0.2512 | Val Acc: 94.69%\n",
      "Epoch [83/200] | Train Loss: 0.7543 | Train Acc: 75.15% | Val Loss: 0.2327 | Val Acc: 96.98%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2327) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [84/200] | Train Loss: 0.7435 | Train Acc: 75.31% | Val Loss: 0.3043 | Val Acc: 93.94%\n",
      "Epoch [85/200] | Train Loss: 0.7513 | Train Acc: 75.78% | Val Loss: 0.2338 | Val Acc: 95.82%\n",
      "Epoch [86/200] | Train Loss: 0.7140 | Train Acc: 76.61% | Val Loss: 0.2132 | Val Acc: 94.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2132) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [87/200] | Train Loss: 0.7086 | Train Acc: 77.06% | Val Loss: 0.2148 | Val Acc: 96.67%\n",
      "Epoch [88/200] | Train Loss: 0.7421 | Train Acc: 75.98% | Val Loss: 0.2210 | Val Acc: 96.41%\n",
      "Epoch [89/200] | Train Loss: 0.7471 | Train Acc: 75.69% | Val Loss: 0.2237 | Val Acc: 94.31%\n",
      "Epoch [90/200] | Train Loss: 0.7074 | Train Acc: 77.03% | Val Loss: 0.2186 | Val Acc: 95.41%\n",
      "Epoch [91/200] | Train Loss: 0.7006 | Train Acc: 77.02% | Val Loss: 0.2058 | Val Acc: 96.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2058) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [92/200] | Train Loss: 0.6884 | Train Acc: 77.86% | Val Loss: 0.2272 | Val Acc: 95.43%\n",
      "Epoch [93/200] | Train Loss: 0.7239 | Train Acc: 76.63% | Val Loss: 0.3013 | Val Acc: 94.76%\n",
      "Epoch [94/200] | Train Loss: 0.7224 | Train Acc: 77.28% | Val Loss: 0.1946 | Val Acc: 97.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1946) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [95/200] | Train Loss: 0.7220 | Train Acc: 77.18% | Val Loss: 0.2011 | Val Acc: 96.65%\n",
      "Epoch [96/200] | Train Loss: 0.7196 | Train Acc: 76.74% | Val Loss: 0.2579 | Val Acc: 92.43%\n",
      "Epoch [97/200] | Train Loss: 0.6852 | Train Acc: 77.69% | Val Loss: 0.2182 | Val Acc: 96.06%\n",
      "Epoch [98/200] | Train Loss: 0.7184 | Train Acc: 77.22% | Val Loss: 0.2300 | Val Acc: 93.45%\n",
      "Epoch [99/200] | Train Loss: 0.6701 | Train Acc: 78.79% | Val Loss: 0.1972 | Val Acc: 97.06%\n",
      "Epoch [100/200] | Train Loss: 0.7087 | Train Acc: 77.27% | Val Loss: 0.2652 | Val Acc: 91.67%\n",
      "Epoch [101/200] | Train Loss: 0.7078 | Train Acc: 77.66% | Val Loss: 0.2285 | Val Acc: 95.39%\n",
      "Epoch [102/200] | Train Loss: 0.6896 | Train Acc: 77.97% | Val Loss: 0.1914 | Val Acc: 95.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1914) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [103/200] | Train Loss: 0.7087 | Train Acc: 77.90% | Val Loss: 0.1994 | Val Acc: 96.14%\n",
      "Epoch [104/200] | Train Loss: 0.6908 | Train Acc: 77.90% | Val Loss: 0.2406 | Val Acc: 96.27%\n",
      "Epoch [105/200] | Train Loss: 0.6926 | Train Acc: 78.49% | Val Loss: 0.2406 | Val Acc: 97.24%\n",
      "Epoch [106/200] | Train Loss: 0.6743 | Train Acc: 78.34% | Val Loss: 0.2609 | Val Acc: 94.04%\n",
      "Epoch [107/200] | Train Loss: 0.6624 | Train Acc: 78.87% | Val Loss: 0.2303 | Val Acc: 94.33%\n",
      "Epoch [108/200] | Train Loss: 0.6981 | Train Acc: 77.95% | Val Loss: 0.2280 | Val Acc: 95.96%\n",
      "Epoch [109/200] | Train Loss: 0.6774 | Train Acc: 78.15% | Val Loss: 0.2421 | Val Acc: 95.43%\n",
      "Epoch [110/200] | Train Loss: 0.6867 | Train Acc: 78.24% | Val Loss: 0.2481 | Val Acc: 96.41%\n",
      "Epoch [111/200] | Train Loss: 0.6744 | Train Acc: 78.50% | Val Loss: 0.2036 | Val Acc: 96.37%\n",
      "Epoch [112/200] | Train Loss: 0.6708 | Train Acc: 78.63% | Val Loss: 0.2339 | Val Acc: 96.63%\n",
      "Epoch [113/200] | Train Loss: 0.7043 | Train Acc: 77.99% | Val Loss: 0.2308 | Val Acc: 97.04%\n",
      "Epoch [114/200] | Train Loss: 0.6968 | Train Acc: 77.86% | Val Loss: 0.2131 | Val Acc: 95.18%\n",
      "Epoch [115/200] | Train Loss: 0.6521 | Train Acc: 78.93% | Val Loss: 0.2166 | Val Acc: 96.12%\n",
      "Epoch [116/200] | Train Loss: 0.6754 | Train Acc: 78.57% | Val Loss: 0.2109 | Val Acc: 96.53%\n",
      "Epoch [117/200] | Train Loss: 0.6555 | Train Acc: 79.13% | Val Loss: 0.1878 | Val Acc: 97.29%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1878) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [118/200] | Train Loss: 0.6531 | Train Acc: 79.19% | Val Loss: 0.2105 | Val Acc: 98.12%\n",
      "Epoch [119/200] | Train Loss: 0.7174 | Train Acc: 77.27% | Val Loss: 0.4686 | Val Acc: 83.86%\n",
      "Epoch [120/200] | Train Loss: 0.6843 | Train Acc: 78.41% | Val Loss: 0.2090 | Val Acc: 97.08%\n",
      "Epoch [121/200] | Train Loss: 0.6423 | Train Acc: 79.61% | Val Loss: 0.3405 | Val Acc: 89.51%\n",
      "Epoch [122/200] | Train Loss: 0.6718 | Train Acc: 78.72% | Val Loss: 0.2283 | Val Acc: 94.31%\n",
      "Epoch [123/200] | Train Loss: 0.6569 | Train Acc: 79.19% | Val Loss: 0.2321 | Val Acc: 96.31%\n",
      "Epoch [124/200] | Train Loss: 0.6377 | Train Acc: 79.98% | Val Loss: 0.2500 | Val Acc: 95.31%\n",
      "Epoch [125/200] | Train Loss: 0.6713 | Train Acc: 79.14% | Val Loss: 0.2099 | Val Acc: 95.80%\n",
      "Epoch [126/200] | Train Loss: 0.6625 | Train Acc: 79.05% | Val Loss: 0.2078 | Val Acc: 96.22%\n",
      "Epoch [127/200] | Train Loss: 0.6593 | Train Acc: 79.43% | Val Loss: 0.1847 | Val Acc: 96.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1847) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [128/200] | Train Loss: 0.6895 | Train Acc: 78.37% | Val Loss: 0.2112 | Val Acc: 95.00%\n",
      "Epoch [129/200] | Train Loss: 0.6582 | Train Acc: 78.95% | Val Loss: 0.2164 | Val Acc: 94.71%\n",
      "Epoch [130/200] | Train Loss: 0.6566 | Train Acc: 79.26% | Val Loss: 0.2106 | Val Acc: 96.00%\n",
      "Epoch [131/200] | Train Loss: 0.6307 | Train Acc: 79.97% | Val Loss: 0.2282 | Val Acc: 94.86%\n",
      "Epoch [132/200] | Train Loss: 0.6453 | Train Acc: 79.41% | Val Loss: 0.1785 | Val Acc: 97.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1785) 至 ./models_save/rssi_csi_cls_gated_0521_no_std_rssi64.pth\n",
      "Epoch [133/200] | Train Loss: 0.6323 | Train Acc: 80.14% | Val Loss: 0.1980 | Val Acc: 97.49%\n",
      "Epoch [134/200] | Train Loss: 0.6370 | Train Acc: 79.90% | Val Loss: 0.1915 | Val Acc: 96.67%\n",
      "Epoch [135/200] | Train Loss: 0.6701 | Train Acc: 79.22% | Val Loss: 0.1923 | Val Acc: 96.24%\n",
      "Epoch [136/200] | Train Loss: 0.6605 | Train Acc: 79.55% | Val Loss: 0.2017 | Val Acc: 95.82%\n",
      "Epoch [137/200] | Train Loss: 0.6436 | Train Acc: 79.24% | Val Loss: 0.1870 | Val Acc: 97.02%\n",
      "Epoch [138/200] | Train Loss: 0.6558 | Train Acc: 79.41% | Val Loss: 0.2469 | Val Acc: 95.22%\n",
      "Epoch [139/200] | Train Loss: 0.6484 | Train Acc: 79.71% | Val Loss: 0.2173 | Val Acc: 97.35%\n",
      "Epoch [140/200] | Train Loss: 0.6467 | Train Acc: 79.80% | Val Loss: 0.2232 | Val Acc: 95.47%\n",
      "Epoch [141/200] | Train Loss: 0.6336 | Train Acc: 80.34% | Val Loss: 0.2113 | Val Acc: 96.12%\n",
      "Epoch [142/200] | Train Loss: 0.6514 | Train Acc: 79.90% | Val Loss: 0.2032 | Val Acc: 96.94%\n",
      "Epoch [143/200] | Train Loss: 0.6642 | Train Acc: 79.52% | Val Loss: 0.2241 | Val Acc: 96.84%\n",
      "Epoch [144/200] | Train Loss: 0.6579 | Train Acc: 79.44% | Val Loss: 0.1955 | Val Acc: 96.35%\n",
      "Epoch [145/200] | Train Loss: 0.6486 | Train Acc: 79.78% | Val Loss: 0.2118 | Val Acc: 96.37%\n",
      "Epoch [146/200] | Train Loss: 0.6269 | Train Acc: 80.34% | Val Loss: 0.1935 | Val Acc: 96.88%\n",
      "Epoch [147/200] | Train Loss: 0.6260 | Train Acc: 80.59% | Val Loss: 0.1865 | Val Acc: 97.65%\n",
      "Epoch [148/200] | Train Loss: 0.6345 | Train Acc: 80.31% | Val Loss: 0.1930 | Val Acc: 97.84%\n",
      "Epoch [149/200] | Train Loss: 0.6315 | Train Acc: 80.36% | Val Loss: 0.2061 | Val Acc: 97.22%\n",
      "Epoch [150/200] | Train Loss: 0.6330 | Train Acc: 79.88% | Val Loss: 0.1959 | Val Acc: 97.35%\n",
      "Epoch [151/200] | Train Loss: 0.6402 | Train Acc: 80.39% | Val Loss: 0.2438 | Val Acc: 97.22%\n",
      "Epoch [152/200] | Train Loss: 0.6505 | Train Acc: 80.06% | Val Loss: 0.2635 | Val Acc: 96.18%\n",
      "Early stop at epoch 152\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 儲存最佳模型相關設定\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_0520_no_std_rssi.pth\"\n",
    "\n",
    "# 訓練參數\n",
    "epochs = 200\n",
    "\n",
    "# Early Stopping 參數\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "# 紀錄訓練過程中的 loss 和 accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- 訓練階段 ----\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 注意：這裡每個 batch 返回三個項目：CSI (amp)、RSSI 與 labels\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        # 將資料移到 device 上\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        # 傳入兩個輸入到模型 (CSI 與 RSSI)\n",
    "        outputs = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # CrossEntropyLoss 需要 target 為 class index\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # 更新訓練 loss 與正確數\n",
    "        train_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        train_correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    # ---- 驗證階段 ----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(amp_inputs, rssi_inputs)\n",
    "            loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "            val_loss += loss.item() * amp_inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            val_correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    # 紀錄每個 epoch 的數值\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    # 輸出當前 epoch 的結果\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    # 儲存最佳模型\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 雙輸出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型摘要:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CSIRSSI_DualHead                         [1, 49]                   --\n",
       "├─Conv1d: 1-1                            [1, 64, 48]               256\n",
       "├─BatchNorm1d: 1-2                       [1, 64, 48]               128\n",
       "├─MaxPool1d: 1-3                         [1, 64, 24]               --\n",
       "├─Conv1d: 1-4                            [1, 128, 24]              24,704\n",
       "├─BatchNorm1d: 1-5                       [1, 128, 24]              256\n",
       "├─MaxPool1d: 1-6                         [1, 128, 12]              --\n",
       "├─Linear: 1-7                            [1, 128]                  196,736\n",
       "├─Dropout: 1-8                           [1, 128]                  --\n",
       "├─Linear: 1-9                            [1, 64]                   8,256\n",
       "├─Dropout: 1-10                          [1, 64]                   --\n",
       "├─Linear: 1-11                           [1, 32]                   160\n",
       "├─Dropout: 1-12                          [1, 32]                   --\n",
       "├─Linear: 1-13                           [1, 32]                   1,056\n",
       "├─Dropout: 1-14                          [1, 32]                   --\n",
       "├─Linear: 1-15                           [1, 64]                   6,208\n",
       "├─Dropout: 1-16                          [1, 64]                   --\n",
       "├─Linear: 1-17                           [1, 49]                   3,185\n",
       "├─Linear: 1-18                           [1, 2]                    130\n",
       "==========================================================================================\n",
       "Total params: 241,075\n",
       "Trainable params: 241,075\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.82\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.96\n",
       "Estimated Total Size (MB): 1.07\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary  # torchinfo 可用來顯示多輸入模型摘要\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# 模型定義：CSIRSSI_DualHead\n",
    "# -----------------------\n",
    "class CSIRSSI_DualHead(nn.Module):\n",
    "    def __init__(self, num_classes=49, rssi_dim=4):\n",
    "        super(CSIRSSI_DualHead, self).__init__()\n",
    "        # ---- CSI 分支 (CNN) ---\n",
    "        # 假設輸入 CSI shape 為 (batch, 1, 48)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 輸入長度 48 → 經過兩次 pooling → 48/2=24，再 24/2=12\n",
    "        self.flatten_dim = 128 * 12\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.flatten_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2   = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        # 此時 CSI 分支輸出 64 維特徵\n",
    "        \n",
    "        # ---- RSSI 分支 (MLP) ----\n",
    "        self.fc_rssi1 = nn.Linear(rssi_dim, 32)\n",
    "        self.dropout_rssi1 = nn.Dropout(0.5)\n",
    "        self.fc_rssi2 = nn.Linear(32, 32)\n",
    "        self.dropout_rssi2 = nn.Dropout(0.5)\n",
    "        # RSSI 分支輸出 32 維特徵\n",
    "        \n",
    "        # ---- 融合層 ----\n",
    "        # 將 CSI (64-d) 與 RSSI (32-d) 連接 → 96-d\n",
    "        self.fc_fusion = nn.Linear(64+32, 64)\n",
    "        self.dropout_fusion = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- 雙輸出頭 ----\n",
    "        # 分類頭：輸出 num_classes 個類別的 logits\n",
    "        self.fc_class = nn.Linear(64, num_classes)\n",
    "        # 回歸頭：輸出 2 個數值 (X, Y)\n",
    "        self.fc_reg = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, csi_input, rssi_input):\n",
    "        # CSI 分支\n",
    "        # 如果輸入為 (batch, 48)，則擴展成 (batch, 1, 48)\n",
    "        if csi_input.dim() == 2:\n",
    "            csi_input = csi_input.unsqueeze(1)\n",
    "        x = F.relu(self.bn1(self.conv1(csi_input)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten → (batch, flatten_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        csi_feat = F.relu(self.fc2(x))\n",
    "        csi_feat = self.dropout2(csi_feat)\n",
    "        \n",
    "        # RSSI 分支\n",
    "        rssi_feat = F.relu(self.fc_rssi1(rssi_input))\n",
    "        rssi_feat = self.dropout_rssi1(rssi_feat)\n",
    "        rssi_feat = F.relu(self.fc_rssi2(rssi_feat))\n",
    "        rssi_feat = self.dropout_rssi2(rssi_feat)\n",
    "        \n",
    "        # 融合特徵\n",
    "        fusion = torch.cat([csi_feat, rssi_feat], dim=1)  # (batch, 96)\n",
    "        fusion = F.relu(self.fc_fusion(fusion))\n",
    "        fusion = self.dropout_fusion(fusion)\n",
    "        \n",
    "        # 雙輸出頭\n",
    "        class_out = self.fc_class(fusion)  # 分類輸出 (batch, num_classes)\n",
    "        reg_out = self.fc_reg(fusion)       # 回歸輸出 (batch, 2)\n",
    "        \n",
    "        return class_out, reg_out\n",
    "\n",
    "# -----------------------\n",
    "# 模型初始化與摘要\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CSIRSSI_DualHead(num_classes=49, rssi_dim=4).to(device)\n",
    "\n",
    "print(\"模型摘要:\")\n",
    "summary(model, input_data=(torch.randn(1, 1, 48).to(device), torch.randn(1, 4).to(device)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義apha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 6.2515 (Cls: 3.5434, Reg: 5.4161) | Train Acc: 9.53% || Val Loss: 3.9222 (Cls: 2.1471, Reg: 3.5502) | Val Acc: 36.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 3.9222) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [2/300] | Train Loss: 3.7535 (Cls: 1.9201, Reg: 3.6669) | Train Acc: 37.13% || Val Loss: 1.7323 (Cls: 0.8134, Reg: 1.8379) | Val Acc: 83.41%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.7323) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [3/300] | Train Loss: 2.5138 (Cls: 1.1533, Reg: 2.7211) | Train Acc: 61.05% || Val Loss: 1.1783 (Cls: 0.4148, Reg: 1.5269) | Val Acc: 92.08%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.1783) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [4/300] | Train Loss: 1.9852 (Cls: 0.8508, Reg: 2.2688) | Train Acc: 71.31% || Val Loss: 0.9002 (Cls: 0.2721, Reg: 1.2562) | Val Acc: 94.96%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9002) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [5/300] | Train Loss: 1.6900 (Cls: 0.6804, Reg: 2.0193) | Train Acc: 76.86% || Val Loss: 0.6235 (Cls: 0.2063, Reg: 0.8344) | Val Acc: 95.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6235) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [6/300] | Train Loss: 1.4931 (Cls: 0.5815, Reg: 1.8232) | Train Acc: 80.67% || Val Loss: 0.5594 (Cls: 0.1473, Reg: 0.8241) | Val Acc: 97.18%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5594) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [7/300] | Train Loss: 1.3349 (Cls: 0.4967, Reg: 1.6764) | Train Acc: 83.80% || Val Loss: 0.5161 (Cls: 0.1216, Reg: 0.7891) | Val Acc: 96.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5161) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [8/300] | Train Loss: 1.2010 (Cls: 0.4375, Reg: 1.5269) | Train Acc: 86.01% || Val Loss: 0.4797 (Cls: 0.0886, Reg: 0.7821) | Val Acc: 99.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4797) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [9/300] | Train Loss: 1.1453 (Cls: 0.4127, Reg: 1.4651) | Train Acc: 87.20% || Val Loss: 0.5899 (Cls: 0.0862, Reg: 1.0073) | Val Acc: 99.06%\n",
      "Epoch [10/300] | Train Loss: 1.0656 (Cls: 0.3749, Reg: 1.3813) | Train Acc: 88.73% || Val Loss: 0.2760 (Cls: 0.0669, Reg: 0.4182) | Val Acc: 99.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2760) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [11/300] | Train Loss: 0.9837 (Cls: 0.3407, Reg: 1.2860) | Train Acc: 89.70% || Val Loss: 0.3112 (Cls: 0.0517, Reg: 0.5190) | Val Acc: 99.31%\n",
      "Epoch [12/300] | Train Loss: 0.9311 (Cls: 0.3103, Reg: 1.2417) | Train Acc: 90.93% || Val Loss: 0.2895 (Cls: 0.0452, Reg: 0.4886) | Val Acc: 99.47%\n",
      "Epoch [13/300] | Train Loss: 0.9016 (Cls: 0.3013, Reg: 1.2005) | Train Acc: 91.14% || Val Loss: 0.5389 (Cls: 0.0821, Reg: 0.9135) | Val Acc: 98.51%\n",
      "Epoch [14/300] | Train Loss: 0.8530 (Cls: 0.2797, Reg: 1.1466) | Train Acc: 91.74% || Val Loss: 0.2066 (Cls: 0.0357, Reg: 0.3418) | Val Acc: 99.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2066) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [15/300] | Train Loss: 0.8257 (Cls: 0.2674, Reg: 1.1164) | Train Acc: 92.22% || Val Loss: 0.2164 (Cls: 0.0301, Reg: 0.3726) | Val Acc: 99.61%\n",
      "Epoch [16/300] | Train Loss: 0.8153 (Cls: 0.2696, Reg: 1.0914) | Train Acc: 92.13% || Val Loss: 0.2398 (Cls: 0.0354, Reg: 0.4088) | Val Acc: 99.37%\n",
      "Epoch [17/300] | Train Loss: 0.7820 (Cls: 0.2570, Reg: 1.0500) | Train Acc: 92.65% || Val Loss: 0.5282 (Cls: 0.0817, Reg: 0.8930) | Val Acc: 97.86%\n",
      "Epoch [18/300] | Train Loss: 0.7414 (Cls: 0.2372, Reg: 1.0084) | Train Acc: 93.48% || Val Loss: 0.1919 (Cls: 0.0254, Reg: 0.3331) | Val Acc: 99.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1919) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [19/300] | Train Loss: 0.7329 (Cls: 0.2355, Reg: 0.9947) | Train Acc: 93.18% || Val Loss: 0.1875 (Cls: 0.0277, Reg: 0.3197) | Val Acc: 99.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1875) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [20/300] | Train Loss: 0.7328 (Cls: 0.2409, Reg: 0.9839) | Train Acc: 93.07% || Val Loss: 0.1977 (Cls: 0.0298, Reg: 0.3357) | Val Acc: 99.57%\n",
      "Epoch [21/300] | Train Loss: 0.6906 (Cls: 0.2213, Reg: 0.9387) | Train Acc: 93.62% || Val Loss: 0.2143 (Cls: 0.0255, Reg: 0.3776) | Val Acc: 99.53%\n",
      "Epoch [22/300] | Train Loss: 0.6749 (Cls: 0.2097, Reg: 0.9305) | Train Acc: 94.07% || Val Loss: 0.2843 (Cls: 0.0316, Reg: 0.5055) | Val Acc: 99.49%\n",
      "Epoch [23/300] | Train Loss: 0.6859 (Cls: 0.2188, Reg: 0.9342) | Train Acc: 93.72% || Val Loss: 0.1515 (Cls: 0.0279, Reg: 0.2472) | Val Acc: 99.41%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1515) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [24/300] | Train Loss: 0.6563 (Cls: 0.2080, Reg: 0.8965) | Train Acc: 93.88% || Val Loss: 0.1402 (Cls: 0.0280, Reg: 0.2246) | Val Acc: 99.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1402) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [25/300] | Train Loss: 0.6659 (Cls: 0.2166, Reg: 0.8985) | Train Acc: 93.75% || Val Loss: 0.1721 (Cls: 0.0290, Reg: 0.2862) | Val Acc: 99.53%\n",
      "Epoch [26/300] | Train Loss: 0.6331 (Cls: 0.1993, Reg: 0.8675) | Train Acc: 94.18% || Val Loss: 0.1424 (Cls: 0.0328, Reg: 0.2193) | Val Acc: 99.45%\n",
      "Epoch [27/300] | Train Loss: 0.6219 (Cls: 0.1980, Reg: 0.8478) | Train Acc: 94.31% || Val Loss: 0.1453 (Cls: 0.0251, Reg: 0.2402) | Val Acc: 99.61%\n",
      "Epoch [28/300] | Train Loss: 0.6157 (Cls: 0.1913, Reg: 0.8489) | Train Acc: 94.66% || Val Loss: 0.1269 (Cls: 0.0236, Reg: 0.2066) | Val Acc: 99.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1269) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [29/300] | Train Loss: 0.5998 (Cls: 0.1871, Reg: 0.8253) | Train Acc: 94.86% || Val Loss: 0.1520 (Cls: 0.0275, Reg: 0.2491) | Val Acc: 99.57%\n",
      "Epoch [30/300] | Train Loss: 0.6096 (Cls: 0.1924, Reg: 0.8343) | Train Acc: 94.52% || Val Loss: 0.1065 (Cls: 0.0220, Reg: 0.1691) | Val Acc: 99.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1065) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [31/300] | Train Loss: 0.6045 (Cls: 0.1946, Reg: 0.8198) | Train Acc: 94.66% || Val Loss: 0.1241 (Cls: 0.0209, Reg: 0.2064) | Val Acc: 99.69%\n",
      "Epoch [32/300] | Train Loss: 0.5784 (Cls: 0.1804, Reg: 0.7961) | Train Acc: 95.11% || Val Loss: 0.1370 (Cls: 0.0269, Reg: 0.2202) | Val Acc: 99.61%\n",
      "Epoch [33/300] | Train Loss: 0.5844 (Cls: 0.1880, Reg: 0.7929) | Train Acc: 94.83% || Val Loss: 0.2705 (Cls: 0.0487, Reg: 0.4436) | Val Acc: 98.96%\n",
      "Epoch [34/300] | Train Loss: 0.5646 (Cls: 0.1807, Reg: 0.7677) | Train Acc: 94.94% || Val Loss: 0.1347 (Cls: 0.0252, Reg: 0.2189) | Val Acc: 99.47%\n",
      "Epoch [35/300] | Train Loss: 0.5468 (Cls: 0.1662, Reg: 0.7612) | Train Acc: 95.06% || Val Loss: 0.1113 (Cls: 0.0249, Reg: 0.1728) | Val Acc: 99.67%\n",
      "Epoch [36/300] | Train Loss: 0.5690 (Cls: 0.1845, Reg: 0.7691) | Train Acc: 94.73% || Val Loss: 0.1073 (Cls: 0.0224, Reg: 0.1698) | Val Acc: 99.67%\n",
      "Epoch [37/300] | Train Loss: 0.5603 (Cls: 0.1777, Reg: 0.7650) | Train Acc: 95.01% || Val Loss: 0.1147 (Cls: 0.0274, Reg: 0.1747) | Val Acc: 99.55%\n",
      "Epoch [38/300] | Train Loss: 0.5418 (Cls: 0.1649, Reg: 0.7537) | Train Acc: 95.38% || Val Loss: 0.1063 (Cls: 0.0280, Reg: 0.1565) | Val Acc: 99.65%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1063) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [39/300] | Train Loss: 0.5357 (Cls: 0.1648, Reg: 0.7420) | Train Acc: 95.38% || Val Loss: 0.1121 (Cls: 0.0241, Reg: 0.1759) | Val Acc: 99.59%\n",
      "Epoch [40/300] | Train Loss: 0.5249 (Cls: 0.1610, Reg: 0.7279) | Train Acc: 95.49% || Val Loss: 0.1068 (Cls: 0.0264, Reg: 0.1608) | Val Acc: 99.59%\n",
      "Epoch [41/300] | Train Loss: 0.5377 (Cls: 0.1653, Reg: 0.7448) | Train Acc: 95.39% || Val Loss: 0.1531 (Cls: 0.0314, Reg: 0.2434) | Val Acc: 99.55%\n",
      "Epoch [42/300] | Train Loss: 0.5269 (Cls: 0.1646, Reg: 0.7246) | Train Acc: 95.32% || Val Loss: 0.1345 (Cls: 0.0310, Reg: 0.2069) | Val Acc: 99.63%\n",
      "Epoch [43/300] | Train Loss: 0.5465 (Cls: 0.1774, Reg: 0.7381) | Train Acc: 95.13% || Val Loss: 0.0961 (Cls: 0.0248, Reg: 0.1426) | Val Acc: 99.65%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0961) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [44/300] | Train Loss: 0.5183 (Cls: 0.1574, Reg: 0.7218) | Train Acc: 95.50% || Val Loss: 0.1066 (Cls: 0.0222, Reg: 0.1687) | Val Acc: 99.65%\n",
      "Epoch [45/300] | Train Loss: 0.5198 (Cls: 0.1602, Reg: 0.7192) | Train Acc: 95.60% || Val Loss: 0.1198 (Cls: 0.0253, Reg: 0.1891) | Val Acc: 99.59%\n",
      "Epoch [46/300] | Train Loss: 0.5122 (Cls: 0.1614, Reg: 0.7015) | Train Acc: 95.55% || Val Loss: 0.1105 (Cls: 0.0252, Reg: 0.1704) | Val Acc: 99.61%\n",
      "Epoch [47/300] | Train Loss: 0.5151 (Cls: 0.1573, Reg: 0.7155) | Train Acc: 95.59% || Val Loss: 0.0880 (Cls: 0.0232, Reg: 0.1296) | Val Acc: 99.67%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0880) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [48/300] | Train Loss: 0.4819 (Cls: 0.1432, Reg: 0.6772) | Train Acc: 96.05% || Val Loss: 0.0688 (Cls: 0.0241, Reg: 0.0895) | Val Acc: 99.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0688) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [49/300] | Train Loss: 0.5113 (Cls: 0.1565, Reg: 0.7097) | Train Acc: 95.76% || Val Loss: 0.0767 (Cls: 0.0231, Reg: 0.1072) | Val Acc: 99.65%\n",
      "Epoch [50/300] | Train Loss: 0.5063 (Cls: 0.1573, Reg: 0.6981) | Train Acc: 95.67% || Val Loss: 0.1235 (Cls: 0.0365, Reg: 0.1741) | Val Acc: 99.22%\n",
      "Epoch [51/300] | Train Loss: 0.5016 (Cls: 0.1564, Reg: 0.6904) | Train Acc: 95.72% || Val Loss: 0.0922 (Cls: 0.0278, Reg: 0.1287) | Val Acc: 99.65%\n",
      "Epoch [52/300] | Train Loss: 0.4821 (Cls: 0.1441, Reg: 0.6759) | Train Acc: 95.91% || Val Loss: 0.0829 (Cls: 0.0251, Reg: 0.1157) | Val Acc: 99.63%\n",
      "Epoch [53/300] | Train Loss: 0.4923 (Cls: 0.1527, Reg: 0.6793) | Train Acc: 95.79% || Val Loss: 0.0962 (Cls: 0.0261, Reg: 0.1401) | Val Acc: 99.61%\n",
      "Epoch [54/300] | Train Loss: 0.4808 (Cls: 0.1474, Reg: 0.6669) | Train Acc: 95.99% || Val Loss: 0.0841 (Cls: 0.0316, Reg: 0.1049) | Val Acc: 99.67%\n",
      "Epoch [55/300] | Train Loss: 0.4944 (Cls: 0.1500, Reg: 0.6888) | Train Acc: 95.94% || Val Loss: 0.0890 (Cls: 0.0247, Reg: 0.1288) | Val Acc: 99.67%\n",
      "Epoch [56/300] | Train Loss: 0.4850 (Cls: 0.1496, Reg: 0.6709) | Train Acc: 95.81% || Val Loss: 0.1400 (Cls: 0.0373, Reg: 0.2054) | Val Acc: 99.22%\n",
      "Epoch [57/300] | Train Loss: 0.4794 (Cls: 0.1460, Reg: 0.6668) | Train Acc: 95.90% || Val Loss: 0.0881 (Cls: 0.0263, Reg: 0.1236) | Val Acc: 99.61%\n",
      "Epoch [58/300] | Train Loss: 0.4844 (Cls: 0.1501, Reg: 0.6687) | Train Acc: 95.98% || Val Loss: 0.0896 (Cls: 0.0247, Reg: 0.1299) | Val Acc: 99.73%\n",
      "Epoch [59/300] | Train Loss: 0.4715 (Cls: 0.1425, Reg: 0.6578) | Train Acc: 96.06% || Val Loss: 0.1041 (Cls: 0.0285, Reg: 0.1513) | Val Acc: 99.61%\n",
      "Epoch [60/300] | Train Loss: 0.4679 (Cls: 0.1466, Reg: 0.6425) | Train Acc: 96.13% || Val Loss: 0.1744 (Cls: 0.0374, Reg: 0.2742) | Val Acc: 99.12%\n",
      "Epoch [61/300] | Train Loss: 0.4681 (Cls: 0.1446, Reg: 0.6471) | Train Acc: 96.13% || Val Loss: 0.0850 (Cls: 0.0242, Reg: 0.1216) | Val Acc: 99.67%\n",
      "Epoch [62/300] | Train Loss: 0.4670 (Cls: 0.1416, Reg: 0.6508) | Train Acc: 95.98% || Val Loss: 0.1082 (Cls: 0.0332, Reg: 0.1499) | Val Acc: 99.57%\n",
      "Epoch [63/300] | Train Loss: 0.4757 (Cls: 0.1489, Reg: 0.6536) | Train Acc: 95.85% || Val Loss: 0.0742 (Cls: 0.0273, Reg: 0.0938) | Val Acc: 99.63%\n",
      "Epoch [64/300] | Train Loss: 0.4768 (Cls: 0.1512, Reg: 0.6512) | Train Acc: 95.84% || Val Loss: 0.0925 (Cls: 0.0309, Reg: 0.1232) | Val Acc: 99.63%\n",
      "Epoch [65/300] | Train Loss: 0.4168 (Cls: 0.1220, Reg: 0.5896) | Train Acc: 96.62% || Val Loss: 0.0661 (Cls: 0.0220, Reg: 0.0881) | Val Acc: 99.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0661) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [66/300] | Train Loss: 0.3942 (Cls: 0.1055, Reg: 0.5775) | Train Acc: 96.92% || Val Loss: 0.0714 (Cls: 0.0289, Reg: 0.0850) | Val Acc: 99.67%\n",
      "Epoch [67/300] | Train Loss: 0.3986 (Cls: 0.1084, Reg: 0.5805) | Train Acc: 97.05% || Val Loss: 0.0978 (Cls: 0.0236, Reg: 0.1483) | Val Acc: 99.65%\n",
      "Epoch [68/300] | Train Loss: 0.3890 (Cls: 0.1068, Reg: 0.5645) | Train Acc: 97.11% || Val Loss: 0.0761 (Cls: 0.0229, Reg: 0.1064) | Val Acc: 99.71%\n",
      "Epoch [69/300] | Train Loss: 0.3924 (Cls: 0.1097, Reg: 0.5653) | Train Acc: 97.00% || Val Loss: 0.0658 (Cls: 0.0214, Reg: 0.0888) | Val Acc: 99.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0658) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [70/300] | Train Loss: 0.3913 (Cls: 0.1065, Reg: 0.5698) | Train Acc: 97.04% || Val Loss: 0.0683 (Cls: 0.0272, Reg: 0.0822) | Val Acc: 99.71%\n",
      "Epoch [71/300] | Train Loss: 0.3980 (Cls: 0.1100, Reg: 0.5759) | Train Acc: 97.00% || Val Loss: 0.0714 (Cls: 0.0231, Reg: 0.0966) | Val Acc: 99.73%\n",
      "Epoch [72/300] | Train Loss: 0.3813 (Cls: 0.1023, Reg: 0.5581) | Train Acc: 97.15% || Val Loss: 0.0648 (Cls: 0.0195, Reg: 0.0906) | Val Acc: 99.76%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0648) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [73/300] | Train Loss: 0.3960 (Cls: 0.1116, Reg: 0.5688) | Train Acc: 96.85% || Val Loss: 0.0693 (Cls: 0.0239, Reg: 0.0908) | Val Acc: 99.71%\n",
      "Epoch [74/300] | Train Loss: 0.3909 (Cls: 0.1080, Reg: 0.5657) | Train Acc: 96.99% || Val Loss: 0.0761 (Cls: 0.0188, Reg: 0.1147) | Val Acc: 99.73%\n",
      "Epoch [75/300] | Train Loss: 0.3828 (Cls: 0.1038, Reg: 0.5579) | Train Acc: 97.32% || Val Loss: 0.0733 (Cls: 0.0212, Reg: 0.1042) | Val Acc: 99.65%\n",
      "Epoch [76/300] | Train Loss: 0.3795 (Cls: 0.1017, Reg: 0.5554) | Train Acc: 97.06% || Val Loss: 0.0717 (Cls: 0.0190, Reg: 0.1054) | Val Acc: 99.73%\n",
      "Epoch [77/300] | Train Loss: 0.3900 (Cls: 0.1077, Reg: 0.5646) | Train Acc: 96.99% || Val Loss: 0.0652 (Cls: 0.0224, Reg: 0.0856) | Val Acc: 99.69%\n",
      "Epoch [78/300] | Train Loss: 0.3823 (Cls: 0.1023, Reg: 0.5601) | Train Acc: 97.25% || Val Loss: 0.0631 (Cls: 0.0228, Reg: 0.0805) | Val Acc: 99.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0631) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [79/300] | Train Loss: 0.3591 (Cls: 0.0908, Reg: 0.5367) | Train Acc: 97.41% || Val Loss: 0.0566 (Cls: 0.0178, Reg: 0.0777) | Val Acc: 99.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0566) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [80/300] | Train Loss: 0.3744 (Cls: 0.1019, Reg: 0.5450) | Train Acc: 97.15% || Val Loss: 0.0602 (Cls: 0.0155, Reg: 0.0895) | Val Acc: 99.71%\n",
      "Epoch [81/300] | Train Loss: 0.3873 (Cls: 0.1124, Reg: 0.5497) | Train Acc: 96.94% || Val Loss: 0.0566 (Cls: 0.0223, Reg: 0.0686) | Val Acc: 99.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0566) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [82/300] | Train Loss: 0.3655 (Cls: 0.0920, Reg: 0.5471) | Train Acc: 97.41% || Val Loss: 0.0633 (Cls: 0.0223, Reg: 0.0821) | Val Acc: 99.73%\n",
      "Epoch [83/300] | Train Loss: 0.3747 (Cls: 0.1027, Reg: 0.5440) | Train Acc: 97.31% || Val Loss: 0.0658 (Cls: 0.0205, Reg: 0.0905) | Val Acc: 99.73%\n",
      "Epoch [84/300] | Train Loss: 0.3840 (Cls: 0.1049, Reg: 0.5581) | Train Acc: 97.15% || Val Loss: 0.0642 (Cls: 0.0278, Reg: 0.0728) | Val Acc: 99.76%\n",
      "Epoch [85/300] | Train Loss: 0.3687 (Cls: 0.0976, Reg: 0.5422) | Train Acc: 97.41% || Val Loss: 0.0577 (Cls: 0.0192, Reg: 0.0770) | Val Acc: 99.65%\n",
      "Epoch [86/300] | Train Loss: 0.3658 (Cls: 0.0953, Reg: 0.5410) | Train Acc: 97.46% || Val Loss: 0.0688 (Cls: 0.0215, Reg: 0.0945) | Val Acc: 99.65%\n",
      "Epoch [87/300] | Train Loss: 0.3817 (Cls: 0.1065, Reg: 0.5504) | Train Acc: 97.21% || Val Loss: 0.0672 (Cls: 0.0219, Reg: 0.0905) | Val Acc: 99.69%\n",
      "Epoch [88/300] | Train Loss: 0.3844 (Cls: 0.1105, Reg: 0.5479) | Train Acc: 97.00% || Val Loss: 0.0903 (Cls: 0.0244, Reg: 0.1320) | Val Acc: 99.61%\n",
      "Epoch [89/300] | Train Loss: 0.3769 (Cls: 0.1027, Reg: 0.5483) | Train Acc: 97.22% || Val Loss: 0.0693 (Cls: 0.0253, Reg: 0.0881) | Val Acc: 99.73%\n",
      "Epoch [90/300] | Train Loss: 0.3623 (Cls: 0.0970, Reg: 0.5306) | Train Acc: 97.21% || Val Loss: 0.0721 (Cls: 0.0242, Reg: 0.0959) | Val Acc: 99.80%\n",
      "Epoch [91/300] | Train Loss: 0.3607 (Cls: 0.0944, Reg: 0.5325) | Train Acc: 97.36% || Val Loss: 0.0626 (Cls: 0.0256, Reg: 0.0740) | Val Acc: 99.76%\n",
      "Epoch [92/300] | Train Loss: 0.3590 (Cls: 0.0905, Reg: 0.5370) | Train Acc: 97.55% || Val Loss: 0.0746 (Cls: 0.0289, Reg: 0.0912) | Val Acc: 99.71%\n",
      "Epoch [93/300] | Train Loss: 0.3645 (Cls: 0.0984, Reg: 0.5323) | Train Acc: 97.27% || Val Loss: 0.0679 (Cls: 0.0273, Reg: 0.0811) | Val Acc: 99.67%\n",
      "Epoch [94/300] | Train Loss: 0.3592 (Cls: 0.0919, Reg: 0.5346) | Train Acc: 97.49% || Val Loss: 0.0625 (Cls: 0.0246, Reg: 0.0759) | Val Acc: 99.73%\n",
      "Epoch [95/300] | Train Loss: 0.3757 (Cls: 0.1061, Reg: 0.5392) | Train Acc: 97.27% || Val Loss: 0.0627 (Cls: 0.0218, Reg: 0.0819) | Val Acc: 99.69%\n",
      "Epoch [96/300] | Train Loss: 0.3530 (Cls: 0.0906, Reg: 0.5248) | Train Acc: 97.46% || Val Loss: 0.0560 (Cls: 0.0236, Reg: 0.0648) | Val Acc: 99.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0560) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [97/300] | Train Loss: 0.3422 (Cls: 0.0843, Reg: 0.5156) | Train Acc: 97.67% || Val Loss: 0.0521 (Cls: 0.0221, Reg: 0.0599) | Val Acc: 99.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0521) 至 ./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\n",
      "Epoch [98/300] | Train Loss: 0.3336 (Cls: 0.0801, Reg: 0.5070) | Train Acc: 97.81% || Val Loss: 0.0697 (Cls: 0.0250, Reg: 0.0893) | Val Acc: 99.67%\n",
      "Epoch [99/300] | Train Loss: 0.3273 (Cls: 0.0739, Reg: 0.5068) | Train Acc: 97.88% || Val Loss: 0.1232 (Cls: 0.0290, Reg: 0.1884) | Val Acc: 99.55%\n",
      "Epoch [100/300] | Train Loss: 0.3305 (Cls: 0.0801, Reg: 0.5008) | Train Acc: 97.85% || Val Loss: 0.0588 (Cls: 0.0285, Reg: 0.0606) | Val Acc: 99.67%\n",
      "Epoch [101/300] | Train Loss: 0.3212 (Cls: 0.0748, Reg: 0.4929) | Train Acc: 97.68% || Val Loss: 0.0654 (Cls: 0.0223, Reg: 0.0863) | Val Acc: 99.73%\n",
      "Epoch [102/300] | Train Loss: 0.3345 (Cls: 0.0799, Reg: 0.5094) | Train Acc: 97.78% || Val Loss: 0.0602 (Cls: 0.0266, Reg: 0.0674) | Val Acc: 99.71%\n",
      "Epoch [103/300] | Train Loss: 0.3312 (Cls: 0.0828, Reg: 0.4968) | Train Acc: 97.75% || Val Loss: 0.0638 (Cls: 0.0214, Reg: 0.0848) | Val Acc: 99.73%\n",
      "Epoch [104/300] | Train Loss: 0.3344 (Cls: 0.0841, Reg: 0.5005) | Train Acc: 97.68% || Val Loss: 0.0682 (Cls: 0.0243, Reg: 0.0878) | Val Acc: 99.67%\n",
      "Epoch [105/300] | Train Loss: 0.3345 (Cls: 0.0821, Reg: 0.5047) | Train Acc: 97.63% || Val Loss: 0.0642 (Cls: 0.0231, Reg: 0.0820) | Val Acc: 99.69%\n",
      "Epoch [106/300] | Train Loss: 0.3300 (Cls: 0.0814, Reg: 0.4972) | Train Acc: 97.79% || Val Loss: 0.0690 (Cls: 0.0222, Reg: 0.0935) | Val Acc: 99.73%\n",
      "Epoch [107/300] | Train Loss: 0.3428 (Cls: 0.0861, Reg: 0.5134) | Train Acc: 97.64% || Val Loss: 0.0635 (Cls: 0.0255, Reg: 0.0759) | Val Acc: 99.71%\n",
      "Epoch [108/300] | Train Loss: 0.3301 (Cls: 0.0767, Reg: 0.5067) | Train Acc: 97.94% || Val Loss: 0.0669 (Cls: 0.0273, Reg: 0.0793) | Val Acc: 99.61%\n",
      "Epoch [109/300] | Train Loss: 0.3236 (Cls: 0.0744, Reg: 0.4986) | Train Acc: 98.04% || Val Loss: 0.0593 (Cls: 0.0270, Reg: 0.0647) | Val Acc: 99.67%\n",
      "Epoch [110/300] | Train Loss: 0.3296 (Cls: 0.0760, Reg: 0.5072) | Train Acc: 97.98% || Val Loss: 0.0597 (Cls: 0.0244, Reg: 0.0707) | Val Acc: 99.69%\n",
      "Epoch [111/300] | Train Loss: 0.3272 (Cls: 0.0770, Reg: 0.5006) | Train Acc: 97.80% || Val Loss: 0.0621 (Cls: 0.0291, Reg: 0.0659) | Val Acc: 99.69%\n",
      "Epoch [112/300] | Train Loss: 0.3249 (Cls: 0.0787, Reg: 0.4923) | Train Acc: 97.80% || Val Loss: 0.0638 (Cls: 0.0257, Reg: 0.0762) | Val Acc: 99.69%\n",
      "Epoch [113/300] | Train Loss: 0.3342 (Cls: 0.0828, Reg: 0.5028) | Train Acc: 97.74% || Val Loss: 0.0640 (Cls: 0.0268, Reg: 0.0745) | Val Acc: 99.67%\n",
      "Epoch [114/300] | Train Loss: 0.3172 (Cls: 0.0738, Reg: 0.4868) | Train Acc: 97.89% || Val Loss: 0.0632 (Cls: 0.0258, Reg: 0.0748) | Val Acc: 99.69%\n",
      "Epoch [115/300] | Train Loss: 0.3091 (Cls: 0.0724, Reg: 0.4735) | Train Acc: 97.98% || Val Loss: 0.0607 (Cls: 0.0252, Reg: 0.0710) | Val Acc: 99.71%\n",
      "Epoch [116/300] | Train Loss: 0.3090 (Cls: 0.0698, Reg: 0.4784) | Train Acc: 98.08% || Val Loss: 0.0571 (Cls: 0.0238, Reg: 0.0667) | Val Acc: 99.71%\n",
      "Epoch [117/300] | Train Loss: 0.3104 (Cls: 0.0711, Reg: 0.4787) | Train Acc: 98.13% || Val Loss: 0.0596 (Cls: 0.0247, Reg: 0.0698) | Val Acc: 99.73%\n",
      "Early stop at epoch 117\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "alpha = 0.5  # 回歸損失權重，可根據需要調整\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "# -----------------------\n",
    "# COORDINATES 字典：將分類標籤轉為 (X, Y) 座標\n",
    "# -----------------------\n",
    "COORDINATES = {\n",
    "    # 下邊界 (1-10 和 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    # 左邊界 (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    # 上邊界 (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    # 右邊界 (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    # 中間點 (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        # 將 0-index 轉換成 1-index (例如 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 訓練參數與 Early Stopping 設定\n",
    "# -----------------------\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\"\n",
    "epochs = 300\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# -----------------------\n",
    "# 訓練迴圈 (分類與回歸雙輸出)\n",
    "# -----------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_class_loss = 0.0\n",
    "    train_reg_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 每個 batch 返回 (amp_inputs, rssi_inputs, labels)\n",
    "    # 其中 labels 為 one-hot 編碼 (用以計算分類損失)\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # 分類目標：one-hot -> class index\n",
    "        target_class = torch.argmax(labels, dim=1)\n",
    "        loss_class = criterion(class_out, target_class)\n",
    "        \n",
    "        # 回歸目標：根據 target_class 透過 COORDINATES 字典取得 (X, Y) 座標\n",
    "        true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "        loss_reg = criterion_reg(reg_out, true_coords)\n",
    "        \n",
    "        loss = loss_class + alpha * loss_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size_curr = amp_inputs.size(0)\n",
    "        train_loss += loss.item() * batch_size_curr\n",
    "        train_class_loss += loss_class.item() * batch_size_curr\n",
    "        train_reg_loss += loss_reg.item() * batch_size_curr\n",
    "        _, predicted = torch.max(class_out, 1)\n",
    "        total_train += batch_size_curr\n",
    "        train_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_class_loss = train_class_loss / len(train_loader.dataset)\n",
    "    avg_train_reg_loss = train_reg_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_class_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            loss_class = criterion(class_out, target_class)\n",
    "            true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "            loss_reg = criterion_reg(reg_out, true_coords)\n",
    "            loss = loss_class + alpha * loss_reg\n",
    "            \n",
    "            batch_size_curr = amp_inputs.size(0)\n",
    "            val_loss += loss.item() * batch_size_curr\n",
    "            val_class_loss += loss_class.item() * batch_size_curr\n",
    "            val_reg_loss += loss_reg.item() * batch_size_curr\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total_val += batch_size_curr\n",
    "            val_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_class_loss = val_class_loss / len(val_loader.dataset)\n",
    "    avg_val_reg_loss = val_reg_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_class_loss:.4f}, Reg: {avg_train_reg_loss:.4f}) | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% || \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} (Cls: {avg_val_class_loss:.4f}, Reg: {avg_val_reg_loss:.4f}) | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x500 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# ---- loss ----\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "\n",
    "# ---- accuracy ----\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 測試損失: 0.2423, 測試準確率: 99.67%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "\n",
    "# 載入最佳模型\n",
    "model.load_state_dict(torch.load(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN/models_save/rssi_csi_0521_reg_class_05_nostd_rssi32_aug005.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 測試模型\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_reg = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for amp_inputs, rssi_input, labels in test_loader:\n",
    "        amp_inputs, rssi_inputs, labels = amp_inputs.to(device), rssi_input.to(device), labels.to(device)\n",
    "        outputs, reg = model(amp_inputs, rssi_inputs)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "        test_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "        # 儲存真實標籤與預測標籤\n",
    "        all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_reg.extend(reg.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"📊 測試損失: {test_loss:.4f}, 測試準確率: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance error: 0.010488759433515297\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adjusted_predictions = np.array(all_predictions) + 1\n",
    "adjusted_labels = np.array(all_labels) + 1\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # 取出對應的座標\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # 計算歐氏距離\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) , errors\n",
    "\n",
    "mean_error, errors = compute_mean_distance_error(adjusted_labels, adjusted_predictions, COORDINATES)\n",
    "print(\"Mean distance error:\", mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.059411708155671\n",
      "1.2000000000000002\n",
      "6.029925372672534\n",
      "0.8485281374238568\n",
      "3.49857113690718\n",
      "1.8000000000000003\n",
      "6.029925372672534\n",
      "3.2310988842807027\n"
     ]
    }
   ],
   "source": [
    "for i in errors:\n",
    "    if i > 0:\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STD前"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Train Loss: 9.7290 (Cls: 4.5358, Reg: 7.4188) | Train Acc: 2.68% || Val Loss: 7.6072 (Cls: 3.7222, Reg: 5.5500) | Val Acc: 11.49%\n",
      "✅ 儲存最佳模型 (Val Loss: 7.6072) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [2/200] | Train Loss: 7.2355 (Cls: 3.5776, Reg: 5.2255) | Train Acc: 6.80% || Val Loss: 6.0366 (Cls: 3.0228, Reg: 4.3054) | Val Acc: 14.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 6.0366) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [3/200] | Train Loss: 6.0965 (Cls: 3.0465, Reg: 4.3572) | Train Acc: 13.28% || Val Loss: 4.7671 (Cls: 2.4328, Reg: 3.3347) | Val Acc: 31.78%\n",
      "✅ 儲存最佳模型 (Val Loss: 4.7671) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [4/200] | Train Loss: 5.1379 (Cls: 2.6025, Reg: 3.6220) | Train Acc: 22.03% || Val Loss: 3.6670 (Cls: 1.8070, Reg: 2.6572) | Val Acc: 56.33%\n",
      "✅ 儲存最佳模型 (Val Loss: 3.6670) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [5/200] | Train Loss: 4.3309 (Cls: 2.1839, Reg: 3.0670) | Train Acc: 32.59% || Val Loss: 2.6883 (Cls: 1.3112, Reg: 1.9672) | Val Acc: 66.86%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.6883) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [6/200] | Train Loss: 3.7998 (Cls: 1.8915, Reg: 2.7262) | Train Acc: 40.19% || Val Loss: 2.4085 (Cls: 1.0310, Reg: 1.9678) | Val Acc: 78.94%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.4085) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [7/200] | Train Loss: 3.4245 (Cls: 1.6778, Reg: 2.4953) | Train Acc: 45.85% || Val Loss: 1.8754 (Cls: 0.8826, Reg: 1.4182) | Val Acc: 79.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.8754) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [8/200] | Train Loss: 3.1130 (Cls: 1.5126, Reg: 2.2863) | Train Acc: 50.57% || Val Loss: 1.6035 (Cls: 0.7123, Reg: 1.2732) | Val Acc: 87.37%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.6035) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [9/200] | Train Loss: 2.8840 (Cls: 1.3945, Reg: 2.1278) | Train Acc: 55.23% || Val Loss: 1.3774 (Cls: 0.5906, Reg: 1.1239) | Val Acc: 88.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.3774) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [10/200] | Train Loss: 2.7291 (Cls: 1.3082, Reg: 2.0299) | Train Acc: 57.01% || Val Loss: 1.2485 (Cls: 0.5329, Reg: 1.0223) | Val Acc: 90.18%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.2485) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [11/200] | Train Loss: 2.5511 (Cls: 1.2308, Reg: 1.8861) | Train Acc: 60.23% || Val Loss: 1.2618 (Cls: 0.5122, Reg: 1.0709) | Val Acc: 92.43%\n",
      "Epoch [12/200] | Train Loss: 2.4131 (Cls: 1.1481, Reg: 1.8072) | Train Acc: 62.55% || Val Loss: 0.9987 (Cls: 0.4250, Reg: 0.8196) | Val Acc: 92.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9987) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [13/200] | Train Loss: 2.3605 (Cls: 1.1272, Reg: 1.7619) | Train Acc: 64.42% || Val Loss: 1.0983 (Cls: 0.4268, Reg: 0.9593) | Val Acc: 90.98%\n",
      "Epoch [14/200] | Train Loss: 2.2210 (Cls: 1.0621, Reg: 1.6555) | Train Acc: 65.86% || Val Loss: 1.0821 (Cls: 0.3886, Reg: 0.9908) | Val Acc: 92.71%\n",
      "Epoch [15/200] | Train Loss: 2.1633 (Cls: 1.0269, Reg: 1.6235) | Train Acc: 67.15% || Val Loss: 0.8832 (Cls: 0.3349, Reg: 0.7832) | Val Acc: 94.41%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.8832) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [16/200] | Train Loss: 2.1191 (Cls: 1.0062, Reg: 1.5898) | Train Acc: 67.87% || Val Loss: 0.7712 (Cls: 0.3191, Reg: 0.6459) | Val Acc: 94.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7712) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [17/200] | Train Loss: 2.0220 (Cls: 0.9577, Reg: 1.5204) | Train Acc: 69.44% || Val Loss: 0.7407 (Cls: 0.2866, Reg: 0.6488) | Val Acc: 93.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7407) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [18/200] | Train Loss: 1.9940 (Cls: 0.9537, Reg: 1.4862) | Train Acc: 70.14% || Val Loss: 0.8192 (Cls: 0.3031, Reg: 0.7372) | Val Acc: 93.10%\n",
      "Epoch [19/200] | Train Loss: 1.9182 (Cls: 0.9162, Reg: 1.4314) | Train Acc: 70.83% || Val Loss: 0.6522 (Cls: 0.2405, Reg: 0.5882) | Val Acc: 97.33%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6522) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [20/200] | Train Loss: 1.8871 (Cls: 0.8953, Reg: 1.4170) | Train Acc: 71.61% || Val Loss: 0.6118 (Cls: 0.2354, Reg: 0.5376) | Val Acc: 97.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6118) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [21/200] | Train Loss: 1.8728 (Cls: 0.8905, Reg: 1.4033) | Train Acc: 71.38% || Val Loss: 0.6519 (Cls: 0.2321, Reg: 0.5997) | Val Acc: 95.59%\n",
      "Epoch [22/200] | Train Loss: 1.8467 (Cls: 0.8731, Reg: 1.3908) | Train Acc: 72.44% || Val Loss: 0.7081 (Cls: 0.2575, Reg: 0.6438) | Val Acc: 95.27%\n",
      "Epoch [23/200] | Train Loss: 1.8063 (Cls: 0.8482, Reg: 1.3688) | Train Acc: 73.25% || Val Loss: 0.7394 (Cls: 0.2895, Reg: 0.6427) | Val Acc: 91.65%\n",
      "Epoch [24/200] | Train Loss: 1.7929 (Cls: 0.8456, Reg: 1.3532) | Train Acc: 73.85% || Val Loss: 0.5469 (Cls: 0.2108, Reg: 0.4802) | Val Acc: 97.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5469) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [25/200] | Train Loss: 1.7852 (Cls: 0.8431, Reg: 1.3459) | Train Acc: 73.60% || Val Loss: 0.5954 (Cls: 0.2275, Reg: 0.5256) | Val Acc: 95.37%\n",
      "Epoch [26/200] | Train Loss: 1.7400 (Cls: 0.8121, Reg: 1.3256) | Train Acc: 74.56% || Val Loss: 0.5429 (Cls: 0.2059, Reg: 0.4815) | Val Acc: 96.94%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5429) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [27/200] | Train Loss: 1.7311 (Cls: 0.8085, Reg: 1.3180) | Train Acc: 74.78% || Val Loss: 0.6124 (Cls: 0.2121, Reg: 0.5719) | Val Acc: 93.90%\n",
      "Epoch [28/200] | Train Loss: 1.7118 (Cls: 0.8009, Reg: 1.3013) | Train Acc: 75.04% || Val Loss: 0.5149 (Cls: 0.1768, Reg: 0.4829) | Val Acc: 96.88%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5149) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [29/200] | Train Loss: 1.6841 (Cls: 0.7870, Reg: 1.2815) | Train Acc: 75.94% || Val Loss: 0.4870 (Cls: 0.1768, Reg: 0.4432) | Val Acc: 97.08%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4870) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [30/200] | Train Loss: 1.7215 (Cls: 0.8056, Reg: 1.3086) | Train Acc: 74.93% || Val Loss: 0.5205 (Cls: 0.1783, Reg: 0.4889) | Val Acc: 96.84%\n",
      "Epoch [31/200] | Train Loss: 1.6540 (Cls: 0.7662, Reg: 1.2683) | Train Acc: 76.48% || Val Loss: 0.4888 (Cls: 0.1728, Reg: 0.4514) | Val Acc: 97.22%\n",
      "Epoch [32/200] | Train Loss: 1.6616 (Cls: 0.7751, Reg: 1.2665) | Train Acc: 76.09% || Val Loss: 0.5488 (Cls: 0.1765, Reg: 0.5318) | Val Acc: 96.39%\n",
      "Epoch [33/200] | Train Loss: 1.6432 (Cls: 0.7625, Reg: 1.2581) | Train Acc: 76.16% || Val Loss: 0.4817 (Cls: 0.1657, Reg: 0.4515) | Val Acc: 97.88%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4817) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [34/200] | Train Loss: 1.6454 (Cls: 0.7584, Reg: 1.2670) | Train Acc: 76.84% || Val Loss: 0.5050 (Cls: 0.1724, Reg: 0.4751) | Val Acc: 97.33%\n",
      "Epoch [35/200] | Train Loss: 1.6114 (Cls: 0.7518, Reg: 1.2280) | Train Acc: 77.17% || Val Loss: 0.4512 (Cls: 0.1536, Reg: 0.4251) | Val Acc: 97.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4512) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [36/200] | Train Loss: 1.6279 (Cls: 0.7526, Reg: 1.2504) | Train Acc: 76.93% || Val Loss: 0.4532 (Cls: 0.1607, Reg: 0.4178) | Val Acc: 97.22%\n",
      "Epoch [37/200] | Train Loss: 1.5808 (Cls: 0.7246, Reg: 1.2230) | Train Acc: 77.66% || Val Loss: 0.4575 (Cls: 0.1496, Reg: 0.4398) | Val Acc: 97.73%\n",
      "Epoch [38/200] | Train Loss: 1.5842 (Cls: 0.7267, Reg: 1.2251) | Train Acc: 77.27% || Val Loss: 0.4351 (Cls: 0.1520, Reg: 0.4044) | Val Acc: 96.29%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4351) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [39/200] | Train Loss: 1.5938 (Cls: 0.7279, Reg: 1.2369) | Train Acc: 77.21% || Val Loss: 0.4515 (Cls: 0.1538, Reg: 0.4253) | Val Acc: 96.20%\n",
      "Epoch [40/200] | Train Loss: 1.5932 (Cls: 0.7394, Reg: 1.2197) | Train Acc: 77.50% || Val Loss: 0.4181 (Cls: 0.1420, Reg: 0.3944) | Val Acc: 97.98%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4181) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [41/200] | Train Loss: 1.5385 (Cls: 0.7060, Reg: 1.1893) | Train Acc: 78.12% || Val Loss: 0.3860 (Cls: 0.1346, Reg: 0.3591) | Val Acc: 98.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3860) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [42/200] | Train Loss: 1.5963 (Cls: 0.7333, Reg: 1.2328) | Train Acc: 77.70% || Val Loss: 0.4017 (Cls: 0.1366, Reg: 0.3787) | Val Acc: 98.16%\n",
      "Epoch [43/200] | Train Loss: 1.5445 (Cls: 0.7020, Reg: 1.2036) | Train Acc: 78.73% || Val Loss: 0.4168 (Cls: 0.1356, Reg: 0.4017) | Val Acc: 98.04%\n",
      "Epoch [44/200] | Train Loss: 1.5558 (Cls: 0.7104, Reg: 1.2077) | Train Acc: 78.53% || Val Loss: 0.4626 (Cls: 0.1502, Reg: 0.4463) | Val Acc: 97.59%\n",
      "Epoch [45/200] | Train Loss: 1.5220 (Cls: 0.6928, Reg: 1.1846) | Train Acc: 78.75% || Val Loss: 0.4175 (Cls: 0.1393, Reg: 0.3974) | Val Acc: 97.78%\n",
      "Epoch [46/200] | Train Loss: 1.5041 (Cls: 0.6776, Reg: 1.1807) | Train Acc: 79.06% || Val Loss: 0.3942 (Cls: 0.1105, Reg: 0.4053) | Val Acc: 98.37%\n",
      "Epoch [47/200] | Train Loss: 1.5456 (Cls: 0.7088, Reg: 1.1955) | Train Acc: 78.33% || Val Loss: 0.4644 (Cls: 0.1437, Reg: 0.4581) | Val Acc: 97.37%\n",
      "Epoch [48/200] | Train Loss: 1.5219 (Cls: 0.6871, Reg: 1.1926) | Train Acc: 79.26% || Val Loss: 0.3880 (Cls: 0.1237, Reg: 0.3776) | Val Acc: 98.12%\n",
      "Epoch [49/200] | Train Loss: 1.5277 (Cls: 0.6929, Reg: 1.1925) | Train Acc: 79.34% || Val Loss: 0.4096 (Cls: 0.1284, Reg: 0.4017) | Val Acc: 97.88%\n",
      "Epoch [50/200] | Train Loss: 1.5220 (Cls: 0.6956, Reg: 1.1805) | Train Acc: 79.34% || Val Loss: 0.3689 (Cls: 0.1288, Reg: 0.3430) | Val Acc: 98.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3689) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [51/200] | Train Loss: 1.5395 (Cls: 0.7025, Reg: 1.1958) | Train Acc: 78.57% || Val Loss: 0.4256 (Cls: 0.1225, Reg: 0.4329) | Val Acc: 98.06%\n",
      "Epoch [52/200] | Train Loss: 1.5079 (Cls: 0.6785, Reg: 1.1848) | Train Acc: 79.54% || Val Loss: 0.3602 (Cls: 0.1233, Reg: 0.3383) | Val Acc: 97.92%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3602) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [53/200] | Train Loss: 1.4526 (Cls: 0.6495, Reg: 1.1472) | Train Acc: 80.01% || Val Loss: 0.4113 (Cls: 0.1322, Reg: 0.3987) | Val Acc: 97.73%\n",
      "Epoch [54/200] | Train Loss: 1.5012 (Cls: 0.6799, Reg: 1.1734) | Train Acc: 79.47% || Val Loss: 0.5000 (Cls: 0.1664, Reg: 0.4766) | Val Acc: 96.86%\n",
      "Epoch [55/200] | Train Loss: 1.4883 (Cls: 0.6684, Reg: 1.1713) | Train Acc: 79.44% || Val Loss: 0.3883 (Cls: 0.1208, Reg: 0.3822) | Val Acc: 98.33%\n",
      "Epoch [56/200] | Train Loss: 1.4658 (Cls: 0.6622, Reg: 1.1479) | Train Acc: 79.81% || Val Loss: 0.3330 (Cls: 0.1137, Reg: 0.3133) | Val Acc: 98.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3330) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [57/200] | Train Loss: 1.5101 (Cls: 0.6948, Reg: 1.1647) | Train Acc: 79.17% || Val Loss: 0.3557 (Cls: 0.1192, Reg: 0.3378) | Val Acc: 98.47%\n",
      "Epoch [58/200] | Train Loss: 1.4700 (Cls: 0.6591, Reg: 1.1585) | Train Acc: 80.29% || Val Loss: 0.3768 (Cls: 0.1236, Reg: 0.3617) | Val Acc: 98.10%\n",
      "Epoch [59/200] | Train Loss: 1.4772 (Cls: 0.6689, Reg: 1.1547) | Train Acc: 79.70% || Val Loss: 0.4006 (Cls: 0.1316, Reg: 0.3843) | Val Acc: 97.71%\n",
      "Epoch [60/200] | Train Loss: 1.4698 (Cls: 0.6639, Reg: 1.1513) | Train Acc: 79.77% || Val Loss: 0.3380 (Cls: 0.1056, Reg: 0.3320) | Val Acc: 98.24%\n",
      "Epoch [61/200] | Train Loss: 1.4552 (Cls: 0.6565, Reg: 1.1410) | Train Acc: 80.03% || Val Loss: 0.4492 (Cls: 0.1510, Reg: 0.4260) | Val Acc: 97.06%\n",
      "Epoch [62/200] | Train Loss: 1.4225 (Cls: 0.6328, Reg: 1.1281) | Train Acc: 80.86% || Val Loss: 0.3453 (Cls: 0.1093, Reg: 0.3371) | Val Acc: 98.45%\n",
      "Epoch [63/200] | Train Loss: 1.4557 (Cls: 0.6563, Reg: 1.1420) | Train Acc: 80.51% || Val Loss: 0.3573 (Cls: 0.1060, Reg: 0.3590) | Val Acc: 98.41%\n",
      "Epoch [64/200] | Train Loss: 1.4603 (Cls: 0.6595, Reg: 1.1440) | Train Acc: 80.30% || Val Loss: 0.3735 (Cls: 0.1227, Reg: 0.3582) | Val Acc: 97.80%\n",
      "Epoch [65/200] | Train Loss: 1.4598 (Cls: 0.6559, Reg: 1.1484) | Train Acc: 80.16% || Val Loss: 0.4098 (Cls: 0.1199, Reg: 0.4141) | Val Acc: 98.04%\n",
      "Epoch [66/200] | Train Loss: 1.4439 (Cls: 0.6517, Reg: 1.1317) | Train Acc: 80.85% || Val Loss: 0.3428 (Cls: 0.1158, Reg: 0.3243) | Val Acc: 98.33%\n",
      "Epoch [67/200] | Train Loss: 1.4339 (Cls: 0.6442, Reg: 1.1282) | Train Acc: 80.65% || Val Loss: 0.3925 (Cls: 0.1142, Reg: 0.3975) | Val Acc: 98.35%\n",
      "Epoch [68/200] | Train Loss: 1.4036 (Cls: 0.6162, Reg: 1.1249) | Train Acc: 81.64% || Val Loss: 0.3276 (Cls: 0.0997, Reg: 0.3256) | Val Acc: 98.49%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3276) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [69/200] | Train Loss: 1.4105 (Cls: 0.6262, Reg: 1.1205) | Train Acc: 81.06% || Val Loss: 0.5861 (Cls: 0.2062, Reg: 0.5427) | Val Acc: 94.67%\n",
      "Epoch [70/200] | Train Loss: 1.4375 (Cls: 0.6522, Reg: 1.1218) | Train Acc: 80.54% || Val Loss: 0.4083 (Cls: 0.1103, Reg: 0.4257) | Val Acc: 98.41%\n",
      "Epoch [71/200] | Train Loss: 1.4113 (Cls: 0.6250, Reg: 1.1233) | Train Acc: 81.13% || Val Loss: 0.3164 (Cls: 0.0886, Reg: 0.3254) | Val Acc: 98.55%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3164) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [72/200] | Train Loss: 1.4297 (Cls: 0.6408, Reg: 1.1270) | Train Acc: 80.65% || Val Loss: 0.4538 (Cls: 0.1358, Reg: 0.4542) | Val Acc: 97.90%\n",
      "Epoch [73/200] | Train Loss: 1.3913 (Cls: 0.6154, Reg: 1.1085) | Train Acc: 81.19% || Val Loss: 0.3005 (Cls: 0.0894, Reg: 0.3016) | Val Acc: 98.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3005) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [74/200] | Train Loss: 1.3975 (Cls: 0.6160, Reg: 1.1165) | Train Acc: 81.66% || Val Loss: 0.3716 (Cls: 0.0977, Reg: 0.3914) | Val Acc: 98.63%\n",
      "Epoch [75/200] | Train Loss: 1.3993 (Cls: 0.6163, Reg: 1.1185) | Train Acc: 81.45% || Val Loss: 0.3871 (Cls: 0.1134, Reg: 0.3910) | Val Acc: 97.71%\n",
      "Epoch [76/200] | Train Loss: 1.3885 (Cls: 0.6150, Reg: 1.1050) | Train Acc: 81.62% || Val Loss: 0.3322 (Cls: 0.1047, Reg: 0.3250) | Val Acc: 98.41%\n",
      "Epoch [77/200] | Train Loss: 1.3660 (Cls: 0.6058, Reg: 1.0859) | Train Acc: 82.02% || Val Loss: 0.2994 (Cls: 0.0931, Reg: 0.2948) | Val Acc: 98.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2994) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [78/200] | Train Loss: 1.4219 (Cls: 0.6302, Reg: 1.1309) | Train Acc: 81.23% || Val Loss: 0.3556 (Cls: 0.1012, Reg: 0.3633) | Val Acc: 98.35%\n",
      "Epoch [79/200] | Train Loss: 1.3964 (Cls: 0.6167, Reg: 1.1139) | Train Acc: 81.28% || Val Loss: 0.3486 (Cls: 0.0927, Reg: 0.3655) | Val Acc: 98.57%\n",
      "Epoch [80/200] | Train Loss: 1.3627 (Cls: 0.5971, Reg: 1.0938) | Train Acc: 81.91% || Val Loss: 0.2996 (Cls: 0.0957, Reg: 0.2912) | Val Acc: 98.20%\n",
      "Epoch [81/200] | Train Loss: 1.3943 (Cls: 0.6181, Reg: 1.1089) | Train Acc: 81.99% || Val Loss: 0.3465 (Cls: 0.1228, Reg: 0.3195) | Val Acc: 97.27%\n",
      "Epoch [82/200] | Train Loss: 1.3901 (Cls: 0.6087, Reg: 1.1163) | Train Acc: 81.88% || Val Loss: 0.3276 (Cls: 0.0881, Reg: 0.3421) | Val Acc: 98.84%\n",
      "Epoch [83/200] | Train Loss: 1.3596 (Cls: 0.5961, Reg: 1.0907) | Train Acc: 81.87% || Val Loss: 0.3098 (Cls: 0.0927, Reg: 0.3101) | Val Acc: 98.69%\n",
      "Epoch [84/200] | Train Loss: 1.3815 (Cls: 0.6121, Reg: 1.0992) | Train Acc: 81.81% || Val Loss: 0.3652 (Cls: 0.1176, Reg: 0.3537) | Val Acc: 97.55%\n",
      "Epoch [85/200] | Train Loss: 1.4035 (Cls: 0.6191, Reg: 1.1206) | Train Acc: 81.43% || Val Loss: 0.3172 (Cls: 0.0863, Reg: 0.3298) | Val Acc: 98.47%\n",
      "Epoch [86/200] | Train Loss: 1.3936 (Cls: 0.6143, Reg: 1.1132) | Train Acc: 81.84% || Val Loss: 0.3026 (Cls: 0.0846, Reg: 0.3113) | Val Acc: 98.59%\n",
      "Epoch [87/200] | Train Loss: 1.3583 (Cls: 0.5953, Reg: 1.0900) | Train Acc: 82.22% || Val Loss: 0.2745 (Cls: 0.0847, Reg: 0.2711) | Val Acc: 98.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2745) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [88/200] | Train Loss: 1.3595 (Cls: 0.5958, Reg: 1.0909) | Train Acc: 82.45% || Val Loss: 0.3616 (Cls: 0.0978, Reg: 0.3769) | Val Acc: 98.27%\n",
      "Epoch [89/200] | Train Loss: 1.3553 (Cls: 0.5954, Reg: 1.0857) | Train Acc: 82.61% || Val Loss: 0.3286 (Cls: 0.0890, Reg: 0.3423) | Val Acc: 98.67%\n",
      "Epoch [90/200] | Train Loss: 1.3833 (Cls: 0.6111, Reg: 1.1031) | Train Acc: 81.99% || Val Loss: 0.4146 (Cls: 0.1279, Reg: 0.4096) | Val Acc: 96.90%\n",
      "Epoch [91/200] | Train Loss: 1.3741 (Cls: 0.6056, Reg: 1.0979) | Train Acc: 81.85% || Val Loss: 0.3412 (Cls: 0.0959, Reg: 0.3504) | Val Acc: 98.33%\n",
      "Epoch [92/200] | Train Loss: 1.3890 (Cls: 0.6088, Reg: 1.1146) | Train Acc: 81.90% || Val Loss: 0.3539 (Cls: 0.0995, Reg: 0.3633) | Val Acc: 98.59%\n",
      "Epoch [93/200] | Train Loss: 1.3773 (Cls: 0.6118, Reg: 1.0935) | Train Acc: 82.24% || Val Loss: 0.3352 (Cls: 0.0869, Reg: 0.3548) | Val Acc: 98.63%\n",
      "Epoch [94/200] | Train Loss: 1.3886 (Cls: 0.6146, Reg: 1.1057) | Train Acc: 82.09% || Val Loss: 0.2894 (Cls: 0.0863, Reg: 0.2900) | Val Acc: 98.84%\n",
      "Epoch [95/200] | Train Loss: 1.3411 (Cls: 0.5808, Reg: 1.0862) | Train Acc: 82.72% || Val Loss: 0.3111 (Cls: 0.0816, Reg: 0.3278) | Val Acc: 98.82%\n",
      "Epoch [96/200] | Train Loss: 1.3473 (Cls: 0.5924, Reg: 1.0784) | Train Acc: 82.62% || Val Loss: 0.3225 (Cls: 0.0858, Reg: 0.3381) | Val Acc: 98.90%\n",
      "Epoch [97/200] | Train Loss: 1.3682 (Cls: 0.6020, Reg: 1.0945) | Train Acc: 82.41% || Val Loss: 0.2868 (Cls: 0.0850, Reg: 0.2883) | Val Acc: 98.73%\n",
      "Epoch [98/200] | Train Loss: 1.3219 (Cls: 0.5711, Reg: 1.0726) | Train Acc: 82.79% || Val Loss: 0.3177 (Cls: 0.0905, Reg: 0.3245) | Val Acc: 98.63%\n",
      "Epoch [99/200] | Train Loss: 1.3706 (Cls: 0.6016, Reg: 1.0985) | Train Acc: 81.97% || Val Loss: 0.2965 (Cls: 0.0871, Reg: 0.2992) | Val Acc: 98.76%\n",
      "Epoch [100/200] | Train Loss: 1.3589 (Cls: 0.5934, Reg: 1.0936) | Train Acc: 82.57% || Val Loss: 0.3128 (Cls: 0.0905, Reg: 0.3176) | Val Acc: 98.63%\n",
      "Epoch [101/200] | Train Loss: 1.3208 (Cls: 0.5774, Reg: 1.0620) | Train Acc: 82.96% || Val Loss: 0.3794 (Cls: 0.1206, Reg: 0.3697) | Val Acc: 97.73%\n",
      "Epoch [102/200] | Train Loss: 1.3456 (Cls: 0.5911, Reg: 1.0778) | Train Acc: 82.67% || Val Loss: 0.3191 (Cls: 0.0835, Reg: 0.3365) | Val Acc: 98.61%\n",
      "Epoch [103/200] | Train Loss: 1.3403 (Cls: 0.5860, Reg: 1.0776) | Train Acc: 82.70% || Val Loss: 0.3460 (Cls: 0.0893, Reg: 0.3667) | Val Acc: 98.57%\n",
      "Epoch [104/200] | Train Loss: 1.2666 (Cls: 0.5489, Reg: 1.0252) | Train Acc: 83.81% || Val Loss: 0.4157 (Cls: 0.1213, Reg: 0.4205) | Val Acc: 97.98%\n",
      "Epoch [105/200] | Train Loss: 1.2354 (Cls: 0.5258, Reg: 1.0137) | Train Acc: 84.51% || Val Loss: 0.2855 (Cls: 0.0752, Reg: 0.3004) | Val Acc: 98.92%\n",
      "Epoch [106/200] | Train Loss: 1.2267 (Cls: 0.5164, Reg: 1.0146) | Train Acc: 84.50% || Val Loss: 0.2727 (Cls: 0.0766, Reg: 0.2802) | Val Acc: 98.80%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2727) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [107/200] | Train Loss: 1.2190 (Cls: 0.5109, Reg: 1.0116) | Train Acc: 85.19% || Val Loss: 0.3051 (Cls: 0.0815, Reg: 0.3194) | Val Acc: 98.65%\n",
      "Epoch [108/200] | Train Loss: 1.2163 (Cls: 0.5208, Reg: 0.9936) | Train Acc: 84.49% || Val Loss: 0.2874 (Cls: 0.0761, Reg: 0.3018) | Val Acc: 98.88%\n",
      "Epoch [109/200] | Train Loss: 1.1924 (Cls: 0.4936, Reg: 0.9983) | Train Acc: 85.45% || Val Loss: 0.2707 (Cls: 0.0700, Reg: 0.2867) | Val Acc: 99.10%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2707) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [110/200] | Train Loss: 1.1937 (Cls: 0.4981, Reg: 0.9937) | Train Acc: 85.27% || Val Loss: 0.2815 (Cls: 0.0763, Reg: 0.2931) | Val Acc: 98.90%\n",
      "Epoch [111/200] | Train Loss: 1.2003 (Cls: 0.4973, Reg: 1.0042) | Train Acc: 85.08% || Val Loss: 0.2809 (Cls: 0.0786, Reg: 0.2891) | Val Acc: 98.76%\n",
      "Epoch [112/200] | Train Loss: 1.1899 (Cls: 0.4999, Reg: 0.9857) | Train Acc: 85.60% || Val Loss: 0.3537 (Cls: 0.0886, Reg: 0.3788) | Val Acc: 98.59%\n",
      "Epoch [113/200] | Train Loss: 1.1954 (Cls: 0.4955, Reg: 0.9998) | Train Acc: 85.40% || Val Loss: 0.3186 (Cls: 0.0809, Reg: 0.3396) | Val Acc: 98.88%\n",
      "Epoch [114/200] | Train Loss: 1.2171 (Cls: 0.5131, Reg: 1.0058) | Train Acc: 85.11% || Val Loss: 0.2744 (Cls: 0.0751, Reg: 0.2848) | Val Acc: 99.02%\n",
      "Epoch [115/200] | Train Loss: 1.2010 (Cls: 0.4987, Reg: 1.0032) | Train Acc: 85.28% || Val Loss: 0.2747 (Cls: 0.0744, Reg: 0.2862) | Val Acc: 98.94%\n",
      "Epoch [116/200] | Train Loss: 1.1982 (Cls: 0.4994, Reg: 0.9984) | Train Acc: 85.28% || Val Loss: 0.2756 (Cls: 0.0747, Reg: 0.2870) | Val Acc: 98.88%\n",
      "Epoch [117/200] | Train Loss: 1.2005 (Cls: 0.5033, Reg: 0.9960) | Train Acc: 85.07% || Val Loss: 0.2694 (Cls: 0.0742, Reg: 0.2788) | Val Acc: 99.06%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2694) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [118/200] | Train Loss: 1.1941 (Cls: 0.4980, Reg: 0.9944) | Train Acc: 85.42% || Val Loss: 0.2537 (Cls: 0.0678, Reg: 0.2656) | Val Acc: 99.06%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2537) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [119/200] | Train Loss: 1.2003 (Cls: 0.5040, Reg: 0.9947) | Train Acc: 85.51% || Val Loss: 0.2657 (Cls: 0.0681, Reg: 0.2823) | Val Acc: 99.16%\n",
      "Epoch [120/200] | Train Loss: 1.1715 (Cls: 0.4865, Reg: 0.9785) | Train Acc: 85.73% || Val Loss: 0.2446 (Cls: 0.0738, Reg: 0.2439) | Val Acc: 99.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2446) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [121/200] | Train Loss: 1.1748 (Cls: 0.4875, Reg: 0.9818) | Train Acc: 86.06% || Val Loss: 0.2932 (Cls: 0.0776, Reg: 0.3081) | Val Acc: 98.92%\n",
      "Epoch [122/200] | Train Loss: 1.1858 (Cls: 0.4922, Reg: 0.9908) | Train Acc: 85.32% || Val Loss: 0.2539 (Cls: 0.0712, Reg: 0.2611) | Val Acc: 99.04%\n",
      "Epoch [123/200] | Train Loss: 1.1822 (Cls: 0.4923, Reg: 0.9856) | Train Acc: 85.71% || Val Loss: 0.2736 (Cls: 0.0795, Reg: 0.2773) | Val Acc: 98.96%\n",
      "Epoch [124/200] | Train Loss: 1.2038 (Cls: 0.5031, Reg: 1.0011) | Train Acc: 85.36% || Val Loss: 0.3010 (Cls: 0.0811, Reg: 0.3142) | Val Acc: 99.00%\n",
      "Epoch [125/200] | Train Loss: 1.1799 (Cls: 0.4886, Reg: 0.9876) | Train Acc: 85.62% || Val Loss: 0.3175 (Cls: 0.0913, Reg: 0.3231) | Val Acc: 98.45%\n",
      "Epoch [126/200] | Train Loss: 1.1708 (Cls: 0.4749, Reg: 0.9942) | Train Acc: 86.21% || Val Loss: 0.3059 (Cls: 0.0790, Reg: 0.3241) | Val Acc: 98.98%\n",
      "Epoch [127/200] | Train Loss: 1.1629 (Cls: 0.4772, Reg: 0.9795) | Train Acc: 86.07% || Val Loss: 0.2597 (Cls: 0.0675, Reg: 0.2745) | Val Acc: 99.08%\n",
      "Epoch [128/200] | Train Loss: 1.1457 (Cls: 0.4696, Reg: 0.9659) | Train Acc: 86.15% || Val Loss: 0.2559 (Cls: 0.0689, Reg: 0.2671) | Val Acc: 99.08%\n",
      "Epoch [129/200] | Train Loss: 1.1453 (Cls: 0.4687, Reg: 0.9665) | Train Acc: 86.20% || Val Loss: 0.2666 (Cls: 0.0674, Reg: 0.2846) | Val Acc: 98.92%\n",
      "Epoch [130/200] | Train Loss: 1.1630 (Cls: 0.4805, Reg: 0.9750) | Train Acc: 85.84% || Val Loss: 0.2696 (Cls: 0.0713, Reg: 0.2833) | Val Acc: 99.00%\n",
      "Epoch [131/200] | Train Loss: 1.1781 (Cls: 0.4941, Reg: 0.9770) | Train Acc: 85.53% || Val Loss: 0.2581 (Cls: 0.0675, Reg: 0.2723) | Val Acc: 99.18%\n",
      "Epoch [132/200] | Train Loss: 1.1806 (Cls: 0.4836, Reg: 0.9957) | Train Acc: 85.90% || Val Loss: 0.2730 (Cls: 0.0758, Reg: 0.2816) | Val Acc: 98.84%\n",
      "Epoch [133/200] | Train Loss: 1.1467 (Cls: 0.4708, Reg: 0.9657) | Train Acc: 86.05% || Val Loss: 0.3150 (Cls: 0.0789, Reg: 0.3373) | Val Acc: 98.90%\n",
      "Epoch [134/200] | Train Loss: 1.1532 (Cls: 0.4756, Reg: 0.9680) | Train Acc: 85.71% || Val Loss: 0.3470 (Cls: 0.0845, Reg: 0.3749) | Val Acc: 98.76%\n",
      "Epoch [135/200] | Train Loss: 1.1852 (Cls: 0.4935, Reg: 0.9882) | Train Acc: 85.27% || Val Loss: 0.2975 (Cls: 0.0747, Reg: 0.3183) | Val Acc: 99.02%\n",
      "Epoch [136/200] | Train Loss: 1.1722 (Cls: 0.4874, Reg: 0.9783) | Train Acc: 85.87% || Val Loss: 0.2776 (Cls: 0.0763, Reg: 0.2877) | Val Acc: 99.08%\n",
      "Epoch [137/200] | Train Loss: 1.1467 (Cls: 0.4790, Reg: 0.9538) | Train Acc: 86.20% || Val Loss: 0.2554 (Cls: 0.0692, Reg: 0.2659) | Val Acc: 99.18%\n",
      "Epoch [138/200] | Train Loss: 1.1211 (Cls: 0.4541, Reg: 0.9529) | Train Acc: 86.83% || Val Loss: 0.2489 (Cls: 0.0700, Reg: 0.2556) | Val Acc: 99.14%\n",
      "Epoch [139/200] | Train Loss: 1.1199 (Cls: 0.4533, Reg: 0.9523) | Train Acc: 87.04% || Val Loss: 0.2444 (Cls: 0.0693, Reg: 0.2501) | Val Acc: 99.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2444) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [140/200] | Train Loss: 1.0972 (Cls: 0.4456, Reg: 0.9310) | Train Acc: 86.81% || Val Loss: 0.2410 (Cls: 0.0665, Reg: 0.2493) | Val Acc: 99.08%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2410) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [141/200] | Train Loss: 1.1134 (Cls: 0.4533, Reg: 0.9430) | Train Acc: 86.91% || Val Loss: 0.2684 (Cls: 0.0720, Reg: 0.2806) | Val Acc: 99.02%\n",
      "Epoch [142/200] | Train Loss: 1.1168 (Cls: 0.4542, Reg: 0.9466) | Train Acc: 86.48% || Val Loss: 0.2559 (Cls: 0.0720, Reg: 0.2628) | Val Acc: 99.10%\n",
      "Epoch [143/200] | Train Loss: 1.0921 (Cls: 0.4414, Reg: 0.9295) | Train Acc: 87.03% || Val Loss: 0.3045 (Cls: 0.0751, Reg: 0.3278) | Val Acc: 98.92%\n",
      "Epoch [144/200] | Train Loss: 1.1142 (Cls: 0.4537, Reg: 0.9437) | Train Acc: 86.64% || Val Loss: 0.2376 (Cls: 0.0649, Reg: 0.2467) | Val Acc: 99.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2376) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [145/200] | Train Loss: 1.1088 (Cls: 0.4510, Reg: 0.9398) | Train Acc: 86.62% || Val Loss: 0.2432 (Cls: 0.0646, Reg: 0.2551) | Val Acc: 99.12%\n",
      "Epoch [146/200] | Train Loss: 1.1040 (Cls: 0.4547, Reg: 0.9275) | Train Acc: 86.61% || Val Loss: 0.2438 (Cls: 0.0632, Reg: 0.2579) | Val Acc: 99.12%\n",
      "Epoch [147/200] | Train Loss: 1.0783 (Cls: 0.4333, Reg: 0.9214) | Train Acc: 87.35% || Val Loss: 0.2440 (Cls: 0.0621, Reg: 0.2598) | Val Acc: 99.08%\n",
      "Epoch [148/200] | Train Loss: 1.0907 (Cls: 0.4395, Reg: 0.9303) | Train Acc: 87.13% || Val Loss: 0.2359 (Cls: 0.0653, Reg: 0.2437) | Val Acc: 99.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2359) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [149/200] | Train Loss: 1.0898 (Cls: 0.4307, Reg: 0.9416) | Train Acc: 87.23% || Val Loss: 0.2429 (Cls: 0.0632, Reg: 0.2567) | Val Acc: 99.14%\n",
      "Epoch [150/200] | Train Loss: 1.0929 (Cls: 0.4398, Reg: 0.9330) | Train Acc: 86.93% || Val Loss: 0.2728 (Cls: 0.0670, Reg: 0.2939) | Val Acc: 99.06%\n",
      "Epoch [151/200] | Train Loss: 1.0963 (Cls: 0.4410, Reg: 0.9361) | Train Acc: 87.22% || Val Loss: 0.2581 (Cls: 0.0641, Reg: 0.2771) | Val Acc: 99.16%\n",
      "Epoch [152/200] | Train Loss: 1.1220 (Cls: 0.4524, Reg: 0.9567) | Train Acc: 87.17% || Val Loss: 0.2424 (Cls: 0.0639, Reg: 0.2550) | Val Acc: 99.18%\n",
      "Epoch [153/200] | Train Loss: 1.0905 (Cls: 0.4378, Reg: 0.9324) | Train Acc: 86.99% || Val Loss: 0.2312 (Cls: 0.0599, Reg: 0.2447) | Val Acc: 99.08%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2312) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [154/200] | Train Loss: 1.0938 (Cls: 0.4397, Reg: 0.9344) | Train Acc: 87.39% || Val Loss: 0.2221 (Cls: 0.0595, Reg: 0.2322) | Val Acc: 99.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2221) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [155/200] | Train Loss: 1.0959 (Cls: 0.4358, Reg: 0.9430) | Train Acc: 87.43% || Val Loss: 0.2393 (Cls: 0.0572, Reg: 0.2602) | Val Acc: 99.12%\n",
      "Epoch [156/200] | Train Loss: 1.0732 (Cls: 0.4268, Reg: 0.9234) | Train Acc: 87.41% || Val Loss: 0.2858 (Cls: 0.0657, Reg: 0.3145) | Val Acc: 99.08%\n",
      "Epoch [157/200] | Train Loss: 1.1132 (Cls: 0.4522, Reg: 0.9443) | Train Acc: 86.70% || Val Loss: 0.2329 (Cls: 0.0572, Reg: 0.2511) | Val Acc: 99.18%\n",
      "Epoch [158/200] | Train Loss: 1.1034 (Cls: 0.4403, Reg: 0.9472) | Train Acc: 87.04% || Val Loss: 0.2312 (Cls: 0.0562, Reg: 0.2501) | Val Acc: 99.12%\n",
      "Epoch [159/200] | Train Loss: 1.1143 (Cls: 0.4483, Reg: 0.9514) | Train Acc: 87.18% || Val Loss: 0.2573 (Cls: 0.0663, Reg: 0.2729) | Val Acc: 99.04%\n",
      "Epoch [160/200] | Train Loss: 1.0761 (Cls: 0.4321, Reg: 0.9199) | Train Acc: 87.31% || Val Loss: 0.2421 (Cls: 0.0569, Reg: 0.2646) | Val Acc: 99.16%\n",
      "Epoch [161/200] | Train Loss: 1.0836 (Cls: 0.4386, Reg: 0.9214) | Train Acc: 87.42% || Val Loss: 0.2295 (Cls: 0.0558, Reg: 0.2481) | Val Acc: 99.20%\n",
      "Epoch [162/200] | Train Loss: 1.0770 (Cls: 0.4335, Reg: 0.9194) | Train Acc: 87.21% || Val Loss: 0.2239 (Cls: 0.0547, Reg: 0.2418) | Val Acc: 99.24%\n",
      "Epoch [163/200] | Train Loss: 1.0735 (Cls: 0.4248, Reg: 0.9268) | Train Acc: 87.52% || Val Loss: 0.2337 (Cls: 0.0537, Reg: 0.2571) | Val Acc: 99.29%\n",
      "Epoch [164/200] | Train Loss: 1.0802 (Cls: 0.4358, Reg: 0.9206) | Train Acc: 87.44% || Val Loss: 0.2219 (Cls: 0.0548, Reg: 0.2387) | Val Acc: 99.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2219) 至 ./models_save/rssi_csi_no_ppstd_07.pth\n",
      "Epoch [165/200] | Train Loss: 1.0654 (Cls: 0.4281, Reg: 0.9104) | Train Acc: 87.72% || Val Loss: 0.2232 (Cls: 0.0543, Reg: 0.2414) | Val Acc: 99.22%\n",
      "Epoch [166/200] | Train Loss: 1.0953 (Cls: 0.4384, Reg: 0.9385) | Train Acc: 87.33% || Val Loss: 0.2288 (Cls: 0.0561, Reg: 0.2469) | Val Acc: 99.22%\n",
      "Epoch [167/200] | Train Loss: 1.0807 (Cls: 0.4365, Reg: 0.9203) | Train Acc: 87.38% || Val Loss: 0.2413 (Cls: 0.0554, Reg: 0.2655) | Val Acc: 99.16%\n",
      "Epoch [168/200] | Train Loss: 1.0806 (Cls: 0.4292, Reg: 0.9305) | Train Acc: 87.41% || Val Loss: 0.2227 (Cls: 0.0550, Reg: 0.2396) | Val Acc: 99.08%\n",
      "Epoch [169/200] | Train Loss: 1.0707 (Cls: 0.4293, Reg: 0.9163) | Train Acc: 87.76% || Val Loss: 0.2366 (Cls: 0.0573, Reg: 0.2562) | Val Acc: 99.08%\n",
      "Epoch [170/200] | Train Loss: 1.0719 (Cls: 0.4264, Reg: 0.9222) | Train Acc: 87.08% || Val Loss: 0.2227 (Cls: 0.0559, Reg: 0.2383) | Val Acc: 99.12%\n",
      "Epoch [171/200] | Train Loss: 1.0830 (Cls: 0.4344, Reg: 0.9267) | Train Acc: 87.28% || Val Loss: 0.2328 (Cls: 0.0573, Reg: 0.2507) | Val Acc: 99.10%\n",
      "Epoch [172/200] | Train Loss: 1.0829 (Cls: 0.4357, Reg: 0.9245) | Train Acc: 87.55% || Val Loss: 0.2235 (Cls: 0.0537, Reg: 0.2426) | Val Acc: 99.20%\n",
      "Epoch [173/200] | Train Loss: 1.0963 (Cls: 0.4434, Reg: 0.9327) | Train Acc: 86.96% || Val Loss: 0.2288 (Cls: 0.0541, Reg: 0.2496) | Val Acc: 99.22%\n",
      "Epoch [174/200] | Train Loss: 1.0604 (Cls: 0.4198, Reg: 0.9151) | Train Acc: 87.63% || Val Loss: 0.2362 (Cls: 0.0564, Reg: 0.2568) | Val Acc: 99.18%\n",
      "Epoch [175/200] | Train Loss: 1.0655 (Cls: 0.4264, Reg: 0.9130) | Train Acc: 87.71% || Val Loss: 0.2297 (Cls: 0.0558, Reg: 0.2484) | Val Acc: 99.16%\n",
      "Epoch [176/200] | Train Loss: 1.0785 (Cls: 0.4284, Reg: 0.9288) | Train Acc: 87.48% || Val Loss: 0.2239 (Cls: 0.0548, Reg: 0.2416) | Val Acc: 99.16%\n",
      "Epoch [177/200] | Train Loss: 1.0789 (Cls: 0.4350, Reg: 0.9198) | Train Acc: 87.25% || Val Loss: 0.2230 (Cls: 0.0575, Reg: 0.2365) | Val Acc: 99.14%\n",
      "Epoch [178/200] | Train Loss: 1.0744 (Cls: 0.4259, Reg: 0.9265) | Train Acc: 87.50% || Val Loss: 0.2434 (Cls: 0.0606, Reg: 0.2611) | Val Acc: 99.18%\n",
      "Epoch [179/200] | Train Loss: 1.0849 (Cls: 0.4234, Reg: 0.9450) | Train Acc: 87.34% || Val Loss: 0.2380 (Cls: 0.0591, Reg: 0.2555) | Val Acc: 99.14%\n",
      "Epoch [180/200] | Train Loss: 1.0913 (Cls: 0.4411, Reg: 0.9288) | Train Acc: 87.33% || Val Loss: 0.2288 (Cls: 0.0562, Reg: 0.2465) | Val Acc: 99.22%\n",
      "Epoch [181/200] | Train Loss: 1.0539 (Cls: 0.4141, Reg: 0.9140) | Train Acc: 87.95% || Val Loss: 0.2292 (Cls: 0.0550, Reg: 0.2488) | Val Acc: 99.22%\n",
      "Epoch [182/200] | Train Loss: 1.0595 (Cls: 0.4149, Reg: 0.9208) | Train Acc: 88.05% || Val Loss: 0.2319 (Cls: 0.0555, Reg: 0.2519) | Val Acc: 99.29%\n",
      "Epoch [183/200] | Train Loss: 1.0367 (Cls: 0.4023, Reg: 0.9063) | Train Acc: 88.22% || Val Loss: 0.2241 (Cls: 0.0526, Reg: 0.2449) | Val Acc: 99.22%\n",
      "Epoch [184/200] | Train Loss: 1.0530 (Cls: 0.4209, Reg: 0.9030) | Train Acc: 87.74% || Val Loss: 0.2323 (Cls: 0.0534, Reg: 0.2556) | Val Acc: 99.20%\n",
      "Early stop at epoch 184\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "alpha = 0.7   # 回歸損失權重，可根據需要調整\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "# -----------------------\n",
    "# COORDINATES 字典：將分類標籤轉為 (X, Y) 座標\n",
    "# -----------------------\n",
    "COORDINATES = {\n",
    "    # 下邊界 (1-10 和 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    # 左邊界 (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    # 上邊界 (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    # 右邊界 (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    # 中間點 (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        # 將 0-index 轉換成 1-index (例如 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 訓練參數與 Early Stopping 設定\n",
    "# -----------------------\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_no_ppstd_07.pth\"\n",
    "epochs = 200\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# -----------------------\n",
    "# 訓練迴圈 (分類與回歸雙輸出)\n",
    "# -----------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_class_loss = 0.0\n",
    "    train_reg_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 每個 batch 返回 (amp_inputs, rssi_inputs, labels)\n",
    "    # 其中 labels 為 one-hot 編碼 (用以計算分類損失)\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # 分類目標：one-hot -> class index\n",
    "        target_class = torch.argmax(labels, dim=1)\n",
    "        loss_class = criterion(class_out, target_class)\n",
    "        \n",
    "        # 回歸目標：根據 target_class 透過 COORDINATES 字典取得 (X, Y) 座標\n",
    "        true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "        loss_reg = criterion_reg(reg_out, true_coords)\n",
    "        \n",
    "        loss = loss_class + alpha * loss_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size_curr = amp_inputs.size(0)\n",
    "        train_loss += loss.item() * batch_size_curr\n",
    "        train_class_loss += loss_class.item() * batch_size_curr\n",
    "        train_reg_loss += loss_reg.item() * batch_size_curr\n",
    "        _, predicted = torch.max(class_out, 1)\n",
    "        total_train += batch_size_curr\n",
    "        train_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_class_loss = train_class_loss / len(train_loader.dataset)\n",
    "    avg_train_reg_loss = train_reg_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_class_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            loss_class = criterion(class_out, target_class)\n",
    "            true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "            loss_reg = criterion_reg(reg_out, true_coords)\n",
    "            loss = loss_class + alpha * loss_reg\n",
    "            \n",
    "            batch_size_curr = amp_inputs.size(0)\n",
    "            val_loss += loss.item() * batch_size_curr\n",
    "            val_class_loss += loss_class.item() * batch_size_curr\n",
    "            val_reg_loss += loss_reg.item() * batch_size_curr\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total_val += batch_size_curr\n",
    "            val_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_class_loss = val_class_loss / len(val_loader.dataset)\n",
    "    avg_val_reg_loss = val_reg_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_class_loss:.4f}, Reg: {avg_train_reg_loss:.4f}) | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% || \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} (Cls: {avg_val_class_loss:.4f}, Reg: {avg_val_reg_loss:.4f}) | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 測試損失: 0.1457, 測試準確率: 99.06%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "\n",
    "# 載入最佳模型\n",
    "model.load_state_dict(torch.load(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN/models_save/rssi_csi_no_ppstd_07.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 測試模型\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_reg = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for amp_inputs, rssi_input, labels in test_loader:\n",
    "        amp_inputs, rssi_inputs, labels = amp_inputs.to(device), rssi_input.to(device), labels.to(device)\n",
    "        outputs, reg = model(amp_inputs, rssi_inputs)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "        test_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "        # 儲存真實標籤與預測標籤\n",
    "        all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_reg.extend(reg.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"📊 測試損失: {test_loss:.4f}, 測試準確率: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance error: 0.031111988074222698\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adjusted_predictions = np.array(all_predictions) + 1\n",
    "adjusted_labels = np.array(all_labels) + 1\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # 取出對應的座標\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # 計算歐氏距離\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) , errors\n",
    "\n",
    "mean_error, errors = compute_mean_distance_error(adjusted_labels, adjusted_predictions, COORDINATES)\n",
    "print(\"Mean distance error:\", mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# std 後\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Train Loss: 8.9381 (Cls: 4.2722, Reg: 6.6655) | Train Acc: 2.90% || Val Loss: 7.1531 (Cls: 3.5513, Reg: 5.1454) | Val Acc: 8.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 7.1531) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [2/200] | Train Loss: 6.6720 (Cls: 3.4335, Reg: 4.6264) | Train Acc: 8.35% || Val Loss: 5.0249 (Cls: 2.7151, Reg: 3.2998) | Val Acc: 36.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 5.0249) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [3/200] | Train Loss: 5.4175 (Cls: 2.8094, Reg: 3.7260) | Train Acc: 18.77% || Val Loss: 3.7352 (Cls: 1.9663, Reg: 2.5271) | Val Acc: 55.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 3.7352) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [4/200] | Train Loss: 4.5681 (Cls: 2.3209, Reg: 3.2103) | Train Acc: 29.25% || Val Loss: 3.0024 (Cls: 1.4407, Reg: 2.2310) | Val Acc: 71.02%\n",
      "✅ 儲存最佳模型 (Val Loss: 3.0024) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [5/200] | Train Loss: 3.9615 (Cls: 1.9495, Reg: 2.8743) | Train Acc: 38.77% || Val Loss: 2.2602 (Cls: 1.1016, Reg: 1.6552) | Val Acc: 82.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.2602) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [6/200] | Train Loss: 3.5454 (Cls: 1.7092, Reg: 2.6231) | Train Acc: 46.36% || Val Loss: 2.3370 (Cls: 0.9053, Reg: 2.0454) | Val Acc: 88.08%\n",
      "Epoch [7/200] | Train Loss: 3.2321 (Cls: 1.5353, Reg: 2.4239) | Train Acc: 51.22% || Val Loss: 1.7205 (Cls: 0.7097, Reg: 1.4439) | Val Acc: 86.00%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.7205) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [8/200] | Train Loss: 2.9490 (Cls: 1.3777, Reg: 2.2447) | Train Acc: 56.19% || Val Loss: 1.5223 (Cls: 0.5749, Reg: 1.3534) | Val Acc: 90.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.5223) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [9/200] | Train Loss: 2.7439 (Cls: 1.2661, Reg: 2.1112) | Train Acc: 59.07% || Val Loss: 1.2173 (Cls: 0.4699, Reg: 1.0676) | Val Acc: 89.78%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.2173) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [10/200] | Train Loss: 2.5586 (Cls: 1.1702, Reg: 1.9835) | Train Acc: 62.70% || Val Loss: 1.3445 (Cls: 0.4594, Reg: 1.2643) | Val Acc: 90.41%\n",
      "Epoch [11/200] | Train Loss: 2.4548 (Cls: 1.1236, Reg: 1.9017) | Train Acc: 64.75% || Val Loss: 1.0772 (Cls: 0.3888, Reg: 0.9835) | Val Acc: 94.47%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.0772) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [12/200] | Train Loss: 2.3118 (Cls: 1.0440, Reg: 1.8111) | Train Acc: 66.31% || Val Loss: 0.9980 (Cls: 0.3440, Reg: 0.9343) | Val Acc: 93.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9980) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [13/200] | Train Loss: 2.2493 (Cls: 1.0109, Reg: 1.7691) | Train Acc: 67.90% || Val Loss: 0.8886 (Cls: 0.2939, Reg: 0.8496) | Val Acc: 94.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.8886) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [14/200] | Train Loss: 2.1251 (Cls: 0.9583, Reg: 1.6669) | Train Acc: 69.26% || Val Loss: 0.8719 (Cls: 0.2942, Reg: 0.8253) | Val Acc: 92.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.8719) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [15/200] | Train Loss: 2.0752 (Cls: 0.9337, Reg: 1.6307) | Train Acc: 70.28% || Val Loss: 0.7840 (Cls: 0.2680, Reg: 0.7372) | Val Acc: 95.76%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7840) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [16/200] | Train Loss: 2.0495 (Cls: 0.9141, Reg: 1.6220) | Train Acc: 70.85% || Val Loss: 0.6785 (Cls: 0.2328, Reg: 0.6368) | Val Acc: 95.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6785) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [17/200] | Train Loss: 1.9740 (Cls: 0.8821, Reg: 1.5599) | Train Acc: 71.78% || Val Loss: 0.7185 (Cls: 0.2277, Reg: 0.7012) | Val Acc: 94.33%\n",
      "Epoch [18/200] | Train Loss: 1.9389 (Cls: 0.8649, Reg: 1.5343) | Train Acc: 72.63% || Val Loss: 0.7464 (Cls: 0.2363, Reg: 0.7286) | Val Acc: 94.45%\n",
      "Epoch [19/200] | Train Loss: 1.8931 (Cls: 0.8363, Reg: 1.5097) | Train Acc: 73.20% || Val Loss: 0.6575 (Cls: 0.2078, Reg: 0.6424) | Val Acc: 96.86%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6575) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [20/200] | Train Loss: 1.9181 (Cls: 0.8522, Reg: 1.5227) | Train Acc: 72.51% || Val Loss: 0.6678 (Cls: 0.2075, Reg: 0.6576) | Val Acc: 93.55%\n",
      "Epoch [21/200] | Train Loss: 1.8634 (Cls: 0.8309, Reg: 1.4750) | Train Acc: 73.38% || Val Loss: 0.7020 (Cls: 0.2078, Reg: 0.7060) | Val Acc: 94.35%\n",
      "Epoch [22/200] | Train Loss: 1.8108 (Cls: 0.7968, Reg: 1.4486) | Train Acc: 74.64% || Val Loss: 0.5961 (Cls: 0.1815, Reg: 0.5924) | Val Acc: 96.00%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5961) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [23/200] | Train Loss: 1.7744 (Cls: 0.7900, Reg: 1.4063) | Train Acc: 74.92% || Val Loss: 0.5803 (Cls: 0.1664, Reg: 0.5913) | Val Acc: 95.96%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5803) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [24/200] | Train Loss: 1.7814 (Cls: 0.7925, Reg: 1.4127) | Train Acc: 75.28% || Val Loss: 0.5950 (Cls: 0.1621, Reg: 0.6185) | Val Acc: 97.49%\n",
      "Epoch [25/200] | Train Loss: 1.7698 (Cls: 0.7926, Reg: 1.3961) | Train Acc: 74.96% || Val Loss: 0.6451 (Cls: 0.1980, Reg: 0.6387) | Val Acc: 96.49%\n",
      "Epoch [26/200] | Train Loss: 1.7485 (Cls: 0.7739, Reg: 1.3924) | Train Acc: 76.07% || Val Loss: 0.5264 (Cls: 0.1509, Reg: 0.5364) | Val Acc: 95.31%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5264) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [27/200] | Train Loss: 1.7137 (Cls: 0.7555, Reg: 1.3689) | Train Acc: 76.47% || Val Loss: 0.4696 (Cls: 0.1433, Reg: 0.4661) | Val Acc: 96.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4696) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [28/200] | Train Loss: 1.6752 (Cls: 0.7376, Reg: 1.3394) | Train Acc: 76.64% || Val Loss: 0.4969 (Cls: 0.1441, Reg: 0.5040) | Val Acc: 95.76%\n",
      "Epoch [29/200] | Train Loss: 1.6869 (Cls: 0.7473, Reg: 1.3423) | Train Acc: 76.90% || Val Loss: 0.4966 (Cls: 0.1612, Reg: 0.4792) | Val Acc: 95.76%\n",
      "Epoch [30/200] | Train Loss: 1.6751 (Cls: 0.7351, Reg: 1.3428) | Train Acc: 77.02% || Val Loss: 0.6150 (Cls: 0.1920, Reg: 0.6044) | Val Acc: 93.61%\n",
      "Epoch [31/200] | Train Loss: 1.6228 (Cls: 0.7069, Reg: 1.3085) | Train Acc: 77.64% || Val Loss: 0.4360 (Cls: 0.1317, Reg: 0.4347) | Val Acc: 97.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4360) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [32/200] | Train Loss: 1.6051 (Cls: 0.6912, Reg: 1.3055) | Train Acc: 77.90% || Val Loss: 0.4440 (Cls: 0.1246, Reg: 0.4563) | Val Acc: 96.53%\n",
      "Epoch [33/200] | Train Loss: 1.6258 (Cls: 0.7092, Reg: 1.3095) | Train Acc: 77.78% || Val Loss: 0.5583 (Cls: 0.1569, Reg: 0.5735) | Val Acc: 96.04%\n",
      "Epoch [34/200] | Train Loss: 1.5981 (Cls: 0.7029, Reg: 1.2789) | Train Acc: 77.89% || Val Loss: 0.4495 (Cls: 0.1251, Reg: 0.4633) | Val Acc: 97.02%\n",
      "Epoch [35/200] | Train Loss: 1.5904 (Cls: 0.6905, Reg: 1.2856) | Train Acc: 78.41% || Val Loss: 0.4542 (Cls: 0.1204, Reg: 0.4767) | Val Acc: 96.78%\n",
      "Epoch [36/200] | Train Loss: 1.5859 (Cls: 0.6932, Reg: 1.2753) | Train Acc: 78.22% || Val Loss: 0.4993 (Cls: 0.1379, Reg: 0.5162) | Val Acc: 97.65%\n",
      "Epoch [37/200] | Train Loss: 1.6241 (Cls: 0.7185, Reg: 1.2937) | Train Acc: 77.20% || Val Loss: 0.4860 (Cls: 0.1430, Reg: 0.4900) | Val Acc: 96.12%\n",
      "Epoch [38/200] | Train Loss: 1.6062 (Cls: 0.7018, Reg: 1.2920) | Train Acc: 78.39% || Val Loss: 0.4179 (Cls: 0.1177, Reg: 0.4289) | Val Acc: 98.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4179) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [39/200] | Train Loss: 1.6058 (Cls: 0.7132, Reg: 1.2752) | Train Acc: 77.78% || Val Loss: 0.5005 (Cls: 0.1313, Reg: 0.5275) | Val Acc: 97.51%\n",
      "Epoch [40/200] | Train Loss: 1.5651 (Cls: 0.6807, Reg: 1.2634) | Train Acc: 78.91% || Val Loss: 0.3804 (Cls: 0.1136, Reg: 0.3810) | Val Acc: 96.78%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3804) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [41/200] | Train Loss: 1.5768 (Cls: 0.6924, Reg: 1.2634) | Train Acc: 78.27% || Val Loss: 0.4680 (Cls: 0.1246, Reg: 0.4905) | Val Acc: 97.82%\n",
      "Epoch [42/200] | Train Loss: 1.5670 (Cls: 0.6891, Reg: 1.2541) | Train Acc: 78.49% || Val Loss: 0.4287 (Cls: 0.1305, Reg: 0.4260) | Val Acc: 97.71%\n",
      "Epoch [43/200] | Train Loss: 1.5243 (Cls: 0.6540, Reg: 1.2433) | Train Acc: 79.63% || Val Loss: 0.3897 (Cls: 0.1068, Reg: 0.4043) | Val Acc: 97.18%\n",
      "Epoch [44/200] | Train Loss: 1.5027 (Cls: 0.6442, Reg: 1.2265) | Train Acc: 79.54% || Val Loss: 0.4387 (Cls: 0.1152, Reg: 0.4622) | Val Acc: 97.16%\n",
      "Epoch [45/200] | Train Loss: 1.5655 (Cls: 0.6873, Reg: 1.2546) | Train Acc: 79.40% || Val Loss: 0.4423 (Cls: 0.1179, Reg: 0.4634) | Val Acc: 98.04%\n",
      "Epoch [46/200] | Train Loss: 1.5283 (Cls: 0.6527, Reg: 1.2508) | Train Acc: 79.28% || Val Loss: 0.4142 (Cls: 0.1097, Reg: 0.4350) | Val Acc: 97.65%\n",
      "Epoch [47/200] | Train Loss: 1.5251 (Cls: 0.6652, Reg: 1.2285) | Train Acc: 79.08% || Val Loss: 0.4012 (Cls: 0.1077, Reg: 0.4193) | Val Acc: 97.63%\n",
      "Epoch [48/200] | Train Loss: 1.5271 (Cls: 0.6587, Reg: 1.2406) | Train Acc: 79.83% || Val Loss: 0.3999 (Cls: 0.1111, Reg: 0.4127) | Val Acc: 97.94%\n",
      "Epoch [49/200] | Train Loss: 1.5276 (Cls: 0.6561, Reg: 1.2450) | Train Acc: 80.05% || Val Loss: 0.4190 (Cls: 0.1217, Reg: 0.4248) | Val Acc: 97.45%\n",
      "Epoch [50/200] | Train Loss: 1.5153 (Cls: 0.6551, Reg: 1.2288) | Train Acc: 79.64% || Val Loss: 0.3851 (Cls: 0.1093, Reg: 0.3940) | Val Acc: 97.12%\n",
      "Epoch [51/200] | Train Loss: 1.5168 (Cls: 0.6572, Reg: 1.2280) | Train Acc: 79.40% || Val Loss: 0.3553 (Cls: 0.1082, Reg: 0.3530) | Val Acc: 96.92%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3553) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [52/200] | Train Loss: 1.5016 (Cls: 0.6521, Reg: 1.2136) | Train Acc: 80.08% || Val Loss: 0.4075 (Cls: 0.1142, Reg: 0.4190) | Val Acc: 96.61%\n",
      "Epoch [53/200] | Train Loss: 1.5160 (Cls: 0.6602, Reg: 1.2225) | Train Acc: 79.32% || Val Loss: 0.3918 (Cls: 0.1042, Reg: 0.4108) | Val Acc: 97.31%\n",
      "Epoch [54/200] | Train Loss: 1.4788 (Cls: 0.6349, Reg: 1.2056) | Train Acc: 80.00% || Val Loss: 0.3834 (Cls: 0.1074, Reg: 0.3943) | Val Acc: 96.84%\n",
      "Epoch [55/200] | Train Loss: 1.5038 (Cls: 0.6542, Reg: 1.2138) | Train Acc: 79.88% || Val Loss: 0.4783 (Cls: 0.1332, Reg: 0.4930) | Val Acc: 97.73%\n",
      "Epoch [56/200] | Train Loss: 1.4750 (Cls: 0.6373, Reg: 1.1966) | Train Acc: 80.12% || Val Loss: 0.4120 (Cls: 0.1147, Reg: 0.4247) | Val Acc: 97.00%\n",
      "Epoch [57/200] | Train Loss: 1.4556 (Cls: 0.6244, Reg: 1.1874) | Train Acc: 80.59% || Val Loss: 0.3245 (Cls: 0.0956, Reg: 0.3270) | Val Acc: 97.04%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3245) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [58/200] | Train Loss: 1.4926 (Cls: 0.6490, Reg: 1.2052) | Train Acc: 79.78% || Val Loss: 0.3500 (Cls: 0.1080, Reg: 0.3458) | Val Acc: 97.29%\n",
      "Epoch [59/200] | Train Loss: 1.4839 (Cls: 0.6352, Reg: 1.2125) | Train Acc: 80.20% || Val Loss: 0.4152 (Cls: 0.1032, Reg: 0.4457) | Val Acc: 97.27%\n",
      "Epoch [60/200] | Train Loss: 1.5037 (Cls: 0.6493, Reg: 1.2206) | Train Acc: 79.85% || Val Loss: 0.3683 (Cls: 0.1007, Reg: 0.3823) | Val Acc: 96.84%\n",
      "Epoch [61/200] | Train Loss: 1.4777 (Cls: 0.6455, Reg: 1.1889) | Train Acc: 79.50% || Val Loss: 0.3621 (Cls: 0.1166, Reg: 0.3506) | Val Acc: 96.57%\n",
      "Epoch [62/200] | Train Loss: 1.4589 (Cls: 0.6293, Reg: 1.1852) | Train Acc: 80.40% || Val Loss: 0.3968 (Cls: 0.1092, Reg: 0.4109) | Val Acc: 97.00%\n",
      "Epoch [63/200] | Train Loss: 1.4376 (Cls: 0.6047, Reg: 1.1898) | Train Acc: 80.89% || Val Loss: 0.4708 (Cls: 0.1273, Reg: 0.4906) | Val Acc: 96.51%\n",
      "Epoch [64/200] | Train Loss: 1.4754 (Cls: 0.6292, Reg: 1.2089) | Train Acc: 80.74% || Val Loss: 0.4032 (Cls: 0.1008, Reg: 0.4321) | Val Acc: 97.16%\n",
      "Epoch [65/200] | Train Loss: 1.4537 (Cls: 0.6205, Reg: 1.1902) | Train Acc: 80.54% || Val Loss: 0.3348 (Cls: 0.0911, Reg: 0.3482) | Val Acc: 98.22%\n",
      "Epoch [66/200] | Train Loss: 1.4706 (Cls: 0.6328, Reg: 1.1970) | Train Acc: 80.44% || Val Loss: 0.4738 (Cls: 0.1282, Reg: 0.4937) | Val Acc: 96.49%\n",
      "Epoch [67/200] | Train Loss: 1.4266 (Cls: 0.6061, Reg: 1.1721) | Train Acc: 81.23% || Val Loss: 0.3885 (Cls: 0.1035, Reg: 0.4072) | Val Acc: 98.06%\n",
      "Epoch [68/200] | Train Loss: 1.4359 (Cls: 0.6177, Reg: 1.1688) | Train Acc: 80.73% || Val Loss: 0.3694 (Cls: 0.0981, Reg: 0.3875) | Val Acc: 97.69%\n",
      "Epoch [69/200] | Train Loss: 1.4388 (Cls: 0.6219, Reg: 1.1670) | Train Acc: 80.70% || Val Loss: 0.3492 (Cls: 0.1065, Reg: 0.3466) | Val Acc: 97.27%\n",
      "Epoch [70/200] | Train Loss: 1.4438 (Cls: 0.6270, Reg: 1.1669) | Train Acc: 80.43% || Val Loss: 0.3627 (Cls: 0.0953, Reg: 0.3820) | Val Acc: 98.57%\n",
      "Epoch [71/200] | Train Loss: 1.4306 (Cls: 0.6137, Reg: 1.1670) | Train Acc: 81.13% || Val Loss: 0.3534 (Cls: 0.1033, Reg: 0.3573) | Val Acc: 97.06%\n",
      "Epoch [72/200] | Train Loss: 1.4313 (Cls: 0.6105, Reg: 1.1726) | Train Acc: 81.32% || Val Loss: 0.3655 (Cls: 0.0915, Reg: 0.3915) | Val Acc: 97.14%\n",
      "Epoch [73/200] | Train Loss: 1.4347 (Cls: 0.6123, Reg: 1.1749) | Train Acc: 81.31% || Val Loss: 0.3467 (Cls: 0.0974, Reg: 0.3561) | Val Acc: 96.96%\n",
      "Epoch [74/200] | Train Loss: 1.3377 (Cls: 0.5620, Reg: 1.1083) | Train Acc: 82.33% || Val Loss: 0.3080 (Cls: 0.0845, Reg: 0.3193) | Val Acc: 98.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3080) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [75/200] | Train Loss: 1.3141 (Cls: 0.5459, Reg: 1.0974) | Train Acc: 82.82% || Val Loss: 0.2906 (Cls: 0.0794, Reg: 0.3016) | Val Acc: 98.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2906) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [76/200] | Train Loss: 1.2942 (Cls: 0.5335, Reg: 1.0867) | Train Acc: 82.93% || Val Loss: 0.2920 (Cls: 0.0813, Reg: 0.3010) | Val Acc: 97.22%\n",
      "Epoch [77/200] | Train Loss: 1.2721 (Cls: 0.5249, Reg: 1.0674) | Train Acc: 83.43% || Val Loss: 0.2905 (Cls: 0.0824, Reg: 0.2973) | Val Acc: 97.43%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2905) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [78/200] | Train Loss: 1.2646 (Cls: 0.5176, Reg: 1.0672) | Train Acc: 84.10% || Val Loss: 0.3265 (Cls: 0.0838, Reg: 0.3466) | Val Acc: 97.82%\n",
      "Epoch [79/200] | Train Loss: 1.2916 (Cls: 0.5372, Reg: 1.0777) | Train Acc: 83.45% || Val Loss: 0.2883 (Cls: 0.0833, Reg: 0.2930) | Val Acc: 97.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2883) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [80/200] | Train Loss: 1.2815 (Cls: 0.5245, Reg: 1.0814) | Train Acc: 83.80% || Val Loss: 0.2951 (Cls: 0.0828, Reg: 0.3033) | Val Acc: 99.18%\n",
      "Epoch [81/200] | Train Loss: 1.2939 (Cls: 0.5382, Reg: 1.0796) | Train Acc: 83.19% || Val Loss: 0.2954 (Cls: 0.0798, Reg: 0.3080) | Val Acc: 98.88%\n",
      "Epoch [82/200] | Train Loss: 1.2685 (Cls: 0.5218, Reg: 1.0668) | Train Acc: 84.03% || Val Loss: 0.2883 (Cls: 0.0775, Reg: 0.3011) | Val Acc: 97.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2883) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [83/200] | Train Loss: 1.2636 (Cls: 0.5240, Reg: 1.0566) | Train Acc: 83.77% || Val Loss: 0.2822 (Cls: 0.0776, Reg: 0.2922) | Val Acc: 97.78%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2822) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [84/200] | Train Loss: 1.2807 (Cls: 0.5302, Reg: 1.0722) | Train Acc: 83.59% || Val Loss: 0.2947 (Cls: 0.0841, Reg: 0.3008) | Val Acc: 98.12%\n",
      "Epoch [85/200] | Train Loss: 1.2530 (Cls: 0.5107, Reg: 1.0605) | Train Acc: 83.96% || Val Loss: 0.2814 (Cls: 0.0792, Reg: 0.2889) | Val Acc: 97.31%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2814) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [86/200] | Train Loss: 1.2517 (Cls: 0.5128, Reg: 1.0556) | Train Acc: 83.94% || Val Loss: 0.2992 (Cls: 0.0852, Reg: 0.3057) | Val Acc: 98.08%\n",
      "Epoch [87/200] | Train Loss: 1.2587 (Cls: 0.5208, Reg: 1.0543) | Train Acc: 83.92% || Val Loss: 0.3085 (Cls: 0.0905, Reg: 0.3114) | Val Acc: 98.90%\n",
      "Epoch [88/200] | Train Loss: 1.2933 (Cls: 0.5425, Reg: 1.0726) | Train Acc: 83.74% || Val Loss: 0.3036 (Cls: 0.0882, Reg: 0.3077) | Val Acc: 97.49%\n",
      "Epoch [89/200] | Train Loss: 1.2489 (Cls: 0.5169, Reg: 1.0457) | Train Acc: 83.77% || Val Loss: 0.3038 (Cls: 0.0816, Reg: 0.3175) | Val Acc: 98.08%\n",
      "Epoch [90/200] | Train Loss: 1.2525 (Cls: 0.5146, Reg: 1.0541) | Train Acc: 84.01% || Val Loss: 0.3300 (Cls: 0.0813, Reg: 0.3553) | Val Acc: 97.31%\n",
      "Epoch [91/200] | Train Loss: 1.2591 (Cls: 0.5151, Reg: 1.0628) | Train Acc: 84.18% || Val Loss: 0.2742 (Cls: 0.0790, Reg: 0.2787) | Val Acc: 98.82%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2742) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [92/200] | Train Loss: 1.2542 (Cls: 0.5153, Reg: 1.0556) | Train Acc: 83.71% || Val Loss: 0.2745 (Cls: 0.0798, Reg: 0.2781) | Val Acc: 98.39%\n",
      "Epoch [93/200] | Train Loss: 1.2575 (Cls: 0.5204, Reg: 1.0531) | Train Acc: 84.15% || Val Loss: 0.2554 (Cls: 0.0797, Reg: 0.2510) | Val Acc: 98.98%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2554) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [94/200] | Train Loss: 1.2585 (Cls: 0.5222, Reg: 1.0519) | Train Acc: 83.98% || Val Loss: 0.2817 (Cls: 0.0851, Reg: 0.2809) | Val Acc: 97.47%\n",
      "Epoch [95/200] | Train Loss: 1.2505 (Cls: 0.5166, Reg: 1.0483) | Train Acc: 84.42% || Val Loss: 0.2518 (Cls: 0.0759, Reg: 0.2514) | Val Acc: 98.41%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2518) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [96/200] | Train Loss: 1.2245 (Cls: 0.5033, Reg: 1.0302) | Train Acc: 84.22% || Val Loss: 0.2889 (Cls: 0.0780, Reg: 0.3013) | Val Acc: 97.69%\n",
      "Epoch [97/200] | Train Loss: 1.2355 (Cls: 0.4976, Reg: 1.0541) | Train Acc: 84.40% || Val Loss: 0.3021 (Cls: 0.0863, Reg: 0.3084) | Val Acc: 97.41%\n",
      "Epoch [98/200] | Train Loss: 1.2328 (Cls: 0.5003, Reg: 1.0465) | Train Acc: 84.43% || Val Loss: 0.3205 (Cls: 0.0834, Reg: 0.3386) | Val Acc: 97.35%\n",
      "Epoch [99/200] | Train Loss: 1.2273 (Cls: 0.4996, Reg: 1.0395) | Train Acc: 84.57% || Val Loss: 0.2660 (Cls: 0.0726, Reg: 0.2763) | Val Acc: 97.55%\n",
      "Epoch [100/200] | Train Loss: 1.2356 (Cls: 0.5088, Reg: 1.0382) | Train Acc: 84.31% || Val Loss: 0.2857 (Cls: 0.0763, Reg: 0.2991) | Val Acc: 97.49%\n",
      "Epoch [101/200] | Train Loss: 1.2115 (Cls: 0.4910, Reg: 1.0292) | Train Acc: 84.72% || Val Loss: 0.2923 (Cls: 0.0775, Reg: 0.3069) | Val Acc: 98.02%\n",
      "Epoch [102/200] | Train Loss: 1.2383 (Cls: 0.5060, Reg: 1.0461) | Train Acc: 84.64% || Val Loss: 0.4154 (Cls: 0.1071, Reg: 0.4405) | Val Acc: 96.73%\n",
      "Epoch [103/200] | Train Loss: 1.2457 (Cls: 0.5141, Reg: 1.0451) | Train Acc: 84.06% || Val Loss: 0.2904 (Cls: 0.0769, Reg: 0.3049) | Val Acc: 97.18%\n",
      "Epoch [104/200] | Train Loss: 1.2567 (Cls: 0.5202, Reg: 1.0521) | Train Acc: 84.26% || Val Loss: 0.2577 (Cls: 0.0716, Reg: 0.2659) | Val Acc: 97.59%\n",
      "Epoch [105/200] | Train Loss: 1.2232 (Cls: 0.5010, Reg: 1.0318) | Train Acc: 84.38% || Val Loss: 0.2802 (Cls: 0.0745, Reg: 0.2939) | Val Acc: 97.55%\n",
      "Epoch [106/200] | Train Loss: 1.2242 (Cls: 0.4981, Reg: 1.0373) | Train Acc: 84.76% || Val Loss: 0.2574 (Cls: 0.0724, Reg: 0.2643) | Val Acc: 99.04%\n",
      "Epoch [107/200] | Train Loss: 1.2171 (Cls: 0.4966, Reg: 1.0293) | Train Acc: 84.93% || Val Loss: 0.2811 (Cls: 0.0740, Reg: 0.2958) | Val Acc: 97.67%\n",
      "Epoch [108/200] | Train Loss: 1.2297 (Cls: 0.5049, Reg: 1.0354) | Train Acc: 84.72% || Val Loss: 0.2447 (Cls: 0.0683, Reg: 0.2521) | Val Acc: 97.86%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2447) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [109/200] | Train Loss: 1.2342 (Cls: 0.5062, Reg: 1.0401) | Train Acc: 84.53% || Val Loss: 0.2922 (Cls: 0.0737, Reg: 0.3122) | Val Acc: 98.86%\n",
      "Epoch [110/200] | Train Loss: 1.2211 (Cls: 0.5015, Reg: 1.0280) | Train Acc: 84.19% || Val Loss: 0.2813 (Cls: 0.0747, Reg: 0.2950) | Val Acc: 98.10%\n",
      "Epoch [111/200] | Train Loss: 1.2304 (Cls: 0.5078, Reg: 1.0324) | Train Acc: 84.71% || Val Loss: 0.2805 (Cls: 0.0730, Reg: 0.2965) | Val Acc: 97.78%\n",
      "Epoch [112/200] | Train Loss: 1.2428 (Cls: 0.5115, Reg: 1.0447) | Train Acc: 84.24% || Val Loss: 0.2928 (Cls: 0.0733, Reg: 0.3135) | Val Acc: 97.49%\n",
      "Epoch [113/200] | Train Loss: 1.2276 (Cls: 0.4993, Reg: 1.0405) | Train Acc: 84.52% || Val Loss: 0.2831 (Cls: 0.0722, Reg: 0.3012) | Val Acc: 99.04%\n",
      "Epoch [114/200] | Train Loss: 1.2188 (Cls: 0.4927, Reg: 1.0372) | Train Acc: 84.89% || Val Loss: 0.2562 (Cls: 0.0737, Reg: 0.2607) | Val Acc: 97.63%\n",
      "Epoch [115/200] | Train Loss: 1.2128 (Cls: 0.4865, Reg: 1.0376) | Train Acc: 85.13% || Val Loss: 0.2785 (Cls: 0.0718, Reg: 0.2952) | Val Acc: 99.20%\n",
      "Epoch [116/200] | Train Loss: 1.2168 (Cls: 0.4920, Reg: 1.0354) | Train Acc: 84.53% || Val Loss: 0.2638 (Cls: 0.0671, Reg: 0.2810) | Val Acc: 99.12%\n",
      "Epoch [117/200] | Train Loss: 1.2146 (Cls: 0.4965, Reg: 1.0259) | Train Acc: 84.66% || Val Loss: 0.2694 (Cls: 0.0710, Reg: 0.2834) | Val Acc: 97.86%\n",
      "Epoch [118/200] | Train Loss: 1.2235 (Cls: 0.4959, Reg: 1.0394) | Train Acc: 84.84% || Val Loss: 0.2532 (Cls: 0.0737, Reg: 0.2565) | Val Acc: 98.39%\n",
      "Epoch [119/200] | Train Loss: 1.2192 (Cls: 0.4948, Reg: 1.0349) | Train Acc: 84.63% || Val Loss: 0.2714 (Cls: 0.0697, Reg: 0.2880) | Val Acc: 99.43%\n",
      "Epoch [120/200] | Train Loss: 1.2218 (Cls: 0.4888, Reg: 1.0472) | Train Acc: 85.05% || Val Loss: 0.2578 (Cls: 0.0688, Reg: 0.2700) | Val Acc: 99.04%\n",
      "Epoch [121/200] | Train Loss: 1.1958 (Cls: 0.4812, Reg: 1.0208) | Train Acc: 84.98% || Val Loss: 0.2699 (Cls: 0.0661, Reg: 0.2912) | Val Acc: 97.84%\n",
      "Epoch [122/200] | Train Loss: 1.2220 (Cls: 0.4919, Reg: 1.0430) | Train Acc: 84.76% || Val Loss: 0.2850 (Cls: 0.0731, Reg: 0.3028) | Val Acc: 99.27%\n",
      "Epoch [123/200] | Train Loss: 1.2134 (Cls: 0.4932, Reg: 1.0288) | Train Acc: 84.85% || Val Loss: 0.2640 (Cls: 0.0696, Reg: 0.2777) | Val Acc: 97.92%\n",
      "Epoch [124/200] | Train Loss: 1.2039 (Cls: 0.4860, Reg: 1.0255) | Train Acc: 85.11% || Val Loss: 0.2612 (Cls: 0.0675, Reg: 0.2767) | Val Acc: 97.84%\n",
      "Epoch [125/200] | Train Loss: 1.1671 (Cls: 0.4655, Reg: 1.0022) | Train Acc: 85.54% || Val Loss: 0.2460 (Cls: 0.0687, Reg: 0.2533) | Val Acc: 98.22%\n",
      "Epoch [126/200] | Train Loss: 1.1666 (Cls: 0.4654, Reg: 1.0017) | Train Acc: 85.52% || Val Loss: 0.2661 (Cls: 0.0685, Reg: 0.2823) | Val Acc: 97.82%\n",
      "Epoch [127/200] | Train Loss: 1.1594 (Cls: 0.4580, Reg: 1.0019) | Train Acc: 85.88% || Val Loss: 0.2355 (Cls: 0.0682, Reg: 0.2390) | Val Acc: 99.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2355) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [128/200] | Train Loss: 1.1518 (Cls: 0.4581, Reg: 0.9911) | Train Acc: 86.13% || Val Loss: 0.2449 (Cls: 0.0660, Reg: 0.2556) | Val Acc: 98.84%\n",
      "Epoch [129/200] | Train Loss: 1.1345 (Cls: 0.4470, Reg: 0.9822) | Train Acc: 86.06% || Val Loss: 0.2332 (Cls: 0.0640, Reg: 0.2417) | Val Acc: 98.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2332) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [130/200] | Train Loss: 1.1417 (Cls: 0.4503, Reg: 0.9877) | Train Acc: 85.98% || Val Loss: 0.2342 (Cls: 0.0657, Reg: 0.2408) | Val Acc: 98.61%\n",
      "Epoch [131/200] | Train Loss: 1.1394 (Cls: 0.4460, Reg: 0.9906) | Train Acc: 86.34% || Val Loss: 0.2437 (Cls: 0.0642, Reg: 0.2566) | Val Acc: 99.47%\n",
      "Epoch [132/200] | Train Loss: 1.1562 (Cls: 0.4581, Reg: 0.9972) | Train Acc: 86.05% || Val Loss: 0.2613 (Cls: 0.0663, Reg: 0.2785) | Val Acc: 99.20%\n",
      "Epoch [133/200] | Train Loss: 1.1243 (Cls: 0.4441, Reg: 0.9717) | Train Acc: 86.15% || Val Loss: 0.2836 (Cls: 0.0726, Reg: 0.3015) | Val Acc: 99.16%\n",
      "Epoch [134/200] | Train Loss: 1.1428 (Cls: 0.4589, Reg: 0.9770) | Train Acc: 85.81% || Val Loss: 0.2465 (Cls: 0.0712, Reg: 0.2504) | Val Acc: 98.82%\n",
      "Epoch [135/200] | Train Loss: 1.1425 (Cls: 0.4506, Reg: 0.9885) | Train Acc: 86.52% || Val Loss: 0.2623 (Cls: 0.0695, Reg: 0.2754) | Val Acc: 99.14%\n",
      "Epoch [136/200] | Train Loss: 1.1398 (Cls: 0.4570, Reg: 0.9754) | Train Acc: 86.06% || Val Loss: 0.2306 (Cls: 0.0687, Reg: 0.2313) | Val Acc: 98.88%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2306) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [137/200] | Train Loss: 1.1131 (Cls: 0.4326, Reg: 0.9721) | Train Acc: 86.55% || Val Loss: 0.2437 (Cls: 0.0659, Reg: 0.2541) | Val Acc: 99.10%\n",
      "Epoch [138/200] | Train Loss: 1.1325 (Cls: 0.4490, Reg: 0.9766) | Train Acc: 86.41% || Val Loss: 0.2604 (Cls: 0.0718, Reg: 0.2694) | Val Acc: 99.31%\n",
      "Epoch [139/200] | Train Loss: 1.1255 (Cls: 0.4380, Reg: 0.9822) | Train Acc: 86.68% || Val Loss: 0.2467 (Cls: 0.0727, Reg: 0.2486) | Val Acc: 98.82%\n",
      "Epoch [140/200] | Train Loss: 1.1256 (Cls: 0.4456, Reg: 0.9713) | Train Acc: 86.03% || Val Loss: 0.2750 (Cls: 0.0716, Reg: 0.2906) | Val Acc: 99.47%\n",
      "Epoch [141/200] | Train Loss: 1.1341 (Cls: 0.4449, Reg: 0.9845) | Train Acc: 86.56% || Val Loss: 0.2529 (Cls: 0.0658, Reg: 0.2673) | Val Acc: 99.10%\n",
      "Epoch [142/200] | Train Loss: 1.1183 (Cls: 0.4395, Reg: 0.9696) | Train Acc: 86.40% || Val Loss: 0.2413 (Cls: 0.0722, Reg: 0.2416) | Val Acc: 99.24%\n",
      "Epoch [143/200] | Train Loss: 1.1366 (Cls: 0.4512, Reg: 0.9791) | Train Acc: 86.38% || Val Loss: 0.2495 (Cls: 0.0717, Reg: 0.2539) | Val Acc: 99.41%\n",
      "Epoch [144/200] | Train Loss: 1.1414 (Cls: 0.4492, Reg: 0.9888) | Train Acc: 86.24% || Val Loss: 0.2467 (Cls: 0.0721, Reg: 0.2495) | Val Acc: 98.69%\n",
      "Epoch [145/200] | Train Loss: 1.0956 (Cls: 0.4246, Reg: 0.9586) | Train Acc: 86.84% || Val Loss: 0.2333 (Cls: 0.0703, Reg: 0.2329) | Val Acc: 98.92%\n",
      "Epoch [146/200] | Train Loss: 1.1369 (Cls: 0.4541, Reg: 0.9753) | Train Acc: 86.40% || Val Loss: 0.2442 (Cls: 0.0719, Reg: 0.2462) | Val Acc: 99.33%\n",
      "Epoch [147/200] | Train Loss: 1.1306 (Cls: 0.4411, Reg: 0.9851) | Train Acc: 86.10% || Val Loss: 0.2265 (Cls: 0.0720, Reg: 0.2207) | Val Acc: 99.37%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2265) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [148/200] | Train Loss: 1.1385 (Cls: 0.4515, Reg: 0.9815) | Train Acc: 86.20% || Val Loss: 0.3010 (Cls: 0.0778, Reg: 0.3188) | Val Acc: 99.14%\n",
      "Epoch [149/200] | Train Loss: 1.1505 (Cls: 0.4615, Reg: 0.9843) | Train Acc: 86.26% || Val Loss: 0.2217 (Cls: 0.0702, Reg: 0.2164) | Val Acc: 99.47%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2217) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [150/200] | Train Loss: 1.1317 (Cls: 0.4414, Reg: 0.9862) | Train Acc: 86.71% || Val Loss: 0.2447 (Cls: 0.0690, Reg: 0.2510) | Val Acc: 99.49%\n",
      "Epoch [151/200] | Train Loss: 1.1121 (Cls: 0.4356, Reg: 0.9665) | Train Acc: 86.61% || Val Loss: 0.2449 (Cls: 0.0675, Reg: 0.2535) | Val Acc: 99.67%\n",
      "Epoch [152/200] | Train Loss: 1.1197 (Cls: 0.4323, Reg: 0.9819) | Train Acc: 87.01% || Val Loss: 0.2445 (Cls: 0.0612, Reg: 0.2618) | Val Acc: 98.94%\n",
      "Epoch [153/200] | Train Loss: 1.1005 (Cls: 0.4218, Reg: 0.9696) | Train Acc: 86.93% || Val Loss: 0.2291 (Cls: 0.0616, Reg: 0.2394) | Val Acc: 99.35%\n",
      "Epoch [154/200] | Train Loss: 1.1339 (Cls: 0.4412, Reg: 0.9896) | Train Acc: 86.72% || Val Loss: 0.2278 (Cls: 0.0610, Reg: 0.2382) | Val Acc: 99.41%\n",
      "Epoch [155/200] | Train Loss: 1.1353 (Cls: 0.4490, Reg: 0.9804) | Train Acc: 86.93% || Val Loss: 0.2991 (Cls: 0.0674, Reg: 0.3310) | Val Acc: 99.47%\n",
      "Epoch [156/200] | Train Loss: 1.1206 (Cls: 0.4396, Reg: 0.9728) | Train Acc: 86.77% || Val Loss: 0.2537 (Cls: 0.0622, Reg: 0.2737) | Val Acc: 99.22%\n",
      "Epoch [157/200] | Train Loss: 1.1235 (Cls: 0.4401, Reg: 0.9763) | Train Acc: 86.60% || Val Loss: 0.2345 (Cls: 0.0599, Reg: 0.2494) | Val Acc: 99.43%\n",
      "Epoch [158/200] | Train Loss: 1.1217 (Cls: 0.4435, Reg: 0.9688) | Train Acc: 86.66% || Val Loss: 0.2280 (Cls: 0.0596, Reg: 0.2406) | Val Acc: 99.57%\n",
      "Epoch [159/200] | Train Loss: 1.1368 (Cls: 0.4543, Reg: 0.9750) | Train Acc: 86.16% || Val Loss: 0.2294 (Cls: 0.0622, Reg: 0.2388) | Val Acc: 99.41%\n",
      "Epoch [160/200] | Train Loss: 1.1150 (Cls: 0.4341, Reg: 0.9728) | Train Acc: 86.96% || Val Loss: 0.2379 (Cls: 0.0623, Reg: 0.2508) | Val Acc: 99.53%\n",
      "Epoch [161/200] | Train Loss: 1.1295 (Cls: 0.4449, Reg: 0.9780) | Train Acc: 86.70% || Val Loss: 0.2330 (Cls: 0.0614, Reg: 0.2451) | Val Acc: 99.47%\n",
      "Epoch [162/200] | Train Loss: 1.1078 (Cls: 0.4321, Reg: 0.9653) | Train Acc: 87.13% || Val Loss: 0.2248 (Cls: 0.0616, Reg: 0.2332) | Val Acc: 99.45%\n",
      "Epoch [163/200] | Train Loss: 1.0879 (Cls: 0.4179, Reg: 0.9571) | Train Acc: 86.86% || Val Loss: 0.2259 (Cls: 0.0577, Reg: 0.2403) | Val Acc: 99.51%\n",
      "Epoch [164/200] | Train Loss: 1.1137 (Cls: 0.4373, Reg: 0.9663) | Train Acc: 86.71% || Val Loss: 0.2298 (Cls: 0.0587, Reg: 0.2444) | Val Acc: 99.51%\n",
      "Epoch [165/200] | Train Loss: 1.1216 (Cls: 0.4425, Reg: 0.9701) | Train Acc: 86.89% || Val Loss: 0.2288 (Cls: 0.0559, Reg: 0.2470) | Val Acc: 99.49%\n",
      "Epoch [166/200] | Train Loss: 1.1081 (Cls: 0.4291, Reg: 0.9701) | Train Acc: 86.73% || Val Loss: 0.2231 (Cls: 0.0575, Reg: 0.2365) | Val Acc: 99.47%\n",
      "Epoch [167/200] | Train Loss: 1.1130 (Cls: 0.4377, Reg: 0.9647) | Train Acc: 87.16% || Val Loss: 0.2649 (Cls: 0.0596, Reg: 0.2933) | Val Acc: 99.45%\n",
      "Epoch [168/200] | Train Loss: 1.0696 (Cls: 0.4114, Reg: 0.9404) | Train Acc: 87.63% || Val Loss: 0.2248 (Cls: 0.0570, Reg: 0.2397) | Val Acc: 99.57%\n",
      "Epoch [169/200] | Train Loss: 1.0723 (Cls: 0.4118, Reg: 0.9436) | Train Acc: 87.69% || Val Loss: 0.2274 (Cls: 0.0557, Reg: 0.2452) | Val Acc: 99.57%\n",
      "Epoch [170/200] | Train Loss: 1.0833 (Cls: 0.4196, Reg: 0.9482) | Train Acc: 87.42% || Val Loss: 0.2847 (Cls: 0.0684, Reg: 0.3091) | Val Acc: 99.33%\n",
      "Epoch [171/200] | Train Loss: 1.1066 (Cls: 0.4365, Reg: 0.9574) | Train Acc: 87.00% || Val Loss: 0.2215 (Cls: 0.0553, Reg: 0.2374) | Val Acc: 99.51%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2215) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [172/200] | Train Loss: 1.0794 (Cls: 0.4099, Reg: 0.9565) | Train Acc: 87.88% || Val Loss: 0.2083 (Cls: 0.0593, Reg: 0.2129) | Val Acc: 99.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2083) 至 ./models_save/rssi_csi_std.pth\n",
      "Epoch [173/200] | Train Loss: 1.0950 (Cls: 0.4234, Reg: 0.9594) | Train Acc: 87.69% || Val Loss: 0.2682 (Cls: 0.0640, Reg: 0.2917) | Val Acc: 99.43%\n",
      "Epoch [174/200] | Train Loss: 1.0995 (Cls: 0.4281, Reg: 0.9591) | Train Acc: 87.36% || Val Loss: 0.2454 (Cls: 0.0575, Reg: 0.2684) | Val Acc: 99.59%\n",
      "Epoch [175/200] | Train Loss: 1.0826 (Cls: 0.4162, Reg: 0.9520) | Train Acc: 87.31% || Val Loss: 0.2260 (Cls: 0.0588, Reg: 0.2389) | Val Acc: 99.59%\n",
      "Epoch [176/200] | Train Loss: 1.0676 (Cls: 0.4108, Reg: 0.9382) | Train Acc: 87.63% || Val Loss: 0.2239 (Cls: 0.0575, Reg: 0.2378) | Val Acc: 99.53%\n",
      "Epoch [177/200] | Train Loss: 1.0944 (Cls: 0.4212, Reg: 0.9618) | Train Acc: 87.69% || Val Loss: 0.2220 (Cls: 0.0570, Reg: 0.2356) | Val Acc: 99.63%\n",
      "Epoch [178/200] | Train Loss: 1.0681 (Cls: 0.4083, Reg: 0.9426) | Train Acc: 87.69% || Val Loss: 0.2167 (Cls: 0.0593, Reg: 0.2249) | Val Acc: 99.47%\n",
      "Epoch [179/200] | Train Loss: 1.0670 (Cls: 0.4036, Reg: 0.9476) | Train Acc: 87.49% || Val Loss: 0.2220 (Cls: 0.0590, Reg: 0.2329) | Val Acc: 99.57%\n",
      "Epoch [180/200] | Train Loss: 1.0829 (Cls: 0.4134, Reg: 0.9565) | Train Acc: 87.59% || Val Loss: 0.2573 (Cls: 0.0627, Reg: 0.2779) | Val Acc: 99.49%\n",
      "Epoch [181/200] | Train Loss: 1.0903 (Cls: 0.4205, Reg: 0.9569) | Train Acc: 87.52% || Val Loss: 0.2370 (Cls: 0.0622, Reg: 0.2498) | Val Acc: 99.45%\n",
      "Epoch [182/200] | Train Loss: 1.0676 (Cls: 0.4028, Reg: 0.9496) | Train Acc: 87.63% || Val Loss: 0.2127 (Cls: 0.0595, Reg: 0.2190) | Val Acc: 99.57%\n",
      "Epoch [183/200] | Train Loss: 1.0854 (Cls: 0.4171, Reg: 0.9547) | Train Acc: 87.85% || Val Loss: 0.2140 (Cls: 0.0561, Reg: 0.2256) | Val Acc: 99.57%\n",
      "Epoch [184/200] | Train Loss: 1.0786 (Cls: 0.4193, Reg: 0.9419) | Train Acc: 87.89% || Val Loss: 0.2544 (Cls: 0.0569, Reg: 0.2821) | Val Acc: 99.57%\n",
      "Epoch [185/200] | Train Loss: 1.0905 (Cls: 0.4222, Reg: 0.9547) | Train Acc: 87.57% || Val Loss: 0.2939 (Cls: 0.0649, Reg: 0.3271) | Val Acc: 99.20%\n",
      "Epoch [186/200] | Train Loss: 1.0822 (Cls: 0.4237, Reg: 0.9407) | Train Acc: 87.37% || Val Loss: 0.2723 (Cls: 0.0646, Reg: 0.2967) | Val Acc: 99.39%\n",
      "Epoch [187/200] | Train Loss: 1.0611 (Cls: 0.3988, Reg: 0.9461) | Train Acc: 88.30% || Val Loss: 0.2205 (Cls: 0.0564, Reg: 0.2344) | Val Acc: 99.61%\n",
      "Epoch [188/200] | Train Loss: 1.0645 (Cls: 0.4038, Reg: 0.9439) | Train Acc: 87.94% || Val Loss: 0.2514 (Cls: 0.0583, Reg: 0.2760) | Val Acc: 99.53%\n",
      "Epoch [189/200] | Train Loss: 1.0645 (Cls: 0.4103, Reg: 0.9346) | Train Acc: 87.32% || Val Loss: 0.2381 (Cls: 0.0574, Reg: 0.2582) | Val Acc: 99.55%\n",
      "Epoch [190/200] | Train Loss: 1.0611 (Cls: 0.4031, Reg: 0.9400) | Train Acc: 87.72% || Val Loss: 0.2195 (Cls: 0.0574, Reg: 0.2316) | Val Acc: 99.59%\n",
      "Epoch [191/200] | Train Loss: 1.0750 (Cls: 0.4155, Reg: 0.9421) | Train Acc: 87.87% || Val Loss: 0.2212 (Cls: 0.0594, Reg: 0.2313) | Val Acc: 99.51%\n",
      "Epoch [192/200] | Train Loss: 1.0806 (Cls: 0.4076, Reg: 0.9613) | Train Acc: 87.81% || Val Loss: 0.2252 (Cls: 0.0577, Reg: 0.2393) | Val Acc: 99.53%\n",
      "Epoch [193/200] | Train Loss: 1.0582 (Cls: 0.4039, Reg: 0.9347) | Train Acc: 88.24% || Val Loss: 0.2234 (Cls: 0.0579, Reg: 0.2364) | Val Acc: 99.61%\n",
      "Epoch [194/200] | Train Loss: 1.0612 (Cls: 0.4095, Reg: 0.9311) | Train Acc: 87.56% || Val Loss: 0.2192 (Cls: 0.0584, Reg: 0.2298) | Val Acc: 99.57%\n",
      "Epoch [195/200] | Train Loss: 1.0550 (Cls: 0.4010, Reg: 0.9342) | Train Acc: 87.84% || Val Loss: 0.2215 (Cls: 0.0575, Reg: 0.2343) | Val Acc: 99.55%\n",
      "Epoch [196/200] | Train Loss: 1.0738 (Cls: 0.4164, Reg: 0.9392) | Train Acc: 87.80% || Val Loss: 0.2450 (Cls: 0.0590, Reg: 0.2656) | Val Acc: 99.55%\n",
      "Epoch [197/200] | Train Loss: 1.0698 (Cls: 0.4039, Reg: 0.9512) | Train Acc: 87.76% || Val Loss: 0.2324 (Cls: 0.0572, Reg: 0.2502) | Val Acc: 99.59%\n",
      "Epoch [198/200] | Train Loss: 1.0701 (Cls: 0.4119, Reg: 0.9403) | Train Acc: 87.83% || Val Loss: 0.2511 (Cls: 0.0589, Reg: 0.2745) | Val Acc: 99.61%\n",
      "Epoch [199/200] | Train Loss: 1.0451 (Cls: 0.3976, Reg: 0.9250) | Train Acc: 87.83% || Val Loss: 0.2180 (Cls: 0.0576, Reg: 0.2292) | Val Acc: 99.57%\n",
      "Epoch [200/200] | Train Loss: 1.0639 (Cls: 0.4070, Reg: 0.9383) | Train Acc: 87.60% || Val Loss: 0.2170 (Cls: 0.0567, Reg: 0.2290) | Val Acc: 99.61%\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "alpha = 0.7  # 回歸損失權重，可根據需要調整\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "# -----------------------\n",
    "# COORDINATES 字典：將分類標籤轉為 (X, Y) 座標\n",
    "# -----------------------\n",
    "COORDINATES = {\n",
    "    # 下邊界 (1-10 和 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    # 左邊界 (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    # 上邊界 (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    # 右邊界 (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    # 中間點 (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        # 將 0-index 轉換成 1-index (例如 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 訓練參數與 Early Stopping 設定\n",
    "# -----------------------\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_std.pth\"\n",
    "epochs = 200\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "train_losses1 = []\n",
    "val_losses1 = []\n",
    "train_accuracies1 = []\n",
    "val_accuracies1 = []\n",
    "\n",
    "# -----------------------\n",
    "# 訓練迴圈 (分類與回歸雙輸出)\n",
    "# -----------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_class_loss = 0.0\n",
    "    train_reg_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 每個 batch 返回 (amp_inputs, rssi_inputs, labels)\n",
    "    # 其中 labels 為 one-hot 編碼 (用以計算分類損失)\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # 分類目標：one-hot -> class index\n",
    "        target_class = torch.argmax(labels, dim=1)\n",
    "        loss_class = criterion(class_out, target_class)\n",
    "        \n",
    "        # 回歸目標：根據 target_class 透過 COORDINATES 字典取得 (X, Y) 座標\n",
    "        true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "        loss_reg = criterion_reg(reg_out, true_coords)\n",
    "        \n",
    "        loss = loss_class + alpha * loss_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size_curr = amp_inputs.size(0)\n",
    "        train_loss += loss.item() * batch_size_curr\n",
    "        train_class_loss += loss_class.item() * batch_size_curr\n",
    "        train_reg_loss += loss_reg.item() * batch_size_curr\n",
    "        _, predicted = torch.max(class_out, 1)\n",
    "        total_train += batch_size_curr\n",
    "        train_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_class_loss = train_class_loss / len(train_loader.dataset)\n",
    "    avg_train_reg_loss = train_reg_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_class_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            loss_class = criterion(class_out, target_class)\n",
    "            true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "            loss_reg = criterion_reg(reg_out, true_coords)\n",
    "            loss = loss_class + alpha * loss_reg\n",
    "            \n",
    "            batch_size_curr = amp_inputs.size(0)\n",
    "            val_loss += loss.item() * batch_size_curr\n",
    "            val_class_loss += loss_class.item() * batch_size_curr\n",
    "            val_reg_loss += loss_reg.item() * batch_size_curr\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total_val += batch_size_curr\n",
    "            val_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_class_loss = val_class_loss / len(val_loader.dataset)\n",
    "    avg_val_reg_loss = val_reg_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    train_losses1.append(avg_train_loss)\n",
    "    val_losses1.append(avg_val_loss)\n",
    "    train_accuracies1.append(train_acc)\n",
    "    val_accuracies1.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_class_loss:.4f}, Reg: {avg_train_reg_loss:.4f}) | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% || \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} (Cls: {avg_val_class_loss:.4f}, Reg: {avg_val_reg_loss:.4f}) | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    # if counter >= patience:\n",
    "    #     print(f\"Early stop at epoch {epoch+1}\")\n",
    "    #     break\n",
    "\n",
    "print(\"訓練完成！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 測試損失: 0.0703, 測試準確率: 99.43%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "\n",
    "# 載入最佳模型\n",
    "model.load_state_dict(torch.load(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN/models_save/rssi_csi_no_ppstd_01.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 測試模型\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_reg = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for amp_inputs, rssi_input, labels in test_loader:\n",
    "        amp_inputs, rssi_inputs, labels = amp_inputs.to(device), rssi_input.to(device), labels.to(device)\n",
    "        outputs, reg = model(amp_inputs, rssi_inputs)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "        test_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "        # 儲存真實標籤與預測標籤\n",
    "        all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_reg.extend(reg.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"📊 測試損失: {test_loss:.4f}, 測試準確率: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance error: 0.018898624654228723\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adjusted_predictions = np.array(all_predictions) + 1\n",
    "adjusted_labels = np.array(all_labels) + 1\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # 取出對應的座標\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # 計算歐氏距離\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) , errors\n",
    "\n",
    "mean_error, errors = compute_mean_distance_error(adjusted_labels, adjusted_predictions, COORDINATES)\n",
    "print(\"Mean distance error:\", mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOSS & ACCURACY CURVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk4AAAGGCAYAAACNCg6xAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAmt9JREFUeJzsnXd8FGX+gJ+Zrem9koQkEDoigiJFmggi2AsqdwJ63tkLttO7U/FU7vROsR2Wn6eep2fBXlARAQsgiKKAlBASICEJhCSbZJNsm/n9seySJclmAwm72XmfzycfyLuzM99n3snmm7dKqqqqCAQCgUAgEAg6RA52AAKBQCAQCAQ9BZE4CQQCgUAgEASISJwEAoFAIBAIAkQkTgKBQCAQCAQBIhIngUAgEAgEggARiZNAIBAIBAJBgIjESSAQCAQCgSBAROIkEAgEAoFAECAicRIIBAKBQCAIEJE4CQTHmZKSEiRJ4uWXX/aW3X///UiSFND7JUni/vvv79KYJk6cyMSJE7v0nILwozuePYGgpyESJ4HAD+eccw6RkZHU19e3e8zs2bMxGo0cPHjwOEbWeX799Vfuv/9+SkpKgh2Kl5UrVyJJEkuWLAl2KAFRVFTEH/7wB/Lz8zGbzcTGxjJ27FieeOIJmpqagh2eQCA4DojESSDww+zZs2lqauK9995r8/XGxkY++OADzjzzTJKSko76On/+85+7/Rfvr7/+yoIFC9pMnL744gu++OKLbr1+T+eTTz5h6NChvPXWW5x99tk89dRTLFy4kJycHO644w5uvvnmYIfY7TQ1NfHnP/852GEIBEFFH+wABIJQ5pxzziEmJobXX3+dK664otXrH3zwAVarldmzZx/TdfR6PXp98H4cjUZj0K7dEyguLubSSy+ld+/efPXVV2RkZHhfu/7669m5cyeffPJJECPsPhRFwW63YzabMZvNwQ5HIAg6osVJIPBDREQEF1xwAcuXL2f//v2tXn/99deJiYnhnHPOobq6mttvv52hQ4cSHR1NbGws06dP5+eff+7wOm2NcbLZbNx6662kpKR4r1FaWtrqvbt37+a6666jf//+REREkJSUxMUXX+zTsvTyyy9z8cUXAzBp0iQkSUKSJFauXAm0PcZp//79XHXVVaSlpWE2mxk2bBivvPKKzzGe8Vr/+Mc/eP755+nTpw8mk4mTTz6Z9evXd+gdKLt27eLiiy8mMTGRyMhITj311DYTlaeeeorBgwcTGRlJQkICI0eO5PXXX/e+Xl9fzy233EJubi4mk4nU1FTOOOMMfvzxR7/Xf+SRR2hoaODFF1/0SZo89O3b16fFyel08te//tV7P3Jzc7nnnnuw2Ww+78vNzWXmzJmsXLmSkSNHEhERwdChQ7318u677zJ06FDMZjMjRozgp59+8nn/3LlziY6OZteuXUybNo2oqCgyMzN54IEHUFXV59h//OMfjBkzhqSkJCIiIhgxYkSbXaSSJHHDDTfw2muvMXjwYEwmE5999pn3tZZjnAK9n2+//TYjRowgIiKC5ORkfvOb31BWVtamS1lZGeeddx7R0dGkpKRw++2343K52qkZgeD4IxIngaADZs+ejdPp5K233vIpr66u5vPPP+f8888nIiKCXbt28f777zNz5kwee+wx7rjjDjZt2sSECRPYt29fp6/7u9/9jkWLFjF16lT+9re/YTAYmDFjRqvj1q9fz+rVq7n00kt58sknueaaa1i+fDkTJ06ksbERgPHjx3PTTTcBcM899/Dqq6/y6quvMnDgwDav3dTUxMSJE3n11VeZPXs2jz76KHFxccydO5cnnnii1fGvv/46jz76KH/4wx948MEHKSkp4YILLsDhcHTa+0gqKysZM2YMn3/+Oddddx0PPfQQzc3NnHPOOT5dqC+88AI33XQTgwYNYtGiRSxYsIATTzyR77//3nvMNddcw+LFi7nwwgv517/+xe23305ERARbt271G8NHH31Efn4+Y8aMCSjm3/3ud9x7772cdNJJPP7440yYMIGFCxdy6aWXtjp2586dXH755Zx99tksXLiQmpoazj77bF577TVuvfVWfvOb37BgwQKKioq45JJLUBTF5/0ul4szzzyTtLQ0HnnkEUaMGMF9993Hfffd53PcE088wfDhw3nggQd4+OGH0ev1XHzxxW0moF999RW33nors2bN4oknniA3N7dNz0Du58svv8wll1yCTqdj4cKFXH311bz77ruMGzeO2traVi7Tpk0jKSmJf/zjH0yYMIF//vOfPP/88wHdd4HguKAKBAK/OJ1ONSMjQx09erRP+bPPPqsC6ueff66qqqo2NzerLpfL55ji4mLVZDKpDzzwgE8ZoL700kvesvvuu09t+eO4ceNGFVCvu+46n/NdfvnlKqDed9993rLGxsZWMa9Zs0YF1P/85z/esrffflsF1BUrVrQ6fsKECeqECRO83y9atEgF1P/+97/eMrvdro4ePVqNjo5W6+rqfFySkpLU6upq77EffPCBCqgfffRRq2u1ZMWKFSqgvv322+0ec8stt6iA+s0333jL6uvr1by8PDU3N9d7z88991x18ODBfq8XFxenXn/99X6PORKLxaIC6rnnnhvQ8Z66+93vfudTfvvtt6uA+tVXX3nLevfurQLq6tWrvWWff/65CqgRERHq7t27veXPPfdcq/qbM2eOCqg33nijt0xRFHXGjBmq0WhUDxw44C0/8jmx2+3qkCFD1MmTJ/uUA6osy+qWLVtauR357HV0P+12u5qamqoOGTJEbWpq8pZ//PHHKqDee++9rVxa/qyoqqoOHz5cHTFiRLvXEAiON6LFSSDoAJ1Ox6WXXsqaNWt8ur9ef/110tLSOP300wEwmUzIsvtHyuVycfDgQaKjo+nfv3+HXUFH8umnnwJ4W4k83HLLLa2OjYiI8P7f4XBw8OBB+vbtS3x8fKev2/L66enpXHbZZd4yg8HATTfdRENDA6tWrfI5ftasWSQkJHi/P+200wB3F9ux8umnn3LKKacwbtw4b1l0dDS///3vKSkp4ddffwUgPj6e0tJSv12E8fHxfP/9951qAayrqwMgJiYm4HgB5s+f71N+2223AbRq4Rk0aBCjR4/2fj9q1CgAJk+eTE5OTqvytu7pDTfc4P2/p6vNbrfz5ZdfestbPic1NTVYLBZOO+20Np+RCRMmMGjQoA5MO76fP/zwA/v37+e6667zGR81Y8YMBgwY0GZr1zXXXOPz/WmnndYlz5FA0FWIxEkgCADP4G/PeJnS0lK++eYbLr30UnQ6HeAeRPv4449TUFCAyWQiOTmZlJQUfvnlFywWS6eut3v3bmRZpk+fPj7l/fv3b3VsU1MT9957L9nZ2T7Xra2t7fR1W16/oKDAmwh68HTt7d6926e85S94wJtE1dTUHNX1j4ylLe8jY7nrrruIjo7mlFNOoaCggOuvv57vvvvO5z2PPPIImzdvJjs7m1NOOYX777+/w1/KsbGxAH6XpDgyXlmW6du3r095eno68fHxHd67uLg4ALKzs9ssP/KeyrJMfn6+T1m/fv0AfBL9jz/+mFNPPRWz2UxiYiIpKSksXry4zWckLy+vI02g4/vpcW2r/gYMGNDqXpjNZlJSUnzKEhISuuQ5Egi6CpE4CQQBMGLECAYMGMD//vc/AP73v/+hqqrPbLqHH36Y+fPnM378eP773//y+eefs2zZMgYPHtxqXEpXcuONN/LQQw9xySWX8NZbb/HFF1+wbNkykpKSuvW6LfEkj0eiHjFAuTsZOHAg27dv54033mDcuHG88847jBs3zmeszyWXXMKuXbt46qmnyMzM5NFHH2Xw4MEsXbq03fPGxsaSmZnJ5s2bOxVPoAuatnfvuvKefvPNN5xzzjmYzWb+9a9/8emnn7Js2TIuv/zyNs/XsnXKH0dzP/3RnrNAEEqIxEkgCJDZs2ezefNmfvnlF15//XUKCgo4+eSTva8vWbKESZMm8eKLL3LppZcydepUpkyZ0moAbCD07t0bRVEoKiryKd++fXurY5csWcKcOXP45z//yUUXXcQZZ5zR5sDbQH+Re65fWFjYKvHatm2b9/XjRe/evdv0biuWqKgoZs2axUsvvcSePXuYMWOGdzC5h4yMDK677jref/99iouLSUpK4qGHHvIbw8yZMykqKmLNmjUBxasoCoWFhT7llZWV1NbWdvm9UxSlVavZjh07ALyDut955x3MZjOff/45V155JdOnT2fKlCldcn1/99Pj2lb9bd++/bg+RwJBVyESJ4EgQDytS/feey8bN25stXaTTqdr9df722+/3WradSBMnz4dgCeffNKnfNGiRa2Obeu6Tz31VKsp3FFRUQABJXJnnXUWFRUVvPnmm94yp9PJU089RXR0NBMmTAhEo0s466yzWLdunU/SYrVaef7558nNzfWOxTly5Xaj0cigQYNQVRWHw4HL5WrVLZWamkpmZmarZQKO5M477yQqKorf/e53VFZWtnq9qKjIO9vwrLPOAlrX1WOPPQbQ5szIY+Xpp5/2/l9VVZ5++mkMBoN3/J1Op0OSJJ9noqSkhPfff/+orxnI/Rw5ciSpqak8++yzPvd46dKlbN26tVvuhUDQ3YgFMAWCAMnLy2PMmDF88MEHAK0Sp5kzZ/LAAw8wb948xowZw6ZNm3jttddajT8JhBNPPJHLLruMf/3rX1gsFsaMGcPy5cvZuXNnq2NnzpzJq6++SlxcHIMGDWLNmjV8+eWXrVYyP/HEE9HpdPz973/HYrFgMpmYPHkyqamprc75+9//nueee465c+eyYcMGcnNzWbJkCd999x2LFi0KeKB0oLzzzjveFqSWzJkzhz/+8Y/873//Y/r06dx0000kJibyyiuvUFxczDvvvOMdhzV16lTS09MZO3YsaWlpbN26laeffpoZM2YQExNDbW0tWVlZXHTRRQwbNozo6Gi+/PJL1q9fzz//+U+/8fXp04fXX3+dWbNmMXDgQK644gqGDBmC3W5n9erVvP3228ydOxeAYcOGMWfOHJ5//nlqa2uZMGEC69at45VXXuG8885j0qRJXXrvzGYzn332GXPmzGHUqFEsXbqUTz75hHvuucc7XmjGjBk89thjnHnmmVx++eXs37+fZ555hr59+/LLL78c1XXr6+s7vJ8Gg4G///3vzJs3jwkTJnDZZZdRWVnpXeLg1ltv7bL7IBAcN4I3oU8g6Hk888wzKqCecsoprV5rbm5Wb7vtNjUjI0ONiIhQx44dq65Zs6bVVP9AliNQVVVtampSb7rpJjUpKUmNiopSzz77bHXv3r2tpoTX1NSo8+bNU5OTk9Xo6Gh12rRp6rZt29TevXurc+bM8TnnCy+8oObn56s6nc5navuRMaqqqlZWVnrPazQa1aFDh/rE3NLl0UcfbXU/joyzLTzLEbT35VmCoKioSL3ooovU+Ph41Ww2q6eccor68ccf+5zrueeeU8ePH68mJSWpJpNJ7dOnj3rHHXeoFotFVVVVtdls6h133KEOGzZMjYmJUaOiotRhw4ap//rXv/zG2JIdO3aoV199tZqbm6sajUY1JiZGHTt2rPrUU0+pzc3N3uMcDoe6YMECNS8vTzUYDGp2drZ69913+xyjqu7lCGbMmNHmvTtymn9b93rOnDlqVFSUWlRUpE6dOlWNjIxU09LS1Pvuu6/V0hgvvviiWlBQoJpMJnXAgAHqSy+91OZz19a1W77mqdPO3M8333xTHT58uGoymdTExER19uzZamlpqc8xHpcjaStGgSCYSKp6HEdvCgQCgaDLmDt3LkuWLKGhoSHYoQgEmkGMcRIIBAKBQCAIEJE4CQQCgUAgEASISJwEAoFAIBAIAkSMcRIIBAKBQCAIENHiJBAIBAKBQBAgInESCAQCgUAgCJAevQCmoijs27ePmJiYTm0nIRAIBAKBQOBBVVXq6+vJzMxstbn5kfToxGnfvn2tdhAXCAQCgUAgOBr27t1LVlaW32N6dOLk2fZh7969xMbGdvn5XS4XxcXF5OXlaWbXbuEsnMMRrfmCcBbO4Ul3+dbV1ZGdnR3QdlI9OnHydM/FxsZ2W+IUGRlJbGysJh5IEM7COTzRmi8IZ+EcnnS3byDDfsTgcIFAIBAIBIIAEYmTQCAQCAQCQYD06AUw6+rqiIuLw2KxdEtXnaqq2O12jEajZmbtCWfhHI5ozReEs3AOT7rLtzP5RI8e43Q80Ou1d4uEszbQmrPWfKFnOLtcLhwOR5ecS1VVFEVBURRNJBGgPeej9TUYDF02Jir0f6qCiKIoFBYWUlBQoIlBdyCchXN4ojVfCH1nVVWpqKigtra2S8/pdDrR6/WaSCJAe87H4hsfH096evox3yeROAkEAoHguONJmlJTU4mMjOySX/qqqmKz2TCZTJpIIkB7zkfjq6oqjY2N7N+/H4CMjIxjikEkTgKBQCA4rrhcLm/SlJSU1GXn9QzZNZvNmkgiQHvOR+sbEREBwP79+0lNTT2mVlgxq04gEAgExxXPmKbIyMggRyLQEp7n7VjH1InEyQ+yLFNQUNDhvjXhhHDWBlpz1pov9Azn7mghMZvNXX7OUEdrzkfr21XPW9B/osrKyvjNb35DUlISERERDB06lB9++CHYYXlxOp3BDuG4I5y1gdacteYL2nTuwSvsHDVacw62b1ATp5qaGsaOHYvBYGDp0qX8+uuv/POf/yQhISGYYXlRFIXi4mIURQl2KMcN4awNtOasNV/QpjOAzWYLdgg+rFy5EkmSOpw9mJuby6JFi47qGqHm3N0E2zeoidPf//53srOzeemllzjllFPIy8tj6tSp9OnTJ5hhebn5/S3MeLuEdzeVBzsUgUAgEASRZ599lpiYGJ9WvIaGBgwGAxMnTvQ51pMsFRUVMWbMGMrLy4mLiwPg5ZdfJj4+/jhG7kugCVpubi6SJCFJElFRUZx00km8/fbb3tfvv/9+7+t6vZ7c3FxuvfVWGhoa2j1nY2Mjd999N3369MFsNpOSksKECRP44IMPKCkp8Z6vva+XX36ZlStXEhkZiSzLyLJMXFwcw4cP584776S8/Pj8rg5q4vThhx8ycuRILr74YlJTUxk+fDgvvPBCMEPyoayumV21Dg402IMdikAgEAiCyKRJk2hoaPAZSvLNN9+Qnp7O999/T3Nzs7d8xYoV5OTk0KdPH4xGY5esHRQMHnjgAcrLy/npp584+eSTmTVrFqtXr/a+PnjwYMrLyykpKeHvf/87zz//PLfddlu757vmmmt49913eeqpp9i2bRufffYZF110EQcPHiQ7O5vy8nLv12233eY9v+dr1qxZ3nNt27aNffv2sX79eu666y6+/PJLhgwZwqZNm7r1nkCQE6ddu3axePFiCgoK+Pzzz7n22mu56aabeOWVV9o83mazUVdX5/MF7qmtni9Ps7SiKAGVe/pK2yo36dy3p8nhW66qqs+x/sqBVuWeWNorDzT2o3E68n61FbvnmHBy6qhcluWwc+qonlpeI1yc/MXouX44OXVU7vllHYpOnvd6rtny+2MplyQp4OM789W/f38yMjJYsWKFt2zFihWce+655OXlsWbNGm/5ypUrmTRpkvcYSZKoqalhxYoVzJs3D4vF4m1Fue+++7wxWq1W5s2bR0xMDDk5OTz33HM+sf/yyy9MnjyZiIgIkpKSuPrqq6mvr/ceM3HiRG6++WafuM877zzmzp3rfX337t3ceuut3uu3dx8BoqOjSUtLo6CggKeffpqIiAg+/PBD73F6vZ60tDSysrK45JJLuPzyy31eP/K+f/jhh9x9992cddZZ9O7dm5NOOokbbriBefPmIcsy6enppKWlkZaWRlRUlPf8ni+z2ew9b2pqqje2WbNm8e2335KSksK1117b4TPj7zMiEIK6jpOiKIwcOZKHH34YgOHDh7N582aeffZZ5syZ0+r4hQsXsmDBglblRUVFREdHAxAXF0dGRgaVlZVYLBbvMcnJySQnJ1NWVobVavWWp6enEx8fT0lJCXb74ZalrKwsIozu27Ov8gCFhe6bmpeXh16vp7Cw0CeGgoICnE4nxcXF3jJZlunXrx9Wq5XS0lJvudFoJD8/H4vFQkVFhbc8KiqK7Oxsqqurqaqq8pZ3pVN0dDRFRUU+4x7actq1a1fYOYH/empoaAg7p/bqydOcvmvXrrBxCqSedu3aFXZO4L+edDode/fuDSmnffv24XQ6sdls6HQ6jEYjTqcTS+PhsSs6nYzRYMTusONyKegkMBt06PV6DAYDtdYmXK7DMRoMevQ6PTaHi0Z7IxEGnfe6zU6VZpvNJykwmYxEmww+LUWA95dzy3E0kiRhNpuZOHEiX331Fbfccgvgblm66667cDgcLFu2jFNPPZWmpia+//57rrzySpxOp/f+Njc3c/LJJ7No0SLuvfdeNm7cCLiTE0/332OPPcZf/vIXbrvtNt577z2uu+46Jk6cSG5uLvX19UybNo1Ro0axdu1aDh48yFVXXcX111/P888/7xNvS6eWiexrr73GqFGjuPLKK7nyyisxm80oiuLzDMiyjMlkAtyTCzzn0ul0GAwGbDYbzc3NOJ1OFEXB4XBgNBq9/9rtdpqbm7315HA4vDGkpaXx6aefcuGFF2I0Gn2eMaPRiE6nw3aonjznV1UVSZK8cbRcSqBlPUmSxFVXXcWdd95JRUWFT3eox8nlcuF0Otm9ezeyLPv8PJWUlBAoQU2cMjIyGDRokE/ZwIEDeeedd9o8/u6772b+/Pne7+vq6sjOzqZPnz7eTfk8f2GlpaWRmprqPdZT3qtXL58fHs9U3dzc3FblRp37PTHxCRQUFPgc7/ne53ijsVU5uD/sWpZ7YomLiyMmJqZVeWJios8A+a50AlqNIWvppKruFVY9fcjh4HRk+ZFOqqrS0NBAZGRk2Dh5aK+eYmNjkWXZu2JzODj5q6f4+Hif5zocnDqqJ8/PsqqqIedkMBjYvXs3JpMJg8EAuPfVS3ngs1bn8XDWgFQ+/t0p3u+zHlxJo6PtVoIJ+YmsuG6M9/vch76gytp6yIXyj5ltTm33JEpHMmnSJG699Vb0ej1NTU1s3LiRCRMmYLfbee655zCbzaxevRqbzcakSZPQ6/UYjUbAnZBFRUURFxeHJEnk5ua2djzrLG6++WYABg0axNNPP82KFSv4wx/+wLvvvovNZuO///0vUVFRSJLE008/zTnnnMPf//53n9WwW8buWehRkiQyMzPR6/UkJCTQu3dvb4tTe9P79Xo9ZrMZu93OP/7xDywWC6effjpmsxm9Xo8sy976++WXX3jrrbeYPHmyz/kMBoP3mOeff947i37YsGGMHTuWiy66iLFjx3qP9yRtnvN7nhvPOT33s616GjJkCAB79uwhPT29lY9O5068e/fu7bOAZmJiYqcWxAxq4jR27Fi2b9/uU7Zjxw569+7d5vEmk8l7U1ui0+laSbe3dklnyj1ddTaX2ur87d3ktsolSepUeVfE7q/cX+wul4t9+/ZRUFDgfah6ulNH5S6Xi9LS0nb39OqJTh7ai11VVW89t3y9Jzv5q6eWvp7YerpTRzG2/FluL5ZgOrUc9Osp94t0xDEdHR7AmKLOjjuaNGkSVquVH374gZqaGvr160dKSgoTJ07kyiuvxGazsWrVKvLz88nJyfG5RkvX9q59wgkn+Byfnp7O/v37kSSJbdu2MWzYMG/vCsC4ceNQFIUtW7Z4E6cjr9PW9QK973/84x/5y1/+QnNzM9HR0fztb39j5syZ3vdt2rSJmJgYXC4XdrudGTNm8PTTT7N3716fRpF77rmHe+65hwkTJrBr1y7Wrl3L6tWrWb58OePHj2fBggX85S9/aXW/OoqvvddaJlxHHu95/o783OsxidOtt97KmDFjePjhh7nkkktYt24dzz//PM8//3www/JiMrg/VJod2prOKxAIBMGg4eHp7b6mk31/Ee6/f2qrY1TV3SUXGeHbglLyp9O7JL6+ffuSlZXFihUrqKmpYcKECQBkZmaSnZ3N6tWrWbFiBZMnTz6q83taZjxIktSp5SQ8fyS05FhWyb7jjjuYO3eud6zTkclI//79+fDDD9Hr9WRmZnpbg5xOp7crEtwtOh4MBgOnnXYap512GnfddRcPPvggDzzwAHfddZdPa9LRsHXrVoA2W/O6kqAmTieffDLvvfced999Nw888AB5eXksWrSI2bNnBzMsLylRRjKj9cSZxZZ+AoFA0N1EmQL/rG3rWFVV0alOzAZdh8ceLZMmTWLlypXU1NRwxx13eMvHjx/P0qVLWbduHddee2277zcajZ0aiOxh4MCBvPzyy1itVqKiogD47rvvvCvEA6SkpPhMyXe5XGzevJlJkyYd1fWTk5Pp27evX5e2Xtfr9X7f15JBgwZ5x1IdS+LU1NTE888/z/jx40lJSTnq8wRC0FcOnzlzJps2baK5uZmtW7dy9dVXBzskL7dP7MM3c/pz79R+wQ7luCFJEkajsUdOnT1ahHP4ozVf0KYztN/92FVMmjSJb7/91ju+ycOECRN47rnnsNvtPonKkeTm5tLQ0MDy5cupqqqisbExoOvOnj0bs9nMnDlz2Lx5MytWrODGG2/kt7/9rbebbvLkyXzyySd88sknbNu2jWuvvbbVwpu5ubl8/fXXlJWV+UwwOB5MnDiR5557jg0bNlBSUsKnn37KPffcw6RJk7zjlANl//79VFRUUFhYyBtvvMHYsWOpqqpi8eLF3RT9YYKeOIUysiyTn58f0ns9dTXCWRtozVlrvqBNZ0mSMJlM3ZosTpo0iaamJvr27UtaWpq3fMKECdTX13uXLWiPMWPGcM011zBr1ixSUlJ45JFHArpuZGQkn3/+OdXV1Zx88slcdNFFnH766Tz99NNe5yuvvJI5c+ZwxRVXMGHCBPLz81slcQ888AAlJSX06dOn21tmjmTatGm88sorTJ06lYEDB3LjjTcybdo03nrrrYDP4anbAQMGkJmZyYgRI/jb3/7GlClT2Lx5c6sJZ92BpB7ZIdqDqKurIy4uDovF0ulsNRBUVcVisXhnQWgB4SycwxGt+UJoOzc3N1NcXExeXl6XblDrWaPHM/hcC2jN+Vh8/T13nckntPOnyFHwweZyTlu8jpvf2xzsUI4biqJQUVGhqf2thHP4ozVf0KYzHNtg6J6K1pyD7StGPfuhtsnB5iobmYnt770jEAgEAoFAO4gWJz+Y9e6ZGc1Obf3FJhAIBAKBoG1E4uQHk959e+waSpw8O2Froa/cg3AOf7TmC9p0hvYX6QxntOYcbF/RVecHz151Npd2EidZlsnOzg52GMcV4Rz+aM0XtOnsWYJBS2jNORR8RYuTHw4tHK6plcMVRaGqqkpTA0qFc/ijNV/QprOqqjgcjlarZ4czWnMOBV+ROPnB01Vn01BXnaqqVFVVaeaHEISzFtCaL2jTGdzbfWgNrTkH21ckTn6INOiIN8kkRIgeTYFAIBAIBGKMk1+GZcay5oo+3n2ABAKBQCAQaBvR4uQHSZJCctXd7kQ4awOtOWvNF7TpDMGfcXUkK1euRJKkVnvGHUlubi6LFi06qmuEmnN3E2xfkTj5QZZlMjIyNLXXk3DWBlpz1povaNO5Ozc2fvbZZ4mJifEZX9PQ0IDBYGDixIk+x3qSpaKiIsaMGUN5eTlxcXEAvPzyy8THx3dJTHPnzkWWZUwmEyaTib59+/LAAw94Y/TE4flKS0vjwgsvZNeuXX7P+8ILLzBs2DCio6OJj49n+PDhLFy4EHAneC3PeeTX3LlzAXzKoqKiKCgoYO7cuWzYsOGYnENh82rt/EQdBTVWG2MXrWTMk9+gKNoYYKkoCuXl5ZqaiSOcwx+t+YI2nVVVxW63d8uA+EmTJtHQ0MAPP/zgLfvmm29IT0/n+++/p7m52Vu+YsUKcnJy6NOnD0ajkfT09G77RX/mmWeye/duduzYwW233cb999/Po48+6nPM9u3b2bdvH2+//TZbtmzh7LPPxuVytXm+f//739xyyy3cdNNNbNy4ke+++44777yThgb3Dhrr16+nvLyc8vJy3nnnHe/5PWVPPPGE91wvvfQS5eXlbNmyhWeeeYaGhgZGjRrFf/7zn6P27c46DhSROPlFZfXeetbsrtXMWk6ejUG1NBNHOIc/WvMFbToD7SYEx0r//v3JyMhg5cqV3rKVK1dy7rnnkpeXx9q1a33KJ02a5P2/p6tu5cqVzJs3D4vF4m2Nuf/++73va2xs5MorryQmJoacnByef/75DuMymUykpKTQu3dvrr32WqZMmcKHH37oc0xqaioZGRmMHz+ee++9l19//ZWdO3e2eb4PP/yQSy65hKuuuoq+ffsyePBgLrvsMh566CEAUlJSSE9PJz09ncTERO/5PWWeljWA+Ph40tPTyc3NZerUqSxZsoTZs2dzww03UFNT06Fbe3RXHQeKSJz84NlyBbS1JIFAIBAcT1RVRbFZg/LVmcRy0qRJrFixwvv9ihUrmDhxIhMmTPCWNzU18f3333sTp5aMGTOGRYsWERsb622huf32272v//Of/2TkyJH89NNPXHfddVx77bVs3769U/cyIiICu93u93Wg3WPS09NZu3Ytu3fv7tR1A+XWW2+lvr6eZcuWdcv5jwdiVp0fDLrDTavNDhdEGIIYjUAgEIQnqr2Rbb+PDsq1BzzfgGSKCujYSZMmccstt+B0OmlqauKnn35iwoQJOBwOnn32WQDWrFmDzWZrM3EyGo3eAfvp6emtXj/rrLO47rrrALjrrrt4/PHHWbFiBf379+8wNlVVWb58OZ9//jk33nhjm8eUl5fzj3/8g169erV7zvvuu48LLriA3Nxc+vXrx+jRoznrrLO46KKLumS83IABAwAoKSk55nMFC9Hi5AdZljEdSp600uIkSRLJycmamokjnMMfrfmCNp27m4kTJ2K1Wlm/fj3ffPMN/fr1IyUlhQkTJnjHOa1cuZL8/HxycnI6ff4TTjjB+39PcrV//36/7/n4449JSUkhIiKC6dOnM2vWLJ/uP4CsrCyioqLIzMzEarXyzjvvYDQaGTx4MNHR0URHRzN9+nQAMjIyWLNmDZs2beLmm2/G6XQyZ84czjzzzC4ZL+dp4TuW51KvD26bj2hx8oMsy5gNOmwup2bGOMmyTHJycrDDOK4I5/BHa77Qs5wlYyQDnm8I2rUDpW/fvmRlZbFixQpqamqYMGECAJmZmWRnZ7N69WpWrFjB5MmTjyoWg8G3V0OSpA6TlUmTJrF48WKMRiOZmZltJhXffPMNsbGxpKamEhMT4y3/9NNPcTgcwOEuPA9DhgxhyJAhXHfddVxzzTWcdtpprFq1qs2WtM6wdetWAPLy8o7q/ZIktbpPxxuROPlBURTN7VenKAplZWX06tVLM9OYhXP4O2vNF3qWsyRJAXeX+cOzj5nBYOi2lrZJkyaxcuVKampquOOOO7zl48ePZ+nSpaxbt45rr7223fcbjcYuHdwcFRVFTk6OX+e8vLw2l0Do3bt3QNcYNGgQAFar9ajj9OAZ4zVlypSjev/xqOOOEImTH1RVJdog4TDrcWlkOQJVVbFaOzdgsqcjnMMfrfmCNp3BPeOqO1skJk2axPXXX4/D4fC2OAFMmDCBG264Abvd7rdVJjc3l4aGBpYvX86wYcOIjIwkMjLwVq+26Erna6+9lszMTCZPnkxWVhbl5eU8+OCDpKSkMHr06E6dq7a2loqKCmw2Gzt27OC5557j/fff5z//+c8xrWXV3XXcEaH9Z0iQqXz1Bj6pu5ndF9gZnhXX8RsEAoFAENZMmjSJpqYm+vbtS1pamrd8woQJ1NfXe5ctaI8xY8ZwzTXXMGvWLFJSUnjkkUeOR9gBM2XKFNauXcvFF19Mv379uPDCCzGbzSxfvpykpKROnWvevHlkZGQwYMAArr32WqKjo1m3bh2XX355N0V/fBAtTn5wVJXAgR24rNXBDkUgEAgEIUBubm6brXi9e/dus3zixImtyhcvXszixYt9ytqaZbZx40a/sbz88suoquqz+GZH1+6ICy+8kAsvvDCgY/2dP5xbOkWLkx8kvcn9H5cjuIEcR2RZJj09PeTHRHQlwjn80ZovaNMZWg+w1gJacw62r7Z+ojqJrDcC8NSqHXxddDDI0RwfJEkiPj5eU1OYhXP4ozVf0K6zXq8XzmFMKPiKxMkfOndP5q79tZRZ2m4KDTcURWHXrl2a2t9KOIc/WvMFbTqrqorNZgvrbqIj0ZpzKPiKxMkfOneLk0F1amYBzFDYQPF4I5zDH635gjadAU0lih605hxsX5E4+UE61FWnx4ktyJsKCgQCgUAgCD4icfKDJ3EyqE7NLIApEAgEx4tgtxwItEVXPW9iOQI/eAaHG9BOV50sy2RlZWlqJo5wDn+05guh7Ww0GpFlmX379pGSkoLRaOySwb6qqqIoCs3NzZoZLK0156Px9XRbHzhwAFmWMRqNxxSDSJz80LLFSSt71UmSRHR0cHYpDxbCOfzRmi+EtrMsy+Tl5VFeXs6+ffuCHY5AI0RGRpKTk3PMf0yIxMkfsvv2GHFoZssVl8tFUVERffr0QafTBTuc44JwDn9nrflC6DsbjUZycnJwOp1dtneby+Viz5495OTkhKRzd6A156P11el0XbaMgUic/OBpcbrmlEx6Tesf5GiOH1ocdyCcwx+t+ULoO3t2uu+qBQ1dLheSJGE2mzWRRID2nEPBN/Q6v0OJQ4mTqqGVwwUCgUAgELSPSJz8IOk8iZM9yJEIBAKBQCAIBUTi5AfPrLrVRZX8c2VRkKM5PngGbYbiTJzuQjiHP1rzBeGsFbTmHAq+Qb3T999/P5Ik+XwNGDAgmCH54BnjdMDSwPd7aoIczfFDr9fe0DfhHP5ozReEs1bQmnOwfYOeog4ePJjy8nLv17fffhvskA6jcw9Y1NKWK4qiUFhYGPKDSrsS4Rz+aM0XhLNW0JpzKPgGPU3V6/Wkp6cHO4w28Yxx0uPSTOIkEAgEAoGgfYLe4lRYWEhmZib5+fnMnj2bPXv2BDskL4cXwHSIxEkgEAgEAkFwW5xGjRrFyy+/TP/+/SkvL2fBggWcdtppbN68mZiYmFbH22w2bDab9/u6ujrAva6DZwE1SZKQZRlFUXx2BW+vXJZlJElqu1x/qKsOJ00O9zU8A9KObCZsr1yn03mXiD8ylvbKA439qJwkqdVicy1jd7lc3n/Dxamjcs97VVX1OU9PdoLA6ulof25C2amtGFs+1+Hi1FHsLpfL+/9wcfJX7onlyJ/jnu7UUT0F8pnd05z8xe7xVVW1VYzH4tSZRViDmjhNnz7d+/8TTjiBUaNG0bt3b9566y2uuuqqVscvXLiQBQsWtCovKirybi0QFxdHRkYGlZWVWCwW7zHJyckkJydTVlaG1Wr1lqenpxMfH09JSQl2++FlB7KyspD1JsA9xqnO2kRhYSF5eXno9XoKCwt9YigoKMDpdFJcXOwtk2WZfv36YbVaKS0t9ZYbjUby8/OxWCxUVFR4y6OiosjOzqa6upqqqipveVc6RUdHU1RU5PPwtHTyPMhFRUX069cvLJwCqaeCggIaGxspKysLGyd/9VRfXw+461mSpLBw8ldPBw4c8PrGx8eHhVNH9aSqKomJiciyTGlpaVg4gf96iouLw2g0ep/rcHDqqJ48v/CLiorIz88PCyd/9dQy2bLb7V3mVFJSQqBIassoQoCTTz6ZKVOmsHDhwlavtdXi5JGOjY0Fujb7tf76FXsemUKhLod7BrzML7eND/uMXlVVHA4HBoPBuyprT3fqqNzzF6rBYGgzlp7oBP7rSVEUbDYbBoPBO6O1pzv5c1UUxftcy7IcFk4dxe75WTabzd6/znu6k79yTyzNzc3e5zocnDqqp0A+s3uak7/YPb4mk8l7nq5wslgsJCYmYrFYvPlEewR9cHhLGhoaKCoq4re//W2br5tMJkwmU6tynU7Xaul1z80+ks6Uq4f2qhuYbGLT7ROR5cN73LS31Htb5ZIkdaq8K2L3V+4vdpfLxe7duykoKPB+8PR0p47KXS4XxcXFFBQUhI2Th/ZiV1XVW88tX+/JTv7qqaWvJ7ae7tRRjC1/ltuLpac5dVTuz7mnOvkr78xndnvloejkr7ylryzLXebUme1bgjo4/Pbbb2fVqlWUlJSwevVqzj//fHQ6HZdddlkww/LiGRwuuRw+SZNAIBAIBAJtEtQWp9LSUi677DIOHjxISkoK48aNY+3ataSkpAQzLC+exEl1ii1XBAKBQCAQBDlxeuONN4J5+Q6RDi2AWWdtZM7/fuKVy4YHOaLjQ3tNqOGMcA5/tOYLwlkraM052L4hNzi8M9TV1REXFxfQYK6jwVZRSNFd/aiXIhmf+ha2R2Z0+TUEAoFAIBAEl87kE9pKUzuJ1GLLFbtLQVF6bI4ZMKqq0tDQQA/OpzuNcA5/tOYLwlkraM05FHxF4uQHz6w6PU4A7K7wXz1cURRKS0tbTfEMZ4Rz+KM1XxDOWkFrzqHgKxInP3gGh+tRkFWxX51AIBAIBFpHJE7+ONRVB2KjX4FAIBAIBCJx8otsOLzYpkF10uwMfC+bnookSRiNRu9CalpAOIc/WvMF4awVtOYcCr4htXJ4qKFrmTjh1ESLkyzL5OfnBzuM44pwDn+05gvCWStozTkUfEWLkz8kGWT3MuzFfxxPn6SoIAfU/aiqSm1trWZmaIBw1gJa8wXhrBW05hwKviJx8oOiKCC7xzlF6hRNbLuiKAoVFRWamaEBwlkLaM0XhLNW0JpzKPiKxKkjDg0QF9uuCAQCgUAgEIlTRxxay+nPH/3C5vK6IAcjEAgEAoEgmIjEyQ+SJHnXcvp481721DYFOaLuR5IkoqKiNDNDA4SzFtCaLwhnraA151DwFbPq/CDLMnpTBI5693IEWplVl52dHewwjivCOfzRmi8IZ62gNedQ8BUtTn5QFAVFcs+qM+Ci2RH+iZOiKFRVVWlmoCEIZy2gNV8QzlpBa86h4CsSJz+oqorr0C0yqA5NtDipqkpVVZVmpraCcNYCWvMF4awVtOYcCr4iceqIQ4PDDTixucJ/5XCBQCAQCATtIxKnjji0HIFWxjgJBAKBQCBoH5E4+UGSJPTGCMDd4qSFMU6SJBEXF6eZGRognLWA1nxBOGsFrTmHgq+YVecHWZYxRUbjBJ6/YCCpp+UFO6RuR5ZlMjIygh3GcUU4hz9a8wXhrBW05hwKvqLFyQ+KomBzuQegxRkhwqALckTdj6IolJeXa2aGBghnLaA1XxDOWkFrzqHgKxInP6iqimdYk1a2XFFVFYvFopkZGiCctYDWfEE4awWtOYeCr0icOuLQ4PC3NpTwxk9lQQ5GIBAIBAJBMBGJU0fI7sRpdVElX+2sCnIwAoFAIBAIgolInPwgSRLmqGgA9BpZjkCSJJKTkzUzQwOEsxbQmi8IZ62gNedQ8BWz6vwgyzLmqFiacS9HYNVA4iTLMsnJycEO47ginMMfrfmCcNYKWnMOBV/R4uQHRVGwNrsHhRtUJ82O8F85XFEU9u7dq5kZGiCctYDWfEE4awWtOYeCr0ic/KCqKo5DyxG4t1wJ/wdTVVWsVqtmZmiAcNYCWvMF4awVtOYcCr4iceoIseWKQCAQCASCQ4jEqSM8iRMOkTgJBAKBQKBxROLkB1mWiYlLBOA3J6bxzpyRQY6o+5FlmfT0dGRZO4+GcA5/tOYLwlkraM05FHzFrDo/SJJERHQs9UCMXiU91hzskLodSZKIj48PdhjHFeEc/mjNF4SzVtCacyj4aiNFPUoURaG6tg4A1aWNLVcURWHXrl2amaEBwlkLaM0XhLNW0JpzKPh2OnH67LPP+Pbbb73fP/PMM5x44olcfvnl1NTUdGlwwUZVVVy4F9natLeKP368NcgRdT+qqmK32zUzQwOEsxbQmi8IZ62gNedQ8O104nTHHXdQV+duhdm0aRO33XYbZ511FsXFxcyfP7/LAww6hwaHF+2v5bGvi4IcjEAgEAgEgmDS6TFOxcXFDBo0CIB33nmHmTNn8vDDD/Pjjz9y1llndXmAQUc2AqBXXThcKg6XgkEnejgFAoFAINAinc4AjEYjjY2NAHz55ZdMnToVgMTERG9LVLggyzKJqWmAewFMAKs9vFcPl2WZrKwszczQAOGsBbTmC8JZK2jNORR8O33lcePGMX/+fP7617+ybt06ZsyYAcCOHTvIyso66kD+9re/IUkSt9xyy1Gfo6uRJImIqFgAjIcSp8YwT5wkSSI6OlozG0aCcNYCWvMF4awVtOYcCr6dTpyefvpp9Ho9S5YsYfHixfTq1QuApUuXcuaZZx5VEOvXr+e5557jhBNOOKr3dxcul4t9lfsBMHlbnJzBDKnbcblc7NixA5crvBPElgjn8EdrviCctYLWnEPBt9NjnHJycvj4449blT/++ONHFUBDQwOzZ8/mhRde4MEHHzyqc3QnquS+RSbJXUnh3lUHaGZaa0uEc/ijNV8QzlpBa87B9u10i9OPP/7Ipk2bvN9/8MEHnHfeedxzzz3Y7Z1f6+j6669nxowZTJkypdPvPS54t1zRxhgngUAgEAgE7dPpFqc//OEP/PGPf2To0KHs2rWLSy+9lPPPP5+3336bxsZGFi1aFPC53njjDX788UfWr18f0PE2mw2bzeb93jMY3eVyeZvtJElClmUURfFZ56G9clmWkSSpzXIAVdYB0DtWz5bbx5ObFIWqqq0yXs/xR5brdLpWx3tiaa880NiPxkmSpFZNnC1jd7lc3n/Dxamjcs97VVX1OU9PdoLA6ulof25C2amtGFs+1+Hi1FHsLpfL+/9wcfJX7v3MPuLnuKc7dVRPgXxm9zQnf7F7fFVVbfN38bF8RgRKpxOnHTt2cOKJJwLw9ttvM378eF5//XW+++47Lr300oATp71793LzzTezbNkyzObAtjJZuHAhCxYsaFVeVFREdHQ0AHFxcWRkZFBZWYnFYvEek5ycTHJyMmVlZVitVm95eno68fHxlJSU+LSYZWVlERUVBbK7xUlyNCFbytEl5qEoEoWFhT4xFBQU4HQ6KS4u9pbJsky/fv2wWq2UlpZ6y41GI/n5+VgsFioqKrzlUVFRZGdnU11dTVVVlbe8K52io6MpKiryeXjy8vLQ6/UUFhZ6H6yioiL69esXFk6B1FNeXh5NTU2UlZWFjZO/eqqvr/fWsyRJYeHkr54OHDjg9Y2Pjw8Lp47qSVVV4uLikGWZ0tLSsHAC//UUFxeHTqfzPtfh4NRRPXkSiaKiIvLz88PCyV89tUx+7HZ7lzmVlJQQKJLayeU3Y2Nj2bBhAwUFBZxxxhnMnDmTm2++mT179tC/f3+ampoCOs/777/P+eefj06n85a1/GvQZrP5vAZttzh5pGNj3bPfurrFqaFoPXv/Ogp9fCZ9HtsT9hm9JyZZlr33v6c7dVSuqiqSJCFJUpux9EQn8F9PiqLgdDq91woHJ3+uni9Zlr1fPd2po9g9f5Hr9Xrv/3u6k79yTyxOp9P7TIeDU0f1FMhndk9z8he7Jy69Xu89T1c4WSwWEhMTsVgs3nyiPTrd4jRy5EgefPBBpkyZwqpVq1i8eDHgXhgzLS0t4POcfvrpPmOlAObNm8eAAQO46667WiVNACaTCZPJ1Kpcp9O1Ot5zs4+kM+Uul4vS8koAmm3N3P9FIWcNTGV0bmKb8XliORJJkjpV3hWx+yv3F7vL5WLXrl0UFBR4P3h6ulNH5S6Xi8LCQgoKCsLGyUN7sauq6q3nlq/3ZCd/9dTS1xNbT3fqKEaXy8XOnTvbfa79nSdUnToqd7lcFBUVtencU538lXfmM7u98lB08lfe0rdlshhI7P6c2rtuW3Q6cVq0aBGzZ8/m/fff509/+hN9+/YFYMmSJYwZMybg88TExDBkyBCfsqioKJKSklqVB5VDg8OdDhsPfllIYqSB0bmJQQ5KIBAIBAJBMOh04nTCCSe0aikCePTRRzuVsfUYDiVOOkU7yxEIBAKBQCBom04nTh42bNjA1q1bARg0aBAnnXTSMQezcuXKYz5Hl+NJnFQHIBIngUAgEAi0TKcTp/379zNr1ixWrVpFfHw8ALW1tUyaNIk33niDlJSUro4xaMiyTH7f/hQBsupCVl1hnzjJsuwzDkQLCOfwR2u+IJy1gtacQ8G301e+8cYbaWhoYMuWLVRXV1NdXc3mzZupq6vjpptu6o4Yg4rS4hbpcYX9livgnpWiNYRz+KM1XxDOWkFrzsH27XTi9Nlnn/Gvf/2LgQMHessGDRrEM888w9KlS7s0uGCjKAolpfu83xtUZ9i3OCmKQnFxcaspnuGMcA5/tOYLwlkraM05FHw7nTgpioLBYGhVbjAYwrPi5MO9mXrCP3ESCAQCgUDQPp1OnCZPnszNN9/Mvn2HW2LKysq49dZbOf3007s0uFBAknVwaNuVL383gqfOD6GlEgQCgUAgEBxXOp04Pf3009TV1ZGbm0ufPn3o06cPeXl51NXV8eSTT3ZHjEFFlmUkvRGAE9IiyE2MDHJE3Y9WBhm2RDiHP1rzBeGsFbTmHGzfTm+5Au5Vh7/88ku2bdsGwMCBA5kyZUqXB9cRdXV1xMXFBbRE+rGw7Zo4lKY6+vx9B6b0gm67jkAgEAgEguNPZ/KJo1rHSZIkzjjjDM444wxv2bZt2zjnnHPYsWPH0ZwyJFFVFavV6m1xevG7ndhTdNwyPj/IkXUfHueoqCjv8v3hjnAOf2et+YJwFs7hSSj4dll7l81mo6ioqKtOFxIoikJpaSmSzp04PfX1dhYuL+zgXT0bj3NYDvRvB+Ec/mjNF4SzVtCacyj4aqtj9CjxtDgZNLAApkAgEAgEgvYRiVMgHGpxMuCk0eHiKIaFCQQCgUAgCANE4uQHSZIwGo1Ieve6VQbVgapCkyN8W528zhroK/cgnMMfrfmCcNYKWnMOBd+AB4cnJCT4DTTYS6B3B7Isk5+fzy794RYncG/0G2k86v2RQxqPs5YQzuGP1nxBOGsFrTmHgm/Av/0XLVrUjWGEJqqqYrFYvF11kbJ7MJrV7iJ8tjL2xeMcFxenmb9ghHP4O2vNF4SzcA5PQsE34MRpzpw53RlHSKIoChUVFZgOddVF692JU2MYDxD3OMfExKDT6YIdznFBOIe/s9Z8QTgL5/AkFHzDs7+pi/EsR3DXhN7cOWwcuYkRQY5IIBAIBAJBMBCJUwBIOneLU79EI/G9E4IcjUAgEAgEgmAhZtX5QZIk9+qkhwaHq057kCPqfrzOGugr9yCcwx+t+YJw1gpacw4FX5E4+UGWZbKzs5EMJgB+2VPFP1YUsaWiPsiRdR8e52Bvong8Ec7hj9Z8QThrBa05h4KvNu70UaIoClVVVd6uutXF+7nj419Zt6cmyJF1Hx5nrSzfD8JZC2jNF4SzVtCacyj4dnqMk8vl4uWXX2b58uXs37+/VfBfffVVlwUXbFRVpaqqimjvcgTu2XThvO2KxzkhQTtjuYRz+KM1XxDOWkFrzqHg2+nE6eabb+bll19mxowZDBkyRBP9qp4xTmYp/BMngUAgEAgE7dPpxOmNN97grbfe4qyzzuqOeEIST1fd4cQp/FZJFwgEAoFA0DGdTpyMRiN9+/btjlhCDkmSiIuLA2+L0+EtV8IVj7MWWhI9COfwR2u+IJy1gtacQ8G304PDb7vtNp544glUVe2OeEIKWZbJyMhAPjSrznRor7pwXjnc66yRGRognLWA1nxBOGsFrTmHgm+nW5y+/fZbVqxYwdKlSxk8eDAGg8Hn9XfffbfLggs2iqJQWVmJfKirzigd3qsuXPE4p6WlaeYHUTiHv7PWfEE4C+fwJBR8O504xcfHc/7553dHLCGHZzPBxEOJU98EA8suPJWsOHOQI+s+PM6pqanBDuW4IZzDH635gnDWClpzDgXfTidOL730UnfEEdocSpyidApT+qUEORiBQCAQCATB4qj3qjtw4ADbt28HoH///qSkhG9C4d1yxRX+W64IBAKBQCBon053EFqtVq688koyMjIYP34848ePJzMzk6uuuorGxsbuiDFoSJJEcnIyst49OLypqYnn1pTw3w2lQY6s+/A4a2WGBghnLaA1XxDOWkFrzqHg2+nEaf78+axatYqPPvqI2tpaamtr+eCDD1i1ahW33XZbd8QYNGRZdidOh2bVNTQ1c82STfx12Y4gR9Z9eJ01MMjQg3AOf7TmC8JZK2jNORR8O33ld955hxdffJHp06cTGxtLbGwsZ511Fi+88AJLlizpjhiDhqIo7N27F2R3j6ZedQDhP6tu7969mtn3CISzFtCaLwhnraA151Dw7XTi1NjYSFpaWqvy1NTUsOuqU1UVq9XqHRyuU8J/AUyPsxbW6fIgnMMfrfmCcNYKWnMOBd9OJ06jR4/mvvvuo7m52VvW1NTEggULGD16dJcGFyp4BofrFE+Lk9hyRSAQCAQCLdLpWXVPPPEE06ZNIysri2HDhgHw888/Yzab+fzzz7s8wFBA0rkTJ/lQ4uRwqThcCgadNvqUBQKBQCAQuOl04jRkyBAKCwt57bXX2LZtGwCXXXYZs2fPJiIiossDDCayLJOeno68dzcAkvNwK5vV7iI+IvwSJ6+zRgYagnDWAlrzBeGsFbTmHAq+R7WOU2RkJFdffXVXxxJySJJEfHw8jQdiAFBtDehkCZeiYrU7iY8wdHCGnofHWUsI5/BHa74gnLWC1pxDwTegxOnDDz9k+vTpGAwGPvzwQ7/HnnPOOQFffPHixSxevJiSkhIABg8ezL333sv06dMDPkd3oigKJSUlpBuj3N831/PuNSMx6WUSI41Bjq578Djn5uZq5i8Y4Rz+zlrzBeEsnMOTUPANKHE677zzqKioIDU1lfPOO6/d4yRJwuUKfMZZVlYWf/vb3ygoKEBVVV555RXOPfdcfvrpJwYPHhzweboLVVWx2+3ICdGAO3E6Z0h6kKPqXjzOWpmhAcJZC2jNF4SzVtCacyj4BpQ4tVwvoSvXTjj77LN9vn/ooYdYvHgxa9euDYnEyYNsPtRV52hGdTmRdEe9U41AIBAIBIIeTKczgP/85z/MmjULk8nkU26323njjTe44oorjioQl8vF22+/jdVqbXdZA5vNhs1m835fV1fnfa+npUuSJGRZRlEUn4y0vXJZlpEkqc1ycCeKqiHSW77y1xIKG/SMz0ukICWqzeNbotPpUFXVp9wTS3vlgcZ+NE5ttQq2jN3lcnn/DRenjso971VV1ec8PdkJAquno/25CWWntmJs+VyHi1NHsbtcLu//w8XJX7knliN/jnu6U0f1FMhndk9z8he7x1dV1VYxHotTZ3rLOp04zZs3jzPPPJPU1FSf8vr6eubNm9fpxGnTpk2MHj2a5uZmoqOjee+99xg0aFCbxy5cuJAFCxa0Ki8qKiI62t2dFhcXR0ZGBpWVlVgsFu8xycnJJCcnU1ZW5l7U8hDp6enEx8dTUlKC3X54E9+srCyioqJQFIVdu/eCbADFwbMrNvHWbj0PT0jj/H6x3uMLCgpwOp0UFxd7y2RZpl+/flitVkpLD+9vZzQayc/Px2KxUFFR4S2PiooiOzub6upqqqqqvOVd6RQdHU1RUZHPw5OXl4der6ewsBBVVXE6nRQVFdGvX7+wcGpJe/WUlZVFU1MTZWVlYePkr57q6+u99SxJUlg4+aunAwcOeH3j4+PDwqmjelJVlejoaGRZprS0NCycwH89xcXFAXif63Bw6qieXC6X99nOz88PCyd/9eT5HQXuBpuucvKMtQ4ESe1kR6Esy1RWVpKSkuJT/vPPPzNp0iSqq6s7czrsdjt79uzBYrGwZMkS/u///o9Vq1a1mTy11eLkkY6NdScx3ZX9Ft6YimKt5rnRb/FkoZlHZw7g1vH5PsdDeGT0wkk4CSfhJJyEk5acLBYLiYmJWCwWbz7RHgG3OA0fPhxJkpAkidNPPx29/vBbXS4XxcXFnHnmmYGezovRaKRv374AjBgxgvXr1/PEE0/w3HPPtTrWZDK16iIE943S6XQ+ZZ6beiSdKXe5XBQVFdGnTx90ETEo1mqS9XbAjKXZ1eqanliORJKkTpV3Rez+ytu6pqe8pbPnL7ae7tRRucvlYseOHe56DhMnD+3FrijK4We7xes92clfPbV8rj2x9XSnjmJ0uVzs3Lmz3efa33lC1amjcp/P7COu0VOd/JV35jO7vfJQdPJXfmQdd5VTe9dti4ATJ89suo0bNzJt2jRv1xi4k5/c3FwuvPDCgC/cHoqi+LQqBRtPduoZIJ6kdzctVjc6ghZTd3NkBq8FhHP4ozVfEM5aQWvOwfYNOHG67777AMjNzWXWrFmYzeZjvvjdd9/N9OnTycnJob6+ntdff52VK1eG5NYtnsQpXnavHl7daPd3uEAgEAgEgjCk04PD58yZ02UX379/P1dccQXl5eXExcVxwgkn8Pnnn3PGGWd02TW6Ck/iFCe7W8PCucVJIBAIBAJB23Q6cXK5XDz++OO89dZb7Nmzx2dUPNCpweEvvvhiZy9/XJFlmby8PGRZRja7uyZjONTi1BSeLU4tnbWCcA5/tOYLwlkraM05FHw7feUFCxbw2GOPMWvWLCwWC/Pnz+eCCy5AlmXuv//+bggxuHgGwXtanLIjnLx1xQieOHdIMMPqVloO/NcKwjn80ZovCGetoDXnYPt2OnF67bXXeOGFF7jtttvQ6/Vcdtll/N///R/33nsva9eu7Y4Yg4aiKBQWFqIoijdxilKbuXhYJmPyEoMcXffQ0lkrCOfwR2u+IJy1gtacQ8G304lTRUUFQ4cOBSA6Otq74NXMmTP55JNPuja6EMKTOCnN9UGORCAQCAQCQbDodOKUlZVFeXk5AH369OGLL74AYP369W2usRQu6FokTu/8so/n1+zGanMGOSqBQCAQCATHk04nTueffz7Lly8H4MYbb+Qvf/kLBQUFXHHFFVx55ZVdHmCocLjFqYF5b/zMH5b8Qnl96Kw3JRAIBAKBoPvp9JYrR7JmzRrWrFlDQUEBZ599dlfFFRB1dXXExcUFtET60eBZnl2WZSzfvsK+/5tH1NAzmdB8G7trmvj+5nGckpPQ5dcNJi2dPavQhjvCOfydteYLwlk4hyfd5duZfOKYh6aPHj2a0aNHH+tpQhan04nRaPQZ45QYaWB3TVPYruXkcdYSwjn80ZovCGetoDXnYPsGlDh9+OGHAZ/wnHPOOepgQg1FUSguLqagoMAncUpIcldYOK4e3tK5M3v39GSEc/g7a80XhLNwDk9CwTegxMmzT50HSZI4sofP02R25M7G4YIc4dviBGL1cIFAIBAItEZAg8MVRfF+ffHFF5x44oksXbqU2tpaamtrWbp0KSeddBKfffZZd8cbNI7sqgOROAkEAoFAoDU6Pcbplltu4dlnn2XcuHHesmnTphEZGcnvf/97tm7d2qUBBhvPsu6yyb3litLcQGJk+HbVAZpZur8lwjn80ZovCGetoDXnYPt2elZdREQE69evZ8gQ3y1HfvnlF0aNGkVTU1OXBuiP7p5V1xJnw0F2XJ8MgOPBKgqrbQxMi2Fweky3XlcgEAgEAkH30pl8otNp28knn8z8+fOprKz0llVWVnLHHXdwyimndD7aEEZVVRoaGlBV1bsAJsCQRJmLhmWGZdLU0lkrCOfwR2u+IJy1gtacQ8G304nTv//9b8rLy8nJyaFv37707duXnJwcysrKePHFF7sjxqChKAqlpaUoioKkNyLp3V104bztSktnrSCcwx+t+YJw1gpacw4F306Pcerbty+//PILy5YtY9u2bQAMHDiQKVOmhP3iW7I5BlfDQWpqqllVqkdRVGYN7xXssAQCgUAgEBwnjmoBTEmSmDp1KlOnTu3qeEIaT+JUUXWQS97eS2q0USROAoFAIBBoiIASpyeffJLf//73mM1mnnzySb/H3nTTTV0SWCggSRJGo9Hbkiab3TPrYqVmwL0cgaqqYdXSdqSzFhDO4Y/WfEE4awWtOYeCb0Cz6vLy8vjhhx9ISkoiLy+v/ZNJErt27erSAP1xPGfVART/dQxNO9eQcu3bpL5tcsfw0HRizMe8c41AIBAIBIIg0eV71RUXF7f5/3BHVVUsFgtxcXFIkuRdBFPvtGLSR2BzKlQ32sMqcTrSWQsI5/B31povCGfhHJ6Egq+2Vs3qJIqiUFFR4R2970mc1OaGsF09/EhnLSCcwx+t+YJw1gpacw4F34CaSubPnx/wCR977LGjDibU8d12xUh5nS1sVw8XCAQCgUDQmoASp59++imgk4V7M6GurY1+m8KrxUkgEAgEAkH7BJQ4rVixorvjCEkkSSIqKsqbEEot9qv70+kF1I9zcmrvhGCG2OUc6awFhHP4ozVfEM5aQWvOoeAbPqOauwFZlsnOzvZ+r2vRVTdtQGqwwupWjnTWAsI5/NGaLwhnraA151DwParE6YcffuCtt95iz5492O2+Y3zefffdLgksFFAUherqahITE5Fl2TvGyRXmW660dNYCwjn8nbXmC8JZOIcnoeDb6au+8cYbjBkzhq1bt/Lee+/hcDjYsmULX331FXFxcd0RY9BQVZWqqirvZoJyizFOxQcbefvnfawqqgpmiF3Okc5aQDiHP1rzBeGsFbTmHAq+nU6cHn74YR5//HE++ugjjEYjTzzxBNu2beOSSy4hJyenO2IMGbyz6prqWbbjAJf8ZwOPrigKclQCgUAgEAiOF51OnIqKipgxYwYARqMRq9WKJEnceuutPP/8810eYCjRcjmC3MQIAHbXNAUzJIFAIBAIBMeRTidOCQkJ1Ne7x/j06tWLzZs3A1BbW0tjY2PXRhdkJEnyWZ3Us1edYmsgNzESgJKaxrBqIj3SWQsI5/BHa74gnLWC1pxDwbfTg8PHjx/PsmXLGDp0KBdffDE333wzX331FcuWLeP000/vjhiDhizLZGRkHP6+RYtT73h3i1ODzUV1o4OkKGNQYuxqjnTWAsI5/NGaLwhnraA151DwDbjFydOy9PTTT3PppZcC8Kc//Yn58+dTWVnJhRdeyIsvvtg9UQYJRVEoLy9vteWK0lyP2aAjI9a90W9Jdfi0tB3prAWEc/ijNV8QzlpBa86h4Btw4nTCCScwatQo3nnnHWJi3AmELMv88Y9/5MMPP+Sf//wnCQnhtRikZzNBT1ecZx0n1WFDdTronXC4uy5cONJZCwjn8EdrviCctYLWnEPBN+DEadWqVQwePJjbbruNjIwM5syZwzfffNOdsYUcnhYnODRAPMHdXVdSLQaICwQCgUCgBQJOnE477TT+/e9/U15ezlNPPUVJSQkTJkygX79+/P3vf6eioqI74wwJJL0ByeDunnM113P92FzenTuSi4dpp39ZIBAIBAIt0+lZdVFRUcybN49Vq1axY8cOLr74Yp555hlycnI455xzuiPGoCFJEsnJyT6j9+UW+9WNy0/i/KEZ5BzqsgsH2nIOd4Rz+KM1XxDOWkFrzqHgK6nH2FFotVp57bXXuPvuu6mtrcXlcnVVbB1SV1dHXFwcFouF2NjY43LNwtvzcRwoJvfP3xFZMOa4XFMgEAgEAkH30Zl84qg3evn666+ZO3cu6enp3HHHHVxwwQV89913nTrHwoULOfnkk4mJiSE1NZXzzjuP7du3H21IXY6iKOzdu9dn9L4+Lh0AZ+0+mh0u3vllH4+vKgqbgXltOYc7wjn80ZovCGetoDXnUPDtVOK0b98+Hn74Yfr168fEiRPZuXMnTz75JPv27eOFF17g1FNP7dTFV61axfXXX8/atWtZtmwZDoeDqVOnYrVaO3We7kJVVaxWq09SZEhybyvjOLgHFbjolQ3M//BXqhsdQYqya2nLOdwRzuGP1nxBOGsFrTmHgm/AC2BOnz6dL7/8kuTkZK644gquvPJK+vfvf0wX/+yzz3y+f/nll0lNTWXDhg2MHz/+mM7dXRxOnPYSYdCRFmOist7G7prGsFkEUyAQCAQCQdsEnDgZDAaWLFnCzJkz0el03RKMxWIBIDExsc3XbTYbNpvN+31dXR0ALpfLO7ZKkiRkWUZRFJ+MtL1yWZaRJKnNcnA3C7Yct6VPzAbAfnA3LpeL3gkRVNbbKK5uZHivuFbNhzqdDlVVfco9sbRXHmjsR+MkSVKrcWhHunr+bVnek506Kve8V1VVn/P0ZCcIrJ6O9ucmlJ3airHlcx0uTh3F7nK5vP8PFyd/5Z5Yjvw57ulOHdVTIJ/ZPc3JX+weX1VVW8V4LE6dGZ8dcOL04YcfBnzSo0FRFG655RbGjh3LkCFD2jxm4cKFLFiwoFV5UVER0dHu2W5xcXFkZGRQWVnpTcQAkpOTSU5OpqyszKcrMD09nfj4eEpKSrDb7d7yrKwsoqKicDqdFBUVeUfwp8ZmAtBQVkhhYSFJeicAxQet2O12iouLveeQZZl+/fphtVopLS31lhuNRvLz87FYLD7LOERFRZGdnU11dTVVVVXe8q50io6OpqioyOfhycvLQ6/XU1hYiKqq2O12ioqK6NevH06ns8c7taSgoKBNp/T0dJqamigrKwsbJ3/1VF9f761nSZLCwslfPR04cMDrGx8fHxZOHdWTqqpEREQgyzKlpaVh4QT+6ykuzv3Ha8vP7J7u1FE9uVwu77Odn58fFk7+6snzOwro0t+5JSUlBMoxz6rrKq699lqWLl3Kt99+S1ZWVpvHtNXi5JH2jILv7uzXvvcXiu87CV1sKn0X7ePuT7fx6Mpd3DAulyfPGxIWGX1H5cJJOAkn4SSchFM4OVksFhITEwOaVdfpTX67gxtuuIGPP/6Yr7/+ut2kCcBkMmEymVqV63S6Vt2Hnpt6JJ0pVxSF3bt3k5ub633dmNwbAFfdfiSXnfykKAD21DQhSVKb3ZidLe+K2P2Vt9fVqtPpUBSFkpIScnNzvX+x9XSnjsoVRWHXrl3k5uaGjZOH9mJXVbXVs300MYaSk796avlce2Lr6U4dxdiWc6DnCVWnjsr9OfdUJ3/lnfnMbq88FJ38lR9Zx13l1JkhSEe9HEFXoKoqN9xwA++99x5fffUVeXl5wQynFZ4mQZ+sOCoByeROlhzVpeQmHtqvLky2XWnLOdwRzuGP1nxBOGsFrTmHgm9QW5yuv/56Xn/9dT744ANiYmK8fY9xcXFEREQEM7R2kSQJQ1IO9n1bcRzcw8je43hnzkj6JIfP6uECgUAgEAjaJqgtTosXL8ZisTBx4kQyMjK8X2+++WYww+oQz5IEzoN7SI42ccEJGQzLjAtyVAKBQCAQCLqboLY4hXrToizLZGVlteqLNRxaksBxcE8wwupW2nMOZ4Rz+KM1XxDOWkFrzqHgGxKDw0MVSZK8yxy0xLsIZvVeALZV1vPhlkpyEiK4dHiv4xpjV9OeczgjnMMfrfmCcNYKWnMOBV9tpKhHicvlYseOHa2mR7bcdgVgZdFB7vpkKy9+3/NboNpzDmeEc/ijNV8QzlpBa86h4CsSpw44co0IaNni5E6URucmAPD9nlpcSmh3PwZCW87hjnAOf7TmC8JZK2jNOdi+InE6CgyJLTb6VVWGpMcSbdJRb3Pya2V9kKMTCAQCgUDQXYjE6SjQJ7oX6VTtTbgaDqKTJU7Jdrc6rSmpCWZoAoFAIBAIuhGROPlBlmXy8vJajd6XDSb0cenA4XFOnu66Nbt7duLUnnM4I5zDH635gnDWClpzDgVfbdzpY0Cvb3viof6IAeKje3tanKqPT2DdSHvO4YxwDn+05gvCWStozTnYviJx8oOiKBQWFvofIH4ocTr1UOK0u6aJ+mbn8Quyi/HnHK4I5/BHa74gnLWC1pxDwVdbaWoX4l09/NBaTklRRjbcehqD02Mw6QPfLFAgEAgEAkHPQSROR4kncbJXlXjLTsqKD04wAoFAIBAIjguiq+4oMSbnAeA4UBzkSAQCgUAgEBwvJDXUN4zzQ11dHXFxcVgsFmJjY7v8/KqqoigKsiwjSZLPa82lm9n1p6HIkfEMWOyeSacoKrd9tIVPt+5n5XVjyIg1d3lM3Y0/53BFOIe/s9Z8QTgL5/Cku3w7k0+IFqcOcDrbHuhtTMkHQGmsxWV1J06yLPFdcQ07Dlj5bNv+4xZjV9OeczgjnMMfrfmCcNYKWnMOtq9InPygKArFxcVtjt6XTZHetZzs+3d5y6cPSAVgaQ9NnPw5hyvCOfzRmi8IZ62gNedQ8BWJ0zFgONTqZN9f5C2bPtCdOH2x/QBOlzYeZIFAIBAItIJInI4BY2ofABwHDrc4nZwdT1KkAUuzs8evIi4QCAQCgcAXkTh1gL9l3Q2pnhanw4mTTpaY1r9nd9dpZen+lgjn8EdrviCctYLWnIPtq6273Ul0Oh39+vVDp2t7QUtPi5P9QJFPuae77uNfK+lpkxY7cg5HhHP4ozVfEM5aQWvOoeArEic/qKpKQ0NDu8mPZ2ado0WLE8CZ/VNIjTZy9qA0nErPSpw6cg5HhHP4ozVfEM5aQWvOoeArEic/KIpCaWlpu6P3PV11joN7UJ0Ob3lytIm9fzmDh84aiEHXs25xR87hiHAOf7TmC8JZK2jNORR8e9Zv9RBDH5eOZIwAVcFxcLfPa0a9uLUCgUAgEIQb4rf7MSBJkre7zn5Edx24mxS/3HGA+z/ffrxDEwgEAoFA0A2IxMkPkiRhNBr9LuvuXcvpQOvEqbi6kanPr2XBFzvYUlHfbXF2JYE4hxvCOfzRmi8IZ62gNedQ8BWJkx9kWSY/P9/v1EfvWk77i1q9lp8UxYyBaQB8tKWie4LsYgJxDjeEc/ijNV8QzlpBa86h4KuNO32UqKpKbW2t39H7xtT2W5wAJvZJAuD7PbVdHl93EIhzuCGcwx+t+YJw1gpacw4FX5E4+UFRFCoqKvyO3je0sySBh1E58QCs3V3TIx7sQJzDDeEc/mjNF4SzVtCacyj4isTpGPEugrm/qM3E6KSsOHSyREW9jb21Tcc7PIFAIBAIBF2ISJyOEUNyLkgySnM91l+Xt3o90qjnhIwYoOd01wkEAoFAIGgbkTj5QZIkoqKi/I7el41mEiZeDUDZc7/BWdt6EPionAQANpXXdU+gXUggzuGGcA5/tOYLwlkraM05FHwltScMvGmHuro64uLisFgsxMbGBi0Oxd5E8YJR2Eo3ETloMr3v+AJJPryPzu7qRmRJIiverJmHWyAQCASCnkJn8gnR4uQHRVGoqqrqcBCabIwg6/q3kExRNP76FTUrnvd5vXdiJNkJET0iaQrUOZwQzuGP1nxBOGsFrTmHgq9InPygqipVVVUBzYYzZQ4g5dy/AFD/0wfdHVq30RnncEE4hz9a8wXhrBW05hwKviJx6kKiTzgLgMbt36A67T6vvflTGee8uI7/bigNRmgCgUAgEAi6AJE4dSGmrCHoYlNR7Y007lzr89qWyno++rWSL3ccCFJ0AoFAIBAIjhWROPlBkiTi4uICHpskSRJRAycDYP31S5/XxuYmAvDmxn2s3V3TtYF2IZ11DgeEc/ijNV8QzlpBa86h4CsSJz/IskxGRkan9sSJGnQ6ANYtvms6ndEvhbMHpdHsVDjn3+soqrJ2aaxdxdE493SEc/ijNV8QzlpBa86h4BvUO/31119z9tlnk5mZiSRJvP/++8EMpxWKolBeXt6p0ftRg6cA0FS8DldTvbdcliVe/81JnJQVx4EGOzP+73uqG+3tnSZoHI1zT0c4hz9a8wXhrBW05hwKvkFNnKxWK8OGDeOZZ54JZhjtoqoqFoulU6P3jSm57v3rXE4at3/t81q0Sc/HV51CdryZ7QesnP/SemxOV1eHfUwcjXNPRziHP1rzBeGsFbTmHAq+QU2cpk+fzoMPPsj5558fzDC6nMPddV+2ei0j1swnvxtFjEnPsMw49BppXhUIBAKBIBzQBzuAcCRq8OnUrnqh1QBxD0MzYtl8xwRyEiKPc2QCgUAgEAiOhR6VONlsNmw2m/f7ujr33m8ulwuXy93lJUkSsiyjKIpPU1575bIsI0lSu+WJiYk+fameAWlH9q+2LDf3nwiyDlvpZmzl2zGm9/M5XpIkchIiUVUVRVFosDm58D8b+POUAib2TQk49qN18tyrtmJXFMXr3J6rTqfzxn5kLO2VB9Opo3JFUUhOTgbwOU9PdgL/9QT4PNvh4OSvnlo+1+Hi1FHsiqKQlJTkN/ae5uSv3N9ndk938ldPgXxm9zQnf7F7fIFWMR6L05HX9UePSpwWLlzIggULWpUXFRURHR0NQFxcHBkZGVRWVmKxWLzHJCcnk5ycTFlZGVbr4Rlt6enpxMfHU1JSgt1+eLB2VlYW0dHR1NbWUl1d7S3Py8tDr9dTWFjoE0NBQQFOp5Pi4uJDB46Foq+xfPcqkWf+kdLSwwtfGo1G8vPzsVgsVFRUsGh9FcsLa1i58yCvXn4SZ+SYqKqq8h7f1U5FRUU+D09bTtXV1a2dcD/E/fr1w2q1+nXyEBUVRXZ2NtXV1UF3arOeWjg1NDSEnVN79VRfX091dbX32Q4Hp0Dqqbq6OuycwH89ybLM3r17w8rJXz01NDT4fGaHg1Mg9VRdXR12TtB+PSUlJWG327vMqaSkhEAJmU1+JUnivffe47zzzmv3mLZanDzSnk35ujL7VVWVvXv3kpmZ6c16A83o675/k/LnZmNIzqXPIztpeZOPzH7rmh1c+85m3vy5HEmCFy4+gbkjszqMvbtanPbt20dmZiZ6vb5N1574V4q/ckVRqKioIDMz0+fYnuwE/uvJ5XJRWlrqfbbDwamjvyY9z7VOpwsLp0BanMrLy8nKcn+WhIOTv3J/n9k92SmQFqeOPrN7mpO/2D2+WVlZ3vN3hZPFYiExMTGgTX57VIuTyWTCZDK1KtfpdOh0Op8yz80+ks6UK4pCU1MTsiy3Ov+R3x9ZHjfyfCpficFRVULTztVE9T+t1bGSJKHT6UiI0vH6b0YQH7mJ59bs5ndv/UK0Uc+s4b263CmQ2D3OngXG2jreE3ug5V0V+9E6dVTu+YsonJyg/diBNp/tnuzkr55UVfX6HvlHUKCxh5pTIDE2Njaiqmq7sfREJ3/lx/KZHUh5qH7uBfKZ3V55qDr5K29qavIb49E4tXfdtgjqlK6GhgY2btzIxo0bASguLmbjxo3s2bMnmGF1CbIxgtiTLwLA8t2rHR8vSyy+cCg3jM0F4Lp3N7G/3ub/TQKBQCAQCI4rQU2cfvjhB4YPH87w4cMBmD9/PsOHD+fee+8NZlhdRtzY3wJQt/4tFHtzh8dLksRj5w7mxMxYqhsd3PLBlu4OUSAQCAQCQScIauI0ceJEVFVt9fXyyy8HMywvsiyTnp7ebpNiR0T2n4A+MRul0cLBz/4Z0HsMOpkXZw1jVE48d03uc1TXPRaO1bknIpzDH635gnDWClpzDgXfkBkcfjTU1dURFxcX0GCuYFH91bNUvHItABnzXiBh4u8Cep+qqt7+aoFAIBAIBN1HZ/IJbaSoR4miKOzatavVqP3OkDj5GpJm3AVA+ct/wLLm9YDe1zJpcriO3548XeHc0xDO4Y/WfEE4awWtOYeCr0ic/KCqKna7/Zj3xEm9eCHxE64GVaHs2dmUv3wtir2pw/fVNTuY/8EWBj2ykmbH8dnTrqucexLCOfzRmi8IZ62gNedQ8BWJ03FAkiQy5i4m6aw7AahZ8SzFC0bhstb4fZ9JL7Pkl33srLLywtqeP9NQIBAIBIKejkicjhOSrCNt1t/JueMLdHFp2Eo3sX/Jn7yv1/+8lLr17/i8x6TXcc/pBQAs/KqQez7dytz//cTHv1Ye19gFAoFAIBC4EYPD/aCqKlarlaioqC4dqG3dtordCyeCJJF333rs+3dS9q9LAchbsIGI3JO8x9qcLgoWfsXe2sPLGUQZdWy7axJZ8RFdFpMHf84NNidbKuoZ1Tuhy68bTLqrnkMZrTlrzReEs3AOT7rLVwwO7yIkSSI6OrrLH8aoAROIHX05qCplz17Ovuev8L525LIFJr2O/1w2nEuGZXLD2FyGZcYyvFccTYfGPK3bU8NZL3zPe5vKuyQ2f85XvrmRU5/8lrd/3tcl1woVuqueQxmtOWvNF4SzVtCacyj4isTJDy6Xix07dnRq1+RASZv1KLI5GnvFDlSnnYiCsYB7jzvHQd/xTBP7JvPmFSN46oKhLL9mNF9fP4aClGg+/rWSif9azdJt+7n01R9Zt8f/mKlAaM9ZVVXe/tmdnBUfbDzm64QS3VnPoYrWnLXmC8JZK2jNORR8ReLUAd015dGQkEnKhQ8CENFvHL3vXEbkwEmguDj4xRPtvi8pyogkSeytaeKCl9fT5FCIjzBgdylc8PIPVB6xTUuj3eltnQqUtpz31LhnAepkiRvG5XbqfD0BrUzlbYnWnLXmC8JZK2jNOdi+InEKIoln3ETevd/T+84vkY0RJE2/A4Dalc93OOMuOyGCJ84bwryTsym6ZzIDUqMpszRz1v99T32zE4Bnvi0m+d7POemxr71l7bHroBW7s/2HcXWJO57hvWKJNPaovaEFAoFAIOgyROIURCRJIqLPKcgGEwDRJ5yJKXMQSnMDO25KZ9f9J1Oz4rl233/tmFxenDWMxEgj7887mVizHlVViTa5d3k2G3Q0ORS27W9g/oft73v3yvq99Hn4K271szfe6pJqAIamx/LNroNUNYgNiAUCgUCgPcSsOj94FtoyGo3HbSBa/cZP2Pf8Fbis1d6y7Fs+JGb42R2+d8eBBg5a7YzOTQRAUVTu+mQr/1hZBMDHV53CiKw40mPN3vdUN9pJ+svn3u8bHp6OHlcr55GPf82GUov3+9dnn8RlJ/U6etEAWbenhmuW/MJT5w9lbF5it1wjGPUcbLTmrDVfEM7COTzpLl8xq64L0euPb7dUzIkz6PdMFX0fLXKvNg7se/EqnLUVHb63X0q0N2kCkGWJR88exK3j8wG46JUfyPrrl5TWHl61/C9Lt3v/3ycpkpLqxlbOVpuTjfvqADh7UBoAP5VZOB5MfW4tP5XVMfPFdd16neNdz6GA1py15gvCWStozTnYviJx8oOiKBQWFh73gWiSJGFMzSf9t09hyj4BV/0Byv5vLk3FG7Dv34WqdG6w90NnDWBAajTNTgVFVVlVdBCAjWUW/rW6BIA3fnMShXdPZkBqVCtnvU7ig3kn89D0AZwz2J04/XgcEidVVbEcGptV2+TotusEq56DidacteYLwlkraM05FHy1lab2MGSDiV7XvE7x/SOxbvqc4k3uLjVDcm8Sp9xI/ITfoYuM6/A8EQYdy/5wKp9sreSsAWlkJ7gXzlz09S4AZp2Yyazh7Xe7mfQ6ZgxKY8agNDbsrQXcLU6qqnZr03BlvQ1ZAuVQZ/L+ehupMaZuu55AIBAIBB0hWpxCHHPWYHpd8zrmnBPRJ/RCMphwVO2m8o3b2X59EjvvLGDPYzOpW7/EZ9ND9YhsPCs+gj+MzvUmTaqqEmPSM7p3Av88Z5D3uCaHi7L69lt3hmTEoJclqhsd7K3teKPiYyE91ozjkZkYdO7k7PsuWKcqGLzxUxnjnvqWx1cVBTsUgUAgEBwjosWpBxA78nxiR54PgGJvwrL6Naq/WIStbAv2yp3YK3fS8PMnRA06naih06j/4R2air7HnDuCmJPOJXbUpZjSC3zOKUkST10w1Kfsoy0VzH7tJwYkGlgxtD9ROh2byut4bNUuLh6WwRn9UjDpdQxOj+HnfXX8WGohJyGyW91lWeJ3o3LYWFaHTu55Ax9/2VfHN7uq+a6khkijjlsn9Dmu11dVlc+27WdEVvxRt9YdaLBx9Vs/89uRWVx4QmYXRygQCAQ9CzGrzg+qqqIoCrIsh9xsBVVVcdbsw165g4bNy6j+/DFURztLBEgycWN/S8o5f8aQ1Bt0+jZ9ig82kv/wcgAMOonMWDO7Dy18GW3ScfCBMzHqZa58YyMvrd/LvWf0Y8GZ/b3vb3a4MBt0XS/bzXRnPc/4v+/5dOt+AMx6mZoHzzyu9+iV9XuZ+8ZGJvVN4qtrx3jLO+M8938/8coPpe73/bPj2Z2hSCj/LHcXwlk4hyPd5Stm1XUhTqf/hSODhSRJGBJ7ETVwEmkXP0yfh38lbsxviBp8Bum/eYo+C38l46oXiRo6DVQFy7evsPPOArZeZWTrlQYqXp/PkTlzXlIkf5pSQFq0EYdLZXdNEwadxDmD0/jwylMw6t2Py+yTevH0+UOYdaK79UFRVP66bAcx9yzl92//3Oq8R0NJdSO9H/ySK9/Y2CXn64juqGeXovJt8eFlJZqdinch0ePFwuWFAKzYeRCb03dSQSDOqqqybEeV9/s9NT13u51Q/VnuToSzNtCac7B9ReLkB0VRKC4u7hGzFYyp+fT6w6v0vvMLEs+4AVPmQBLGX0nv2z8j9961RA2ecvhgxUX1549j+e5V97cOG/UbP6axcDX3T85lxaU5FP5xIp9dPYrK+6fywZWnMKlvsvftp/dL4fpxeQxKj6HB5mTa82u597PtOBWVF9bu4elvS3xie29TOY32zj3oK3ceZE9NE1v3N3j/qmiwObHanKzYWcWc//3U4Tkb7c6AZuN1Vz3/sq+OumYnMSY9lx8afP9l4YEuvYY/Gu1Ob4shwNrdh5O2QJ33N9hxtjhmeWGVn6NDl570s9xVCGdtoDXnUPAVY5w0QGSfUfS+cxmKvQnV0Uz1sqc48N59VLx6A4akbCrfvIvm4vUASKZo1NwxpF90L30HuDceVpobQJKRTa3HM+2uaeLLwiqMOpnzhqTz1s/7mP/hFk7sFctp+Ums3V3DBS//QE5CBGcNSKWkppG9tc3UNDqoszl4ePpAbjwtr9V5V+1yL5kwIT8JgHlvbOQ/P+zlrsl9eerbYu6a1BeT3t3lpSgqOw9aqay30Tshgiijnie/KWbRN7uIMurY8cfJRJuO/6P+9SGHcXmJTBuQwus/lbFsxwEePmtgp8+lKCoqdGqc1+fbD9B8aBudl2adyOC0mE5fNy3GxL77pnL+S+v56NdKlhdWMe+UHL/vcboU9DrxN5lAIAhPROKkIWRjBBgjSD7nT1i3fEnjjm/Y/bfJ7tci45BkPa6Gg7D9C/Y89AXmvJNRGmuxV7q7ewxJORjT+2PM6I8pYwBRQ6byya8SQzNieO6iEzi1dwI6WeL9zeVY7e5uIavNSU5CBHtqmnh2zW6feEx6mQtOSPd+P+d/P/H1roNYmpzUNrtbiib0cS/omRRpQFFh4fKdAKwqOsgfJ/cFJPou/Iri6ra7kF6bfZI3adq+v4F5b2wkxqTn1cuHBzRYusnhYktFPbVNDqb0Swn0VrtjPJQ4jc9P5PQCd4vdhlIL1Y12EiONAZ/H4VKY9K/VFFc3sf6W08iMM3f8JuD9ze5FU+dPyGfuKdmdir0lOlli/oR8vi2uJsZPArqnppE/frKNvbVNfH39GE2MtxAIBNpDJE4dIMvh95ezJOvI/MOr7PrzCShNdZjzTyHr+rcwJGbTWLKBPe/9DXXLh95WKA+Og3twHNyDdcuyQyeSuHzkRdxw3q2YM92tUU8Ns3D6j0/T/KqOpqvu4/R+J7P1zom8sHYPFfU2+iRF0jshkqQoA7FmA73iIrznr6y3UVJ9uGspNzGC8YdanEb1TvCWp8eYePXy4d5WjYLkKCrqm8mINbO3tgmHS2Vwegz3T+3HWQNSAXh/UzlX/G8j9TZ3996pT37Lfy47kZNz4tFL7nr+qczC4jV7mNY/lVN7x/PqhlIeW7WL6kYHyVFGDjwwDXC3qHxZWMWKnVUUHWykd0IE/VOimTEo1eujqipfH1podEKfJHrFRTAmN4G0GBO1TY5OJU6PrNjJd4fGRv1p6TZeuvTEgN739PlDmTkord2Wpo6ebavNSYRBhyxLnJafxIEHprXZ4lXb5CA+woBRJ/PupnJsToVvdlUzvk9SQHEeL8LxZ7kjhLM20JpzsH3FrDoN07x7I03F64kfNwdJ7/uL3HFwLw2bPseQlIO593CQJOzl27FVbMdevp3mkg1Yf11++A2SjC46CVe97xie6BPPJuakczHnnoQ+Ng3VacdZuw/rli+xbl2BYrMi6Y3oY1Ko6XMm1oIziYtLJC5CT5LcTN2XT1C/fgmMuIxBqwfhkg0s//3JjEm0o49LQ9Ib2V9vIzHSgF4n41JUDlrtJEcZkQ/9klcUlb99tZM/Ld3GuLxE9tU1s+ugu4XqmQuGct3YXAAWry7hunc2tbpPyVFGCpKjWHndGIx6GadLYeg/VrFtf4PPcQkRBt6dO5KJfZP5taKewY+uJMIgU/vgdO/A+pZ8sX0/6TFmZAne2LiPDzZXkB5j4qpROZw/NB2TXse2ynqG/fNr7K7D/fnrbzmNkdnxrNtTw6byepodLhyKe12uOLOegpQoBqfF+HSX/VpRz2fb9zM+P4mR2fEdPxzAHz/eyqsbSnlwev92u+e+313DZf/9kTP6JfPk+UO49YMtLF69mzMHpPDp70YBiJYngUAQ8nQmnxCJkx9UVcVqtRIVFaWZD//OODfv+YWqT/9Ow8+foDS6t2CRjJHEjZ6N6rRhWf1fUDs3gE8ymDGm98OQ0IvGorUo1sMDmu0pA3FmjSBm5+fuBE2SMSRlE33CWSSf/ScMib1QbI0079mIKXMguih3K1VRlZVTn/yW347I4u8zB1Lb5OCcf69n7e4azhuSzrtzR2K1WimsdfLaj2Us3bafXysbGJQWzZ+n9OOSEzN9Wlo+27afOz76ldG5CQxMjWZPbRNf7qhic0U9hkPb04zPT2JF0UFKa5u4ZkxuK8+fSi2c9PjX7d6Hd+aM5IITMrj+nU38a3UJ0wekkhRl4L8bypjcN5kvrzmV059dw4qdB9t8f5RRx8rrxniTpKve3Mi/1+3ljol9mNo/hcdX7WJc7xjuPH0AujbGI/13Qylz39iIS1F564oRXDzMPYNSVVUq620oKjz+9S4eW1WEokLvhAjW3XwaDXYnBQu/QlFhWv8Uzhmc7k1M26K2yUGsSe9Nctvioy0VLC+s4vKTenFKTkK7x/kjkOf6u+JqMmPN5CV1zdpkb/+8jye/Keah6QOC0vomPr+EczjSXb4iceoiXC4XhYWFFBQUoNP1vPWJjoajcVZVFWdtOY4DxZiyhni3gbGVb6f263/TVPIDzSU/ojTXI+mNyBGxRPY7jejBZ6BP6IXqtGPb+wuW7/+HvXy7z7mNmQOJO2UW1V8+5R5/5UGSoMWjKxnMRPY7jcbCb1HtTcjmGBLPuJG4cXNQnQ7sB/fS8MMS6n/6AH1cOimXPkZpyqnkVK2l6uO/0RiZSd95T2CMSURVFGr2bCU2NQd9pLubS7E307znJyJyR7RqnVNVFUvh9zy0dDPf10Xx/i3nkBgT5fee/XdDKU9+U0zRQSt1zU6m9U/h0uG9KDxg5dNtlay+cRyGQy1oT39bzPlD09HJEnd9vJW/njmAvKRIdhxo4Nolm0iMNKCTJRpsTmqaHGwqr6fB7qTmr2cSF2EA4LUNpfzm9Z+INeuptzlRVRiVEcF3t05Cp9Oxs8rK/A+20Cc5Er0s84+V7lXO54zM4t+zTkSWJfbWNHHqk9+yr67Zx+U3I3qx6NwhJEW578vl//2R//1UBkCEQab4T1NIizGx40AD/VKiUVWVL3dU8bevdvLVziqm9kvh8z+c2uoe1TU7uPn9Lby8fq/3XOX3TfU6BcprG0oZlROH82CZ97l2KSprSqoZd6grWFVVBj2ykm37G5gxMJUXZ51IWjtj4P71XQmyBKcXJFOQEo3TpfD0dyU8u7qEOyf15cpROby3qZyL/7MBl6ISH2Fg3c3jKEiJ7lTc4J5JurLoIKcXJBPRyfW/nE4nOwoL6d+vn/j8CmO05txdviJx6iK09kBCcJ1VVcVesQPHgWIcNWXoY5KJPnEmkqzDWV/FwaX/QLFZiTnpXKL6j8fVUE3T7p+o+ughmgq/855HNke7ZwJ2gClrCLbSzd7v9fGZxI39LXXrl+DYX4RsjiZ21GXoIuOo/eYlXA0HMWUNJfP3rxDReziqy0nDps848MFfad61znseyRRF3OjZJE6+FlPOMCRJwnFwD/U/fkBj4XfoYlIwpvUlqv8EzL1PRFFUnxYXVVVxVu/FunUFTcU/YMocSOzJF6OPTUFVXDjr9oPiAiR3AilJyMZIb8LqUlR2VlSRZ7BiSO6NJEmU1zWTuWCZ9xrDMmKYf1IssycMQ6fTsenTF/n23Wd5PWIG3xpHAHDjuDwWnTvYp8sz66/LKK9zL7Q6KieeP00p4OzBhwf4A2wqr2PUE98QZdSzZM4Ixucnce9n23loeSHj8hIpPGClov7wYq13TurD32e6t/1ptDs547m11NvcSynUNTuRJZhSkEJBShRPH1rtXlVVzn5xHWcPTuOy4b34YvsBXli7B5eqMvfkbC46IQOzQeezqGt2jIHxBSkMSovl1Q2lbD/QwObbJzIoPQZLk4OLXvmB5TurUFXIiDXxyqXDqW12UFlv44Zxh2d+Zj+wjFKLO3kcm5tAvc3FL+V1mPUyP98+gYLkKM5+cR2fbN1PrFlPXbOT/ilR/O83IxiWGduqde274mr21jYxfUBqq6Tw92//zAtr9zA0I4Ylc0bSL8Dka1VRFVe/9TNOh4NVN5xGdqJvIu90KWzcV8fGMgs7DliZ2j+l05MfAmXJz/vYVF7Pn88owNDNsy3FZ3b4O4vE6RgRiVPX0xOdVVXFuvkLbKWbiRw0GXP2MOp/+pCqjx7GVroJ2RSFHBFL1OAziB15IfUbP6Jm+TPuFiudgbjT5mH5+XOoaTHrT5JbdzN6Wrl0esw5J2Ir24Jqdw9ml4wR6OMycNaUojrtvm8zRniPO5LY0ZeTNPUWGjZ9hmX1f3FUl6I6mnxa0wCQdRiSc3FW7211fg/RJ84kcerNNO/+iYOfPoqr/gCG5FxiRpyPMa0vi9bso/hgI/OGJTAk2cBBNY68UWdS/dGD1H79b+95NqWcTmyv/hTsX4njQBGmrBOIyD+FuFMvoyhmKL/sq2NyQTIZsb6z+5wNB7GVbsacfQL77GYSIg1EYsdeuZOnPvuer34pxIaRWjkG1WDm4kGJXDIogVjZiQk7hoRMPqzP4bLXfwZVJVPZT2JiIot/M4Fx+UnuhVBVBaWpnm179nHmc9/RIEVil40kumrJcB1gvy6R3bpefPmHUzm9Xwo/ltZy03ubWbunFpfie08TIgz8e9Ywzhua4S379btPeOizzbxuHeiub9yr6O/58xTSY82oqsrtH/3Kz/vqWLGzyrsBdUKEgT+M7s1D0wcgyxLNDhfPfFfCpcMzGf3kt+ytdSdaP982gRMy3Z9VK3dWce/n2/lml3uR1EijjlnDMpk5KI3T8hNJiTZR3Wgn6S/uzb2jTTr+OLkvJ2bGsXZ3DdeM6e2djPDahlJ6J0TQ5FD46NdKnvq2GIDUSB0/3jaRXvHu7seaRjsvfr+Xp74rZk+LNb483bGKorKlsp6f99WRFWdmeK84bzKnqiouRfWOnVMUlc0V9dQ02TlodaCXJWLMejJjzfRLOdyNUllvY+xT3zIgNZq3rhiBSa9jdUk1sWY9wzI73qS8M7T3+WV3KnywpYJGu4vzh6YTa+5cq+XxRFVV6m3uNehSok0dtjR292d2daMds14m0th6LpmqqjQ7lU63htY2OVhTUs3I7HhSoju3FZRInI6R7k6cFEWhpKSE3NzcoI/iP15oxbmx6Hsafv6U+LFXoE/Jo7hwG9Gb/0fzrnXEjbqU2FMupqlkA7XfvIRisxI/5reY80+m4tUbqf/hHe955Mg4EiZdQ9K0W9HHpaEqCo07vqXmq39R98M74Dq0SKckEVkwjqih01Ca67GVbqbhl09bJ0jeE+uIyBtJRP4oGneuprn4h8OvSTLIOkB1v/9QQnFMSBLRw2a6Y1Jc7R4WfcJ0ok88m8btX9Nc8gP6uAyMmQNxHCjGuvUr93slGXPv4e4u2LItnYpNikmlNnU4Ufs3oa/fB5JMRJ9TMWUNoXnPRmx7NrabOHr4JWEcZ9/wDyL7nOL95V3baOO977dRYjOxuaKekdnxXD821+cXaPXyf1Hxn+sB2J48nj+45pGYnsOMQWncNC6P3om+Y5/2WZp57cdSmp0K14zu3e4vgJ9KLUxavJomh8J/Zg3hwj4m9PHpTH9hLZ9tO4BBJ9E7IZKdVVbvex6dOYjbJ7n3NSyva+ay//7IqkOzNHNc+zjFsYm4nME8d8sc1pU3M/6Z1TiPSAynD0jh/tGJjBzUl6KDjdz/+Q7e3VTuXdsrzqzn5Ox4RvVO4MHpAwD3eMC+C7/yOU9KtJEIgw67U+Gm0/K4+/QCr3+vB5bRFjkJEcwfn8/N4/NZv6eWc/69jop6G8MyY6my2ik71GI39+Rs7yzRmkY7X++q5qDVTqPDhUEnoZMkapoc7G+wM7FPEjMGpQHuxVxfWLsbnSzRJymKCIPMN7uq+ba4mma7k//+5iRmDEr3Hnvhyz94u5ijTTouPbEXaTEmEiMNzG+xf6TDpWDQyVhtTj7YUsHWygbSYkxEG/WckBnDSVnxgDtheHdTOTpJot7mpN7mOvSvkwabk/OGpDPx0KLBWyvruf7dTfx9xiDqbU7e31xBvc3J5L5JTOmX4v0DZG9NE1e9tZGVRQdxuNx1qZMlBqVFMyQ9lol9kvj96N4+z5+78VJlx67dxKdmsP2AlbW7a0iKNHLPFHc9WW1OVhQdRFFUXIc+b/KTIhmUFuO3BXBLRT0PfLGDt3/ZR4xJz9yTs5k/Pp/eiZE4XArPr9nNIyuL2FPTRJxZT05CBL85KYvrx+YSdWjZEkVRufS/G5h1YianZCfw4ZYK3t9cwcqigzgVleI/nU7uoZ+rB77Ywbb9DUwpSObiYZnU2RwsXr2bDaW1XHRCJleNyjl0zu75HSUSJ4Ggm1BVlcZtq3DWlmPOPQljWgFSOz+8ir0JpakOxd6ELiIWXXSiz+tNxRuo/N98Grd/TeSAiSRM+B0RfUcjGyORI+Pc624dwlZRiLOmDENKHobELCTZ9y8tW8UOqj97nNrv/oM+PoOUc/5MzMgLsG5ZTsMvS3FZq1EdzYCKbI4BWYdt90Zs+35FH59Brz/8l6hBk2nevZEDHz2EJOuIGX4O5t7Dad77C9ZNn1P73X/8JlUA+rh0nJYKnzJddBL6uHR00UkojmZcDQdR7U1IxghkY6T7X4OZ5tJNPpMB0OkPJ55HIBlMIOtRbdZD35vRJ/TCcWCXNxmVoxKIyB2BLiYF1eVAkvUYkntjTMlDMrq7rmSjGX18Jo07vmX/W3cdOrm7ZVEyxxI3ahYxJ87EkNwbV1MdSpPFXafN9cjmWPTxGUjGCBRrDa6mOvTxGRjT+uKyVNKw6TNsZb8S2X880pCZOLZ/Re379+LYX4TpxPO43XYByfkn8OcpBeQkRLB6226+++ojtldYuOa0AvqaGmje/RPOmjIiTziLN6Vx1K/5L1O3/xOTeqirU2dAlzWMlY4+/FwfwWBdOblqJSl5Q+h/6nQMSTk4LRVUlO/lkaU/E6E2kxOpMDJNT9/kKCIy+mHKHEhEn1PRxySjqir9F37Fifoyciq+ZkjdOpzIvG2exnLjaM4clMHHh2ZLbiyzcMZza0mMNJAUacSlqtQ3O9h1sBGbS+WRmQO5Y1JfAL7ZdZCZL66jrtldnwkRBqx2F89cMITfnepOBpZtP8DU59e2+2wtvnCod6LFe5vKueDlH9o99svz4zklpoGogRN58cf9/P7tX0iPNhJn1rO96vCab/lJkRTdc7r3+2nPraWi3kbRQat3LToPV52Sw//NGgbAj6W1jHj8m3av39L9/JfWe9dUO5JTcuL5/ubTAPdenxkLlnl3O9DLkk8y3DLJtDldRN29tFUrqofbJ/bh0bPd3d87q6wUHJEMAxh1MqnRRm4cl8edk92xFlVZGfjICgBv8taStTeNY1TvBBRFZcg/VrK1su0hESV/Op3eiZG8tG4PV775c5vHjM1N4Nsbx3m/P+PZNXx5aGeCSKM7Uff4/2F0b5696IQ2z9NViMSpi1BVFYvFQlxcnCZmK4BwDoazqrhaJULdfs1DztFGGZ05ut3kryX2yp1UffQw9qoSIvudRmTBWFz1B7Dt24ocGUfMSedhSi/AUV1G445vkI0RmPNOxpCQGVhMTgfWrSto3vMT5pzhRPYbh6vhIA2/fIr9QAnmnGFE5J2MPjEL2eBu3VEVlzsJM7m7hmz7tlH18ULqvn+jw5aptkiaeTdxoy9n3/9d2Wodsy5HkjDnjsCUMRBXkwXr5s/b36gb3y5fU9ZQnPX7cVkquy6W3idhSMyisfA7XPWtt9ZRopIxGIwoDVVIpiiMqX0wJGYj6QwgSTiqdmMr34aquKjvM43k0bNIjY/FWVeJ0mihrLqO73dV0jfBSL9EAzabDQNOZHsDjqrdWCqKsdbVYFJtqJKOqogc9kdkIxsjiNDrGJCVSlp2X/RxaRysLOXnX38ltm4XUTU7MNtrOZgxhphh0zEXLkNf6G4JkyPjOJh3JlLjQRL3/4jqaMaWOpidhnyaDbEYI6IYnyFjr9yJw1LBylIHtUSxR5dBdcJAeuf3x9FQg7O+iuHxTqZm63A1HKS0rJTivXuJcdUR66rDZohhf/xg6hL6YTCaGZwWQ36UE2ddJXVV5RSWlNBsOUCNMQVj+gCcCbmsr5Zp1EXz9LkDUQ/t6rBxdyWpZtzLq0gqNVXlHCgrobGhjqjkLAr69kOfmE2ZmsjsN38hyXmQBMWCWVYx6WV6mewMMDcwJFEmo+8JmLOHcdBl5C8f/YxJtWHCgc5lo8LSRJPDhYTKmQNSuPSENJyWCg6W7+bNDbtplkw0SSbyM1KYNDCLepvCtv31nDvY3aqOqvBrhYW6JgdDsxKxytFsr9fx7uYKquqbODE9irsm5uFotrJi/Y/sKd6OVYpATsqnf1YqQ6W9RFTvdA9xiE1DF53ETouT8kb4saKZskYVG0ZyUxPon5nMmIJ0eiXGo4uIJaJgTLd8XovEqYvoieN9jhXhLJzDAdVpp7l0s3s2p60BVdJxoKKMOKw4D+5GdboTFKW5AWftPhSblaTpt5F01p1IkoSquLBuWU79xo9o+GUpqs2KHBF7+MsUjdJUh9NSjmJrRB+dhGSOxllThqNqN5IxgqiBkzFlDcH6y1Ka92xENseQdNYdRA89k6pP/kb9D++2ituYVuBuIXPa0EXGY8o5EV1ELLXfvISjqgR0BtIuXkjitFsPJSslNO1cQ2PhavfkhcyBGFLyaN6zEevWldgsB4hIzkYfl+aNWzZFIZujUV1O7BXbsZVudneptsAd/ySih83Aaamg5qtnW63RFtJIkrv1s7Y82JEIuhBDcm/yHykK+hgnsXK4QCAIOyS9kYjck4jIPQlwJ4pVhYWkBfhhK8k6oodOJXro1E5fW3XaQZKRdIc+Xi9+GEd1KXJEHLoI9/IW2Te+g33/Lpr3/Iy9fBuqqhBz0rmYeg1u86/o5HP+hHXrCgwJvTBlHt7r0JiShzElj7jRl/u+YexvvclxTgDOjtpyrJuX4ao/QETf0UTkjfRZdiN55j00Fa9HNkW5u1yb6rDvL8JZuw8UF6qqYIjPxJgxAKW5Hsva/2Hd9BmSIcLdTRsZj2QwIemNSDqj+9w6g3t5EmMkhqQcDMm90UUlIhsjURxN2Mu3Y9+/E9XpAElCabLgOLgHp6USfWyq+z2pfTBnDUUyRdLw44c0bFlGU0QaebP+ijlzAI3bVlK/8WMMiVlEFoxDjoihefdP2Mq2oDQ3oNgbkc0xGNP6oo/LQGmux9VwENu+X93dpLXl6KKT3F8xyeijk73/d5cno4tOxFlbTnPJBuwVO1BVd4uMLiIOXWwq+thUdDGp6KLicRzci718K46De3E1HMTVWIukMxzqto5AMpiR9CbvxAR9dBL6xGxkT1JeXYqzuhRH9V5UlwN9Qi90salYG5uJjo5GF3Go+9hgxla2BVvpZlSnHdlgdp/bGOGuB0nnvQaShCTr0MemoY/PRDKYUeyNqDYrir0RxWZ1d3/LMpIku8dYSrL7OZVkVKcNl7UGV5MFCQlknbsFXdYh6Y0YknpjSM5FsTXgqNyJq7keU6/BmLOGoLqcOC0VuKw1qE4bqqMZ1dGMcuhf1Wlz/9/u/l4fn97W43vcEYmTQCAQdCFHrvMFYEjMalVmTM3HmJof2DllHdGDpxxzbO1hiM8gftwV7b4uG81E9T/Np8ycPbTd4yMLxhxzTOasIZ06PqL3cBLP+TOFhYUYM9xjD6MGTSZq0GSf41omnl1J7IjzuuW8HeFJkHuFaevxkbhc/sdZHg/Cd9pUFyBJkmZWY/UgnLWB1py15gvCWStozTkUfMUYJ4FAIBAIBJqmM/lESLQ4PfPMM+Tm5mI2mxk1ahTr1q3r+E3HAUVRqKqqQlGOcY2cHoRw1gZac9aaLwhnraA151DwDXri9OabbzJ//nzuu+8+fvzxR4YNG8a0adPYv39/sENDVVWqqqrowY1ynUY4awOtOWvNF4SzVtCacyj4Bj1xeuyxx7j66quZN28egwYN4tlnnyUyMpJ///vfHb9ZIBAIBAKB4DgS1MTJbrezYcMGpkw5PFtElmWmTJnCmjVrghiZQCAQCAQCQWuCuhxBVVUVLpeLtLQ0n/K0tDS2bdvW6nibzYbNdnhl3bq6OsA9PdEzRVGSJGRZRlEUn6a89spl2b0eRXvlMTExPn2pnr1xjuxfba9cp9OhqqpPuSeW9soDjf1onY6cztkydkVRvM7h4tRRuaIoxMW5NxtteZ6e7AT+6wnwebbDwclfPbV8rsPFqaPYFUUhNjbWb+w9zclfub/P7J7u5K+eAvnM7mlO/mL3+AKtYjwWp84sc9Cj1nFauHAhCxYsaFVeVFREdHQ0AHFxcWRkZFBZWYnFYvEek5ycTHJyMmVlZVithzfTTE9PJz4+npKSEuz2w1s0ZGVlER0djdVqpaioyFuel5eHXq+nsLDQJ4aCggKcTifFxcXeMlmW6devH1arldLSUm+50WgkPz8fi8VCRcXhPYyioqLIzs6murqaqqrDWx50tVNRUZHPw9OWU319fdg5gf96amhoCDun9uqpvr7e+xUuToHUU319fdg5gf96kmWZvXv3hpWTv3qy2Ww+n9nh4BRIPdXX14edE7RfT+np6djt9i5zKikpIVCCuhyB3W4nMjKSJUuWcN5553nL58yZQ21tLR988IHP8W21OHmkPdMHuzL7VVWV8vJyUlNTvVlvuGf0iqKwf/9+UlNT0ev13vKe7NRRuWeWRmpqqs+xPdkJ/NeTy+WioqLC+2yHg1NHf016nmudThcWToG0OB04cID0dPdqy+Hg5K/c32d2T3YKpMWpo8/snubkL3aPb3p6uvf8XeFksVhITEwM/S1XjEYjI0aMYPny5d7ESVEUli9fzg033NDqeJPJhMlkalWu0+larZjqudlH0plyRVGor68nPT291fnbW6G1rXJJkjpV3hWx+yvvKHaPs2eBsXBw6qjcYrF4f6keSU91gvZjB9p8tnuyk796UlXV63vkH0GBxh5qToHEWFdXR1paWrux9EQnf+Vd9ZndXnmofu4F8pndXnmoOvkrb+nbVU6dWXU96F118+fPZ86cOYwcOZJTTjmFRYsWYbVamTdvXrBDEwgEAoFAIPAh6InTrFmzOHDgAPfeey8VFRWceOKJfPbZZ60GjLeFp5nPM0i8q3G5XDQ0NFBXV6eJPYBAOAvn8ERrviCchXN40l2+njwikNFLPXrLldLSUrKzs4MdhkAgEAgEgjBg7969ZGW13pS7JT06cVIUhX379hETE+Pt2+1KPIPP9+7dq5m98ISzcA5HtOYLwlk4hyfd5esZB5mZmdnu+CsPQe+qOxZkWe4wM+wKYmNjNfFAtkQ4awOtOWvNF4SzVtCac3f4etbz64igb7kiEAgEAoFA0FMQiZNAIBAIBAJBgIjEyQ8mk4n77ruvzbWjwhXhrA205qw1XxDOWkFrzqHg26MHhwsEAoFAIBAcT0SLk0AgEAgEAkGAiMRJIBAIBAKBIEBE4iQQCAQCgUAQICJx8sMzzzxDbm4uZrOZUaNGsW7dumCH1CUsXLiQk08+mZiYGFJTUznvvPPYvn27zzETJ05EkiSfr2uuuSZIER87999/fyufAQMGeF9vbm7m+uuvJykpiejoaC688EIqKyuDGPGxk5ub28pZkiSuv/56IDzq+Ouvv+bss88mMzMTSZJ4//33fV5XVZV7772XjIwMIiIimDJlCoWFhT7HVFdXM3v2bGJjY4mPj+eqq66ioaHhOFoEjj9fh8PBXXfdxdChQ4mKiiIzM5MrrriCffv2+Zyjrefib3/723E2CZyO6nju3LmtfM4880yfY3pSHUPHzm39XEuSxKOPPuo9pifVcyC/kwL5jN6zZw8zZswgMjKS1NRU7rjjDpxOZ5fHKxKndnjzzTeZP38+9913Hz/++CPDhg1j2rRp7N+/P9ihHTOrVq3i+uuvZ+3atSxbtgyHw8HUqVOxWq0+x1199dWUl5d7vx555JEgRdw1DB482Mfn22+/9b5266238tFHH/H222+zatUq9u3bxwUXXBDEaI+d9evX+/guW7YMgIsvvth7TE+vY6vVyrBhw3jmmWfafP2RRx7hySef5Nlnn+X/27v3mKbuNg7g3xahFibXAi0zMEDGcFyiOJvGzWVChM5E5lhU1my4GwOBsUwXwjbmJdk0MYE/9gfZEi4mGsxYRI1Ojdx2gYqKVFCxEYKQbVQmpggicunz/uHLeXcGQl9XKGXPJ2ly+vv9Tnl+POfy0HNKGxsb4ebmhoSEBAwPDwtjdDodrl27hnPnzuHkyZP4+eefkZaWNldT+L9MN9+hoSFcvnwZ+fn5uHz5Mo4ePQqj0YiNGzdOGrt3715R3rOzs+ci/CcyU44BIDExUTSf8vJyUb8j5RiYec5/nWtPTw9KSkogkUiQnJwsGucoebbmnDTTMXp8fBwbNmzAyMgIGhoacPDgQZSVleHLL7+0fcDEprR69WrKzMwUno+Pj1NAQADt27fPjlHNjt7eXgJAP/30k9D28ssvU05Ojv2CsrFdu3ZRTEzMlH1ms5mcnZ2poqJCaGtrayMApNfr5yjC2ZeTk0OhoaFksViIaOHlGABVVlYKzy0WCymVSjpw4IDQZjabSSaTUXl5ORERXb9+nQDQxYsXhTGnT58miURCv//++5zF/iT+Pt+pXLhwgQBQV1eX0BYUFESFhYWzG9wsmWrOqamplJSU9Nh1HDnHRNblOSkpidatWydqc+Q8//2cZM0x+scffySpVEomk0kYU1RURO7u7vTw4UObxsfvOE1hZGQETU1NiI+PF9qkUini4+Oh1+vtGNns6O/vBwB4e3uL2g8fPgyFQoHIyEjk5eVhaGjIHuHZzM2bNxEQEICQkBDodDp0d3cDAJqamjA6OirK93PPPYfAwMAFk++RkREcOnQI7777ruh7HRdajv+qs7MTJpNJlFcPDw+o1Wohr3q9Hp6enli1apUwJj4+HlKpFI2NjXMes6319/dDIpHA09NT1L5//374+PhgxYoVOHDgwKxczphLdXV18PPzQ3h4ODIyMtDX1yf0LfQc3759G6dOncJ77703qc9R8/z3c5I1x2i9Xo+oqCj4+/sLYxISEnDv3j1cu3bNpvE59HfVzZY7d+5gfHxclAAA8Pf3x40bN+wU1eywWCz4+OOPsWbNGkRGRgrtb775JoKCghAQEICWlhbk5ubCaDTi6NGjdoz2yanVapSVlSE8PBw9PT3Ys2cPXnrpJVy9ehUmkwkuLi6TTi7+/v4wmUz2CdjGjh07BrPZjG3btgltCy3HfzeRu6n244k+k8kEPz8/Uf+iRYvg7e3t8LkfHh5Gbm4uUlJSRN/p9dFHH2HlypXw9vZGQ0MD8vLy0NPTg4KCAjtG++QSExPx+uuvIzg4GB0dHfjss8+g1Wqh1+vh5OS0oHMMAAcPHsSSJUsm3VrgqHme6pxkzTHaZDJNua9P9NkSF07/cpmZmbh69arofh8Aouv/UVFRUKlUiIuLQ0dHB0JDQ+c6zH9Mq9UKy9HR0VCr1QgKCsL3338PuVxux8jmRnFxMbRaLQICAoS2hZZj9j+jo6PYvHkziAhFRUWivk8++URYjo6OhouLCz788EPs27fPIf/79NatW4XlqKgoREdHIzQ0FHV1dYiLi7NjZHOjpKQEOp0OixcvFrU7ap4fd06aT/hS3RQUCgWcnJwm3bF/+/ZtKJVKO0Vle1lZWTh58iRqa2uxdOnSaceq1WoAQHt7+1yENus8PT3x7LPPor29HUqlEiMjIzCbzaIxCyXfXV1dqKqqwvvvvz/tuIWW44ncTbcfK5XKSR/4GBsbw927dx029xNFU1dXF86dOzfjN8ir1WqMjY3h1q1bcxPgLAsJCYFCoRC244WY4wm//PILjEbjjPs24Bh5ftw5yZpjtFKpnHJfn+izJS6cpuDi4oLY2FhUV1cLbRaLBdXV1dBoNHaMzDaICFlZWaisrERNTQ2Cg4NnXMdgMAAAVCrVLEc3NwYHB9HR0QGVSoXY2Fg4OzuL8m00GtHd3b0g8l1aWgo/Pz9s2LBh2nELLcfBwcFQKpWivN67dw+NjY1CXjUaDcxmM5qamoQxNTU1sFgsQiHpSCaKpps3b6Kqqgo+Pj4zrmMwGCCVSiddznJUv/32G/r6+oTteKHl+K+Ki4sRGxuLmJiYGcfO5zzPdE6y5hit0WjQ2toqKpIn/nBYvny5zQNmUzhy5AjJZDIqKyuj69evU1paGnl6eoru2HdUGRkZ5OHhQXV1ddTT0yM8hoaGiIiovb2d9u7dS5cuXaLOzk46fvw4hYSE0Nq1a+0c+ZPbsWMH1dXVUWdnJ9XX11N8fDwpFArq7e0lIqL09HQKDAykmpoaunTpEmk0GtJoNHaO+p8bHx+nwMBAys3NFbUvlBwPDAxQc3MzNTc3EwAqKCig5uZm4VNk+/fvJ09PTzp+/Di1tLRQUlISBQcH04MHD4TXSExMpBUrVlBjYyP9+uuvFBYWRikpKfaa0rSmm+/IyAht3LiRli5dSgaDQbRvT3yqqKGhgQoLC8lgMFBHRwcdOnSIfH196e2337bzzB5vujkPDAzQzp07Sa/XU2dnJ1VVVdHKlSspLCyMhoeHhddwpBwTzbxdExH19/eTq6srFRUVTVrf0fI80zmJaOZj9NjYGEVGRtL69evJYDDQmTNnyNfXl/Ly8mweLxdO0/jmm28oMDCQXFxcaPXq1XT+/Hl7h2QTAKZ8lJaWEhFRd3c3rV27lry9vUkmk9GyZcvo008/pf7+fvsG/g9s2bKFVCoVubi40NNPP01btmyh9vZ2of/Bgwe0fft28vLyIldXV9q0aRP19PTYMWLbOHv2LAEgo9Eoal8oOa6trZ1yW05NTSWiR/+SID8/n/z9/Ukmk1FcXNyk30VfXx+lpKTQU089Re7u7vTOO+/QwMCAHWYzs+nm29nZ+dh9u7a2loiImpqaSK1Wk4eHBy1evJgiIiLo66+/FhUZ8810cx4aGqL169eTr68vOTs7U1BQEH3wwQeT/sB1pBwTzbxdExF9++23JJfLyWw2T1rf0fI80zmJyLpj9K1bt0ir1ZJcLieFQkE7duyg0dFRm8cr+W/QjDHGGGNsBnyPE2OMMcaYlbhwYowxxhizEhdOjDHGGGNW4sKJMcYYY8xKXDgxxhhjjFmJCyfGGGOMMStx4cQYY4wxZiUunBhjjDHGrMSFE2OMTUEikeDYsWP2DoMxNs9w4cQYm3e2bdsGiUQy6ZGYmGjv0Bhj/3KL7B0AY4xNJTExEaWlpaI2mUxmp2gYY+wRfseJMTYvyWQyKJVK0cPLywvAo8toRUVF0Gq1kMvlCAkJwQ8//CBav7W1FevWrYNcLoePjw/S0tIwODgoGlNSUoLnn38eMpkMKpUKWVlZov47d+5g06ZNcHV1RVhYGE6cODG7k2aMzXtcODHGHFJ+fj6Sk5Nx5coV6HQ6bN26FW1tbQCA+/fvIyEhAV5eXrh48SIqKipQVVUlKoyKioqQmZmJtLQ0tLa24sSJE1i2bJnoZ+zZswebN29GS0sLXn31Veh0Oty9e3dO58kYm2eIMcbmmdTUVHJyciI3NzfR46uvviIiIgCUnp4uWketVlNGRgYREX333Xfk5eVFg4ODQv+pU6dIKpWSyWQiIqKAgAD6/PPPHxsDAPriiy+E54ODgwSATp8+bbN5MsYcD9/jxBibl1555RUUFRWJ2ry9vYVljUYj6tNoNDAYDACAtrY2xMTEwM3NTehfs2YNLBYLjEYjJBIJ/vjjD8TFxU0bQ3R0tLDs5uYGd3d39Pb2PumUGGMLABdOjLF5yc3NbdKlM1uRy+VWjXN2dhY9l0gksFgssxESY8xB8D1OjDGHdP78+UnPIyIiAAARERG4cuUK7t+/L/TX19dDKpUiPDwcS5YswTPPPIPq6uo5jZkx5vj4HSfG2Lz08OFDmEwmUduiRYugUCgAABUVFVi1ahVefPFFHD58GBcuXEBxcTEAQKfTYdeuXUhNTcXu3bvx559/Ijs7G2+99Rb8/f0BALt370Z6ejr8/Pyg1WoxMDCA+vp6ZGdnz+1EGWMOhQsnxti8dObMGahUKlFbeHg4bty4AeDRJ96OHDmC7du3Q6VSoby8HMuXLwcAuLq64uzZs8jJycELL7wAV1dXJCcno6CgQHit1NRUDA8Po7CwEDt37oRCocAbb7wxdxNkjDkkCRGRvYNgjLH/h0QiQWVlJV577TV7h8IY+5fhe5wYY4wxxqzEhRNjjDHGmJX4HifGmMPhOwwYY/bC7zgxxhhjjFmJCyfGGGOMMStx4cQYY4wxZiUunBhjjDHGrMSFE2OMMcaYlbhwYowxxhizEhdOjDHGGGNW4sKJMcYYY8xKXDgxxhhjjFnpPw0fbqA3aH7HAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 色盲安全配色\n",
    "colors = {\n",
    "    'blue': '#0072B2',\n",
    "    'orange': '#D55E00'\n",
    "}\n",
    "\n",
    "# ----- Train Loss -----\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_losses[:200], label='Without PP-STD', color=colors['blue'], linestyle='--')\n",
    "plt.plot(train_losses1[:200], label='With PP-STD', color=colors['orange'], linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Loss')\n",
    "plt.title('Train Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Validation Loss -----\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(val_losses[:200], label='Without PP-STD', color=colors['blue'], linestyle='--')\n",
    "plt.plot(val_losses1[:200], label='With PP-STD', color=colors['orange'], linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Loss')\n",
    "plt.title('Validation Loss Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Train Accuracy -----\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(train_accuracies[:200], label='Without PP-STD', color=colors['blue'], linestyle='--')\n",
    "plt.plot(train_accuracies1[:200], label='With PP-STD', color=colors['orange'], linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Train Accuracy (%)')\n",
    "plt.title('Train Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ----- Validation Accuracy -----\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(val_accuracies[:200], label='Without PP-STD', color=colors['blue'], linestyle='--')\n",
    "plt.plot(val_accuracies1[:200], label='With PP-STD', color=colors['orange'], linestyle='-')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy (%)')\n",
    "plt.title('Validation Accuracy Comparison')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'val_accuracies1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[412], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mval_accuracies1\u001b[49m[:\u001b[38;5;241m200\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'val_accuracies1' is not defined"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 動態調整"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 4.3049 (Cls: 3.6159, Reg: 6.8894) | Train Acc: 7.10% || Val Loss: 4.7610 (Cls: 2.3101, Reg: 4.9019) | Val Acc: 34.10%\n",
      "✅ 儲存最佳模型 (Val Loss: 4.7610) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [2/300] | Train Loss: 4.7523 (Cls: 2.5035, Reg: 4.4975) | Train Acc: 21.13% || Val Loss: 2.7650 (Cls: 1.4652, Reg: 2.5995) | Val Acc: 60.86%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.7650) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [3/300] | Train Loss: 3.5975 (Cls: 1.9118, Reg: 3.3713) | Train Acc: 37.73% || Val Loss: 2.1164 (Cls: 0.9867, Reg: 2.2595) | Val Acc: 77.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 2.1164) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [4/300] | Train Loss: 3.0337 (Cls: 1.5650, Reg: 2.9375) | Train Acc: 48.55% || Val Loss: 1.5490 (Cls: 0.6672, Reg: 1.7636) | Val Acc: 86.88%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.5490) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [5/300] | Train Loss: 2.6419 (Cls: 1.3366, Reg: 2.6107) | Train Acc: 56.41% || Val Loss: 1.4774 (Cls: 0.5658, Reg: 1.8232) | Val Acc: 91.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.4774) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [6/300] | Train Loss: 2.3948 (Cls: 1.2017, Reg: 2.3862) | Train Acc: 61.14% || Val Loss: 1.2649 (Cls: 0.4567, Reg: 1.6164) | Val Acc: 91.92%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.2649) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [7/300] | Train Loss: 2.2150 (Cls: 1.0958, Reg: 2.2384) | Train Acc: 64.23% || Val Loss: 1.0145 (Cls: 0.3540, Reg: 1.3210) | Val Acc: 93.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.0145) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [8/300] | Train Loss: 2.0425 (Cls: 1.0069, Reg: 2.0710) | Train Acc: 66.70% || Val Loss: 0.9110 (Cls: 0.3337, Reg: 1.1546) | Val Acc: 92.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.9110) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [9/300] | Train Loss: 1.9685 (Cls: 0.9701, Reg: 1.9968) | Train Acc: 68.79% || Val Loss: 0.7491 (Cls: 0.2795, Reg: 0.9391) | Val Acc: 94.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7491) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [10/300] | Train Loss: 1.8917 (Cls: 0.9216, Reg: 1.9402) | Train Acc: 70.29% || Val Loss: 0.8178 (Cls: 0.2788, Reg: 1.0780) | Val Acc: 94.06%\n",
      "Epoch [11/300] | Train Loss: 1.8292 (Cls: 0.8972, Reg: 1.8640) | Train Acc: 71.30% || Val Loss: 0.6821 (Cls: 0.2255, Reg: 0.9132) | Val Acc: 97.35%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6821) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [12/300] | Train Loss: 1.7563 (Cls: 0.8504, Reg: 1.8117) | Train Acc: 72.33% || Val Loss: 0.6657 (Cls: 0.2154, Reg: 0.9005) | Val Acc: 95.14%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6657) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [13/300] | Train Loss: 1.6801 (Cls: 0.8158, Reg: 1.7286) | Train Acc: 73.77% || Val Loss: 0.6146 (Cls: 0.1886, Reg: 0.8520) | Val Acc: 96.86%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6146) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [14/300] | Train Loss: 1.6260 (Cls: 0.7854, Reg: 1.6812) | Train Acc: 74.71% || Val Loss: 0.5643 (Cls: 0.1892, Reg: 0.7503) | Val Acc: 97.67%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5643) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [15/300] | Train Loss: 1.6003 (Cls: 0.7788, Reg: 1.6431) | Train Acc: 74.92% || Val Loss: 0.5942 (Cls: 0.1831, Reg: 0.8222) | Val Acc: 97.27%\n",
      "Epoch [16/300] | Train Loss: 1.5646 (Cls: 0.7603, Reg: 1.6085) | Train Acc: 75.88% || Val Loss: 0.4926 (Cls: 0.1658, Reg: 0.6548) | Val Acc: 96.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4926) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [17/300] | Train Loss: 1.5696 (Cls: 0.7678, Reg: 1.6037) | Train Acc: 75.55% || Val Loss: 0.5238 (Cls: 0.1587, Reg: 0.7301) | Val Acc: 97.82%\n",
      "Epoch [18/300] | Train Loss: 1.5401 (Cls: 0.7477, Reg: 1.5849) | Train Acc: 76.97% || Val Loss: 0.4860 (Cls: 0.1490, Reg: 0.6747) | Val Acc: 97.29%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4860) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [19/300] | Train Loss: 1.4921 (Cls: 0.7199, Reg: 1.5444) | Train Acc: 77.48% || Val Loss: 0.4937 (Cls: 0.1363, Reg: 0.7149) | Val Acc: 98.18%\n",
      "Epoch [20/300] | Train Loss: 1.4970 (Cls: 0.7255, Reg: 1.5430) | Train Acc: 77.25% || Val Loss: 0.4285 (Cls: 0.1351, Reg: 0.5876) | Val Acc: 96.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4285) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [21/300] | Train Loss: 1.4459 (Cls: 0.6980, Reg: 1.4958) | Train Acc: 78.02% || Val Loss: 0.4299 (Cls: 0.1270, Reg: 0.6060) | Val Acc: 98.73%\n",
      "Epoch [22/300] | Train Loss: 1.4307 (Cls: 0.6938, Reg: 1.4739) | Train Acc: 77.98% || Val Loss: 0.4025 (Cls: 0.1204, Reg: 0.5648) | Val Acc: 98.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4025) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [23/300] | Train Loss: 1.4323 (Cls: 0.6899, Reg: 1.4849) | Train Acc: 78.33% || Val Loss: 0.4157 (Cls: 0.1246, Reg: 0.5829) | Val Acc: 96.98%\n",
      "Epoch [24/300] | Train Loss: 1.3942 (Cls: 0.6738, Reg: 1.4406) | Train Acc: 79.11% || Val Loss: 0.4008 (Cls: 0.1165, Reg: 0.5691) | Val Acc: 98.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4008) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [25/300] | Train Loss: 1.4096 (Cls: 0.6849, Reg: 1.4495) | Train Acc: 78.84% || Val Loss: 0.4061 (Cls: 0.1048, Reg: 0.6026) | Val Acc: 98.82%\n",
      "Epoch [26/300] | Train Loss: 1.3947 (Cls: 0.6716, Reg: 1.4460) | Train Acc: 79.31% || Val Loss: 0.3642 (Cls: 0.0983, Reg: 0.5322) | Val Acc: 98.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3642) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [27/300] | Train Loss: 1.3825 (Cls: 0.6614, Reg: 1.4423) | Train Acc: 79.72% || Val Loss: 0.3473 (Cls: 0.0994, Reg: 0.4970) | Val Acc: 98.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3473) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [28/300] | Train Loss: 1.3662 (Cls: 0.6554, Reg: 1.4216) | Train Acc: 79.92% || Val Loss: 0.3648 (Cls: 0.0987, Reg: 0.5330) | Val Acc: 98.84%\n",
      "Epoch [29/300] | Train Loss: 1.3423 (Cls: 0.6417, Reg: 1.4012) | Train Acc: 80.55% || Val Loss: 0.3664 (Cls: 0.1000, Reg: 0.5333) | Val Acc: 98.88%\n",
      "Epoch [30/300] | Train Loss: 1.3596 (Cls: 0.6525, Reg: 1.4141) | Train Acc: 79.76% || Val Loss: 0.3451 (Cls: 0.0954, Reg: 0.5005) | Val Acc: 98.94%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3451) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [31/300] | Train Loss: 1.3365 (Cls: 0.6432, Reg: 1.3865) | Train Acc: 80.45% || Val Loss: 0.3266 (Cls: 0.0862, Reg: 0.4821) | Val Acc: 98.80%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3266) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [32/300] | Train Loss: 1.3518 (Cls: 0.6466, Reg: 1.4104) | Train Acc: 79.90% || Val Loss: 0.3343 (Cls: 0.0959, Reg: 0.4781) | Val Acc: 98.80%\n",
      "Epoch [33/300] | Train Loss: 1.3273 (Cls: 0.6315, Reg: 1.3914) | Train Acc: 80.45% || Val Loss: 0.3172 (Cls: 0.0911, Reg: 0.4534) | Val Acc: 98.96%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3172) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [34/300] | Train Loss: 1.2983 (Cls: 0.6162, Reg: 1.3642) | Train Acc: 80.84% || Val Loss: 0.3227 (Cls: 0.0927, Reg: 0.4620) | Val Acc: 98.51%\n",
      "Epoch [35/300] | Train Loss: 1.3182 (Cls: 0.6296, Reg: 1.3773) | Train Acc: 81.05% || Val Loss: 0.2787 (Cls: 0.0865, Reg: 0.3867) | Val Acc: 98.90%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2787) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [36/300] | Train Loss: 1.2834 (Cls: 0.6054, Reg: 1.3561) | Train Acc: 81.49% || Val Loss: 0.3077 (Cls: 0.0783, Reg: 0.4605) | Val Acc: 99.39%\n",
      "Epoch [37/300] | Train Loss: 1.2914 (Cls: 0.6195, Reg: 1.3439) | Train Acc: 81.28% || Val Loss: 0.2987 (Cls: 0.0878, Reg: 0.4230) | Val Acc: 99.04%\n",
      "Epoch [38/300] | Train Loss: 1.2835 (Cls: 0.6145, Reg: 1.3379) | Train Acc: 81.32% || Val Loss: 0.3082 (Cls: 0.0804, Reg: 0.4558) | Val Acc: 99.04%\n",
      "Epoch [39/300] | Train Loss: 1.2818 (Cls: 0.6195, Reg: 1.3245) | Train Acc: 81.43% || Val Loss: 0.3262 (Cls: 0.0906, Reg: 0.4725) | Val Acc: 98.88%\n",
      "Epoch [40/300] | Train Loss: 1.2516 (Cls: 0.5912, Reg: 1.3209) | Train Acc: 82.33% || Val Loss: 0.2909 (Cls: 0.0763, Reg: 0.4308) | Val Acc: 98.98%\n",
      "Epoch [41/300] | Train Loss: 1.2718 (Cls: 0.6016, Reg: 1.3404) | Train Acc: 81.83% || Val Loss: 0.2800 (Cls: 0.0758, Reg: 0.4101) | Val Acc: 98.92%\n",
      "Epoch [42/300] | Train Loss: 1.2390 (Cls: 0.5818, Reg: 1.3145) | Train Acc: 82.31% || Val Loss: 0.2904 (Cls: 0.0818, Reg: 0.4191) | Val Acc: 98.76%\n",
      "Epoch [43/300] | Train Loss: 1.2496 (Cls: 0.5950, Reg: 1.3091) | Train Acc: 82.26% || Val Loss: 0.2603 (Cls: 0.0736, Reg: 0.3756) | Val Acc: 99.00%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2603) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [44/300] | Train Loss: 1.2345 (Cls: 0.5779, Reg: 1.3132) | Train Acc: 82.25% || Val Loss: 0.2769 (Cls: 0.0693, Reg: 0.4162) | Val Acc: 99.02%\n",
      "Epoch [45/300] | Train Loss: 1.2221 (Cls: 0.5814, Reg: 1.2813) | Train Acc: 82.40% || Val Loss: 0.2896 (Cls: 0.0754, Reg: 0.4289) | Val Acc: 98.98%\n",
      "Epoch [46/300] | Train Loss: 1.2345 (Cls: 0.5797, Reg: 1.3096) | Train Acc: 82.85% || Val Loss: 0.2904 (Cls: 0.0768, Reg: 0.4283) | Val Acc: 99.06%\n",
      "Epoch [47/300] | Train Loss: 1.2505 (Cls: 0.5914, Reg: 1.3183) | Train Acc: 82.48% || Val Loss: 0.2872 (Cls: 0.0713, Reg: 0.4327) | Val Acc: 99.10%\n",
      "Epoch [48/300] | Train Loss: 1.2193 (Cls: 0.5639, Reg: 1.3109) | Train Acc: 83.03% || Val Loss: 0.3127 (Cls: 0.0747, Reg: 0.4766) | Val Acc: 99.04%\n",
      "Epoch [49/300] | Train Loss: 1.2096 (Cls: 0.5697, Reg: 1.2799) | Train Acc: 83.31% || Val Loss: 0.2749 (Cls: 0.0656, Reg: 0.4192) | Val Acc: 99.00%\n",
      "Epoch [50/300] | Train Loss: 1.2123 (Cls: 0.5743, Reg: 1.2760) | Train Acc: 82.90% || Val Loss: 0.2528 (Cls: 0.0654, Reg: 0.3758) | Val Acc: 98.98%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2528) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [51/300] | Train Loss: 1.1734 (Cls: 0.5464, Reg: 1.2539) | Train Acc: 83.25% || Val Loss: 0.2833 (Cls: 0.0724, Reg: 0.4226) | Val Acc: 98.92%\n",
      "Epoch [52/300] | Train Loss: 1.1972 (Cls: 0.5605, Reg: 1.2734) | Train Acc: 83.74% || Val Loss: 0.2713 (Cls: 0.0664, Reg: 0.4106) | Val Acc: 98.82%\n",
      "Epoch [53/300] | Train Loss: 1.2023 (Cls: 0.5640, Reg: 1.2765) | Train Acc: 83.20% || Val Loss: 0.2670 (Cls: 0.0780, Reg: 0.3789) | Val Acc: 98.41%\n",
      "Epoch [54/300] | Train Loss: 1.1814 (Cls: 0.5519, Reg: 1.2590) | Train Acc: 83.45% || Val Loss: 0.2403 (Cls: 0.0584, Reg: 0.3637) | Val Acc: 99.12%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2403) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [55/300] | Train Loss: 1.1634 (Cls: 0.5416, Reg: 1.2436) | Train Acc: 83.72% || Val Loss: 0.2276 (Cls: 0.0635, Reg: 0.3297) | Val Acc: 99.10%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2276) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [56/300] | Train Loss: 1.1821 (Cls: 0.5498, Reg: 1.2646) | Train Acc: 83.66% || Val Loss: 0.2461 (Cls: 0.0628, Reg: 0.3677) | Val Acc: 98.98%\n",
      "Epoch [57/300] | Train Loss: 1.1856 (Cls: 0.5452, Reg: 1.2809) | Train Acc: 83.49% || Val Loss: 0.2682 (Cls: 0.0692, Reg: 0.3997) | Val Acc: 98.80%\n",
      "Epoch [58/300] | Train Loss: 1.1845 (Cls: 0.5550, Reg: 1.2589) | Train Acc: 83.64% || Val Loss: 0.2102 (Cls: 0.0596, Reg: 0.3047) | Val Acc: 99.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2102) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [59/300] | Train Loss: 1.1614 (Cls: 0.5442, Reg: 1.2344) | Train Acc: 83.57% || Val Loss: 0.2409 (Cls: 0.0586, Reg: 0.3670) | Val Acc: 99.16%\n",
      "Epoch [60/300] | Train Loss: 1.1831 (Cls: 0.5546, Reg: 1.2569) | Train Acc: 83.81% || Val Loss: 0.2267 (Cls: 0.0599, Reg: 0.3358) | Val Acc: 99.10%\n",
      "Epoch [61/300] | Train Loss: 1.1448 (Cls: 0.5260, Reg: 1.2376) | Train Acc: 83.90% || Val Loss: 0.2184 (Cls: 0.0584, Reg: 0.3217) | Val Acc: 99.14%\n",
      "Epoch [62/300] | Train Loss: 1.1798 (Cls: 0.5497, Reg: 1.2601) | Train Acc: 83.47% || Val Loss: 0.2516 (Cls: 0.0649, Reg: 0.3757) | Val Acc: 99.27%\n",
      "Epoch [63/300] | Train Loss: 1.1582 (Cls: 0.5323, Reg: 1.2519) | Train Acc: 84.30% || Val Loss: 0.2481 (Cls: 0.0608, Reg: 0.3766) | Val Acc: 99.10%\n",
      "Epoch [64/300] | Train Loss: 1.1331 (Cls: 0.5259, Reg: 1.2144) | Train Acc: 84.24% || Val Loss: 0.2253 (Cls: 0.0538, Reg: 0.3445) | Val Acc: 99.10%\n",
      "Epoch [65/300] | Train Loss: 1.1393 (Cls: 0.5275, Reg: 1.2236) | Train Acc: 84.54% || Val Loss: 0.2383 (Cls: 0.0601, Reg: 0.3575) | Val Acc: 99.08%\n",
      "Epoch [66/300] | Train Loss: 1.1397 (Cls: 0.5249, Reg: 1.2296) | Train Acc: 84.39% || Val Loss: 0.2296 (Cls: 0.0602, Reg: 0.3412) | Val Acc: 98.92%\n",
      "Epoch [67/300] | Train Loss: 1.1492 (Cls: 0.5352, Reg: 1.2280) | Train Acc: 84.38% || Val Loss: 0.2340 (Cls: 0.0651, Reg: 0.3396) | Val Acc: 99.12%\n",
      "Epoch [68/300] | Train Loss: 1.1490 (Cls: 0.5380, Reg: 1.2219) | Train Acc: 83.85% || Val Loss: 0.2116 (Cls: 0.0573, Reg: 0.3102) | Val Acc: 99.06%\n",
      "Epoch [69/300] | Train Loss: 1.1538 (Cls: 0.5387, Reg: 1.2302) | Train Acc: 84.36% || Val Loss: 0.2269 (Cls: 0.0508, Reg: 0.3535) | Val Acc: 99.10%\n",
      "Epoch [70/300] | Train Loss: 1.1382 (Cls: 0.5291, Reg: 1.2183) | Train Acc: 84.47% || Val Loss: 0.2456 (Cls: 0.0540, Reg: 0.3841) | Val Acc: 99.10%\n",
      "Epoch [71/300] | Train Loss: 1.0818 (Cls: 0.4902, Reg: 1.1832) | Train Acc: 85.11% || Val Loss: 0.2218 (Cls: 0.0584, Reg: 0.3285) | Val Acc: 99.12%\n",
      "Epoch [72/300] | Train Loss: 1.1281 (Cls: 0.5168, Reg: 1.2225) | Train Acc: 84.85% || Val Loss: 0.2522 (Cls: 0.0713, Reg: 0.3635) | Val Acc: 98.84%\n",
      "Epoch [73/300] | Train Loss: 1.1384 (Cls: 0.5299, Reg: 1.2170) | Train Acc: 84.65% || Val Loss: 0.2282 (Cls: 0.0621, Reg: 0.3340) | Val Acc: 99.00%\n",
      "Epoch [74/300] | Train Loss: 1.1159 (Cls: 0.5105, Reg: 1.2108) | Train Acc: 84.71% || Val Loss: 0.2157 (Cls: 0.0507, Reg: 0.3320) | Val Acc: 98.98%\n",
      "Epoch [75/300] | Train Loss: 1.0690 (Cls: 0.4843, Reg: 1.1694) | Train Acc: 85.91% || Val Loss: 0.2043 (Cls: 0.0500, Reg: 0.3110) | Val Acc: 99.29%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2043) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [76/300] | Train Loss: 1.0465 (Cls: 0.4731, Reg: 1.1467) | Train Acc: 86.09% || Val Loss: 0.2102 (Cls: 0.0503, Reg: 0.3221) | Val Acc: 99.27%\n",
      "Epoch [77/300] | Train Loss: 1.0106 (Cls: 0.4472, Reg: 1.1268) | Train Acc: 86.70% || Val Loss: 0.1900 (Cls: 0.0447, Reg: 0.2926) | Val Acc: 99.37%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1900) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [78/300] | Train Loss: 1.0102 (Cls: 0.4466, Reg: 1.1272) | Train Acc: 86.30% || Val Loss: 0.2024 (Cls: 0.0504, Reg: 0.3060) | Val Acc: 99.16%\n",
      "Epoch [79/300] | Train Loss: 0.9963 (Cls: 0.4341, Reg: 1.1244) | Train Acc: 86.87% || Val Loss: 0.2053 (Cls: 0.0484, Reg: 0.3154) | Val Acc: 99.24%\n",
      "Epoch [80/300] | Train Loss: 0.9990 (Cls: 0.4395, Reg: 1.1191) | Train Acc: 86.73% || Val Loss: 0.1878 (Cls: 0.0476, Reg: 0.2830) | Val Acc: 99.33%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1878) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [81/300] | Train Loss: 1.0084 (Cls: 0.4439, Reg: 1.1291) | Train Acc: 86.83% || Val Loss: 0.2012 (Cls: 0.0485, Reg: 0.3073) | Val Acc: 99.29%\n",
      "Epoch [82/300] | Train Loss: 0.9859 (Cls: 0.4347, Reg: 1.1024) | Train Acc: 87.19% || Val Loss: 0.1822 (Cls: 0.0473, Reg: 0.2723) | Val Acc: 99.33%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1822) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [83/300] | Train Loss: 0.9907 (Cls: 0.4391, Reg: 1.1033) | Train Acc: 87.26% || Val Loss: 0.2071 (Cls: 0.0497, Reg: 0.3170) | Val Acc: 99.24%\n",
      "Epoch [84/300] | Train Loss: 0.9888 (Cls: 0.4336, Reg: 1.1104) | Train Acc: 87.47% || Val Loss: 0.2032 (Cls: 0.0502, Reg: 0.3079) | Val Acc: 99.37%\n",
      "Epoch [85/300] | Train Loss: 0.9884 (Cls: 0.4279, Reg: 1.1209) | Train Acc: 87.35% || Val Loss: 0.1899 (Cls: 0.0459, Reg: 0.2906) | Val Acc: 99.45%\n",
      "Epoch [86/300] | Train Loss: 0.9875 (Cls: 0.4308, Reg: 1.1133) | Train Acc: 87.41% || Val Loss: 0.1923 (Cls: 0.0468, Reg: 0.2938) | Val Acc: 99.45%\n",
      "Epoch [87/300] | Train Loss: 1.0063 (Cls: 0.4465, Reg: 1.1197) | Train Acc: 86.87% || Val Loss: 0.1861 (Cls: 0.0468, Reg: 0.2812) | Val Acc: 99.31%\n",
      "Epoch [88/300] | Train Loss: 0.9783 (Cls: 0.4282, Reg: 1.1002) | Train Acc: 87.61% || Val Loss: 0.1905 (Cls: 0.0455, Reg: 0.2928) | Val Acc: 99.37%\n",
      "Epoch [89/300] | Train Loss: 0.9944 (Cls: 0.4390, Reg: 1.1109) | Train Acc: 87.10% || Val Loss: 0.2045 (Cls: 0.0474, Reg: 0.3160) | Val Acc: 99.35%\n",
      "Epoch [90/300] | Train Loss: 0.9596 (Cls: 0.4126, Reg: 1.0941) | Train Acc: 87.54% || Val Loss: 0.1976 (Cls: 0.0475, Reg: 0.3026) | Val Acc: 99.27%\n",
      "Epoch [91/300] | Train Loss: 0.9861 (Cls: 0.4332, Reg: 1.1057) | Train Acc: 87.38% || Val Loss: 0.2026 (Cls: 0.0454, Reg: 0.3168) | Val Acc: 99.43%\n",
      "Epoch [92/300] | Train Loss: 0.9685 (Cls: 0.4231, Reg: 1.0908) | Train Acc: 87.64% || Val Loss: 0.1935 (Cls: 0.0475, Reg: 0.2944) | Val Acc: 99.31%\n",
      "Epoch [93/300] | Train Loss: 0.9921 (Cls: 0.4318, Reg: 1.1207) | Train Acc: 87.19% || Val Loss: 0.1927 (Cls: 0.0475, Reg: 0.2934) | Val Acc: 99.35%\n",
      "Epoch [94/300] | Train Loss: 0.9493 (Cls: 0.4111, Reg: 1.0764) | Train Acc: 87.69% || Val Loss: 0.1857 (Cls: 0.0457, Reg: 0.2837) | Val Acc: 99.31%\n",
      "Epoch [95/300] | Train Loss: 0.9638 (Cls: 0.4210, Reg: 1.0855) | Train Acc: 87.41% || Val Loss: 0.1712 (Cls: 0.0468, Reg: 0.2522) | Val Acc: 99.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1712) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [96/300] | Train Loss: 0.9700 (Cls: 0.4235, Reg: 1.0930) | Train Acc: 87.44% || Val Loss: 0.1995 (Cls: 0.0496, Reg: 0.3024) | Val Acc: 99.29%\n",
      "Epoch [97/300] | Train Loss: 0.9582 (Cls: 0.4100, Reg: 1.0963) | Train Acc: 88.20% || Val Loss: 0.1859 (Cls: 0.0480, Reg: 0.2789) | Val Acc: 99.24%\n",
      "Epoch [98/300] | Train Loss: 0.9572 (Cls: 0.4162, Reg: 1.0819) | Train Acc: 87.26% || Val Loss: 0.1951 (Cls: 0.0452, Reg: 0.3022) | Val Acc: 99.31%\n",
      "Epoch [99/300] | Train Loss: 0.9681 (Cls: 0.4176, Reg: 1.1009) | Train Acc: 87.63% || Val Loss: 0.1871 (Cls: 0.0456, Reg: 0.2855) | Val Acc: 99.39%\n",
      "Epoch [100/300] | Train Loss: 0.9799 (Cls: 0.4328, Reg: 1.0943) | Train Acc: 87.49% || Val Loss: 0.2005 (Cls: 0.0451, Reg: 0.3133) | Val Acc: 99.31%\n",
      "Epoch [101/300] | Train Loss: 0.9738 (Cls: 0.4245, Reg: 1.0986) | Train Acc: 87.69% || Val Loss: 0.1902 (Cls: 0.0460, Reg: 0.2912) | Val Acc: 99.39%\n",
      "Epoch [102/300] | Train Loss: 0.9621 (Cls: 0.4181, Reg: 1.0880) | Train Acc: 87.62% || Val Loss: 0.1768 (Cls: 0.0452, Reg: 0.2664) | Val Acc: 99.41%\n",
      "Epoch [103/300] | Train Loss: 0.9670 (Cls: 0.4170, Reg: 1.1000) | Train Acc: 87.57% || Val Loss: 0.1788 (Cls: 0.0457, Reg: 0.2693) | Val Acc: 99.39%\n",
      "Epoch [104/300] | Train Loss: 0.9627 (Cls: 0.4236, Reg: 1.0783) | Train Acc: 87.87% || Val Loss: 0.1764 (Cls: 0.0451, Reg: 0.2652) | Val Acc: 99.39%\n",
      "Epoch [105/300] | Train Loss: 0.9542 (Cls: 0.4190, Reg: 1.0704) | Train Acc: 87.64% || Val Loss: 0.1788 (Cls: 0.0475, Reg: 0.2653) | Val Acc: 99.35%\n",
      "Epoch [106/300] | Train Loss: 0.9364 (Cls: 0.4027, Reg: 1.0673) | Train Acc: 88.25% || Val Loss: 0.1946 (Cls: 0.0481, Reg: 0.2957) | Val Acc: 99.35%\n",
      "Epoch [107/300] | Train Loss: 0.9661 (Cls: 0.4188, Reg: 1.0946) | Train Acc: 88.05% || Val Loss: 0.1800 (Cls: 0.0471, Reg: 0.2695) | Val Acc: 99.43%\n",
      "Epoch [108/300] | Train Loss: 0.9479 (Cls: 0.4078, Reg: 1.0803) | Train Acc: 87.98% || Val Loss: 0.1914 (Cls: 0.0470, Reg: 0.2923) | Val Acc: 99.45%\n",
      "Epoch [109/300] | Train Loss: 0.9454 (Cls: 0.4083, Reg: 1.0741) | Train Acc: 88.10% || Val Loss: 0.2002 (Cls: 0.0465, Reg: 0.3103) | Val Acc: 99.33%\n",
      "Epoch [110/300] | Train Loss: 0.9376 (Cls: 0.3968, Reg: 1.0815) | Train Acc: 88.12% || Val Loss: 0.1691 (Cls: 0.0419, Reg: 0.2570) | Val Acc: 99.47%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1691) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [111/300] | Train Loss: 0.9547 (Cls: 0.4066, Reg: 1.0963) | Train Acc: 88.10% || Val Loss: 0.1989 (Cls: 0.0457, Reg: 0.3089) | Val Acc: 99.41%\n",
      "Epoch [112/300] | Train Loss: 0.9416 (Cls: 0.4038, Reg: 1.0756) | Train Acc: 88.19% || Val Loss: 0.1900 (Cls: 0.0493, Reg: 0.2846) | Val Acc: 99.31%\n",
      "Epoch [113/300] | Train Loss: 0.9367 (Cls: 0.3982, Reg: 1.0770) | Train Acc: 88.36% || Val Loss: 0.1919 (Cls: 0.0455, Reg: 0.2954) | Val Acc: 99.31%\n",
      "Epoch [114/300] | Train Loss: 0.9704 (Cls: 0.4218, Reg: 1.0973) | Train Acc: 87.91% || Val Loss: 0.1842 (Cls: 0.0424, Reg: 0.2863) | Val Acc: 99.41%\n",
      "Epoch [115/300] | Train Loss: 0.9492 (Cls: 0.4073, Reg: 1.0838) | Train Acc: 88.29% || Val Loss: 0.1778 (Cls: 0.0436, Reg: 0.2710) | Val Acc: 99.37%\n",
      "Epoch [116/300] | Train Loss: 0.9460 (Cls: 0.4064, Reg: 1.0792) | Train Acc: 88.13% || Val Loss: 0.1783 (Cls: 0.0448, Reg: 0.2696) | Val Acc: 99.33%\n",
      "Epoch [117/300] | Train Loss: 0.9330 (Cls: 0.4017, Reg: 1.0625) | Train Acc: 88.12% || Val Loss: 0.1730 (Cls: 0.0415, Reg: 0.2650) | Val Acc: 99.47%\n",
      "Epoch [118/300] | Train Loss: 0.9519 (Cls: 0.4135, Reg: 1.0768) | Train Acc: 87.91% || Val Loss: 0.1847 (Cls: 0.0415, Reg: 0.2882) | Val Acc: 99.39%\n",
      "Epoch [119/300] | Train Loss: 0.9334 (Cls: 0.3968, Reg: 1.0734) | Train Acc: 88.12% || Val Loss: 0.1806 (Cls: 0.0424, Reg: 0.2777) | Val Acc: 99.33%\n",
      "Epoch [120/300] | Train Loss: 0.9472 (Cls: 0.4087, Reg: 1.0770) | Train Acc: 87.81% || Val Loss: 0.1695 (Cls: 0.0464, Reg: 0.2482) | Val Acc: 99.18%\n",
      "Epoch [121/300] | Train Loss: 0.9644 (Cls: 0.4231, Reg: 1.0825) | Train Acc: 87.66% || Val Loss: 0.1845 (Cls: 0.0466, Reg: 0.2787) | Val Acc: 99.31%\n",
      "Epoch [122/300] | Train Loss: 0.9344 (Cls: 0.3965, Reg: 1.0759) | Train Acc: 88.25% || Val Loss: 0.1848 (Cls: 0.0451, Reg: 0.2811) | Val Acc: 99.16%\n",
      "Epoch [123/300] | Train Loss: 0.9674 (Cls: 0.4271, Reg: 1.0806) | Train Acc: 87.61% || Val Loss: 0.1801 (Cls: 0.0461, Reg: 0.2703) | Val Acc: 99.31%\n",
      "Epoch [124/300] | Train Loss: 0.9342 (Cls: 0.3993, Reg: 1.0698) | Train Acc: 88.78% || Val Loss: 0.1765 (Cls: 0.0442, Reg: 0.2669) | Val Acc: 99.37%\n",
      "Epoch [125/300] | Train Loss: 0.9485 (Cls: 0.4100, Reg: 1.0770) | Train Acc: 87.94% || Val Loss: 0.1701 (Cls: 0.0420, Reg: 0.2584) | Val Acc: 99.41%\n",
      "Epoch [126/300] | Train Loss: 0.9376 (Cls: 0.4009, Reg: 1.0735) | Train Acc: 88.18% || Val Loss: 0.1766 (Cls: 0.0428, Reg: 0.2697) | Val Acc: 99.35%\n",
      "Epoch [127/300] | Train Loss: 0.9143 (Cls: 0.3912, Reg: 1.0463) | Train Acc: 88.70% || Val Loss: 0.1706 (Cls: 0.0416, Reg: 0.2602) | Val Acc: 99.39%\n",
      "Epoch [128/300] | Train Loss: 0.8961 (Cls: 0.3800, Reg: 1.0323) | Train Acc: 89.00% || Val Loss: 0.1760 (Cls: 0.0416, Reg: 0.2718) | Val Acc: 99.41%\n",
      "Epoch [129/300] | Train Loss: 0.8845 (Cls: 0.3754, Reg: 1.0182) | Train Acc: 89.21% || Val Loss: 0.1649 (Cls: 0.0407, Reg: 0.2515) | Val Acc: 99.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1649) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [130/300] | Train Loss: 0.8851 (Cls: 0.3701, Reg: 1.0301) | Train Acc: 89.23% || Val Loss: 0.1737 (Cls: 0.0407, Reg: 0.2690) | Val Acc: 99.47%\n",
      "Epoch [131/300] | Train Loss: 0.8999 (Cls: 0.3806, Reg: 1.0386) | Train Acc: 88.94% || Val Loss: 0.1615 (Cls: 0.0381, Reg: 0.2494) | Val Acc: 99.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1615) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [132/300] | Train Loss: 0.8830 (Cls: 0.3667, Reg: 1.0326) | Train Acc: 89.25% || Val Loss: 0.1653 (Cls: 0.0415, Reg: 0.2508) | Val Acc: 99.43%\n",
      "Epoch [133/300] | Train Loss: 0.8804 (Cls: 0.3675, Reg: 1.0260) | Train Acc: 88.87% || Val Loss: 0.1660 (Cls: 0.0392, Reg: 0.2567) | Val Acc: 99.45%\n",
      "Epoch [134/300] | Train Loss: 0.8759 (Cls: 0.3612, Reg: 1.0294) | Train Acc: 89.29% || Val Loss: 0.1825 (Cls: 0.0385, Reg: 0.2904) | Val Acc: 99.57%\n",
      "Epoch [135/300] | Train Loss: 0.8725 (Cls: 0.3585, Reg: 1.0280) | Train Acc: 89.64% || Val Loss: 0.1694 (Cls: 0.0404, Reg: 0.2606) | Val Acc: 99.39%\n",
      "Epoch [136/300] | Train Loss: 0.8852 (Cls: 0.3721, Reg: 1.0262) | Train Acc: 89.29% || Val Loss: 0.1655 (Cls: 0.0413, Reg: 0.2516) | Val Acc: 99.41%\n",
      "Epoch [137/300] | Train Loss: 0.8870 (Cls: 0.3740, Reg: 1.0260) | Train Acc: 88.87% || Val Loss: 0.1624 (Cls: 0.0385, Reg: 0.2507) | Val Acc: 99.43%\n",
      "Epoch [138/300] | Train Loss: 0.9045 (Cls: 0.3855, Reg: 1.0381) | Train Acc: 88.87% || Val Loss: 0.1663 (Cls: 0.0400, Reg: 0.2558) | Val Acc: 99.45%\n",
      "Epoch [139/300] | Train Loss: 0.8754 (Cls: 0.3645, Reg: 1.0219) | Train Acc: 89.73% || Val Loss: 0.1650 (Cls: 0.0397, Reg: 0.2535) | Val Acc: 99.47%\n",
      "Epoch [140/300] | Train Loss: 0.8608 (Cls: 0.3557, Reg: 1.0101) | Train Acc: 89.70% || Val Loss: 0.1673 (Cls: 0.0396, Reg: 0.2578) | Val Acc: 99.39%\n",
      "Epoch [141/300] | Train Loss: 0.8761 (Cls: 0.3673, Reg: 1.0176) | Train Acc: 89.21% || Val Loss: 0.1813 (Cls: 0.0420, Reg: 0.2815) | Val Acc: 99.43%\n",
      "Epoch [142/300] | Train Loss: 0.8782 (Cls: 0.3625, Reg: 1.0313) | Train Acc: 89.45% || Val Loss: 0.1650 (Cls: 0.0418, Reg: 0.2493) | Val Acc: 99.43%\n",
      "Epoch [143/300] | Train Loss: 0.8780 (Cls: 0.3706, Reg: 1.0148) | Train Acc: 89.14% || Val Loss: 0.1541 (Cls: 0.0413, Reg: 0.2287) | Val Acc: 99.49%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1541) 至 ./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\n",
      "Epoch [144/300] | Train Loss: 0.8775 (Cls: 0.3619, Reg: 1.0313) | Train Acc: 89.62% || Val Loss: 0.1744 (Cls: 0.0404, Reg: 0.2711) | Val Acc: 99.41%\n",
      "Epoch [145/300] | Train Loss: 0.8627 (Cls: 0.3583, Reg: 1.0088) | Train Acc: 89.30% || Val Loss: 0.1650 (Cls: 0.0403, Reg: 0.2521) | Val Acc: 99.43%\n",
      "Epoch [146/300] | Train Loss: 0.8776 (Cls: 0.3693, Reg: 1.0168) | Train Acc: 89.07% || Val Loss: 0.1799 (Cls: 0.0408, Reg: 0.2805) | Val Acc: 99.39%\n",
      "Epoch [147/300] | Train Loss: 0.8788 (Cls: 0.3679, Reg: 1.0219) | Train Acc: 89.18% || Val Loss: 0.1644 (Cls: 0.0386, Reg: 0.2548) | Val Acc: 99.47%\n",
      "Epoch [148/300] | Train Loss: 0.8601 (Cls: 0.3500, Reg: 1.0201) | Train Acc: 89.73% || Val Loss: 0.1699 (Cls: 0.0396, Reg: 0.2639) | Val Acc: 99.47%\n",
      "Epoch [149/300] | Train Loss: 0.8824 (Cls: 0.3665, Reg: 1.0317) | Train Acc: 89.37% || Val Loss: 0.1666 (Cls: 0.0395, Reg: 0.2571) | Val Acc: 99.47%\n",
      "Epoch [150/300] | Train Loss: 0.8709 (Cls: 0.3598, Reg: 1.0222) | Train Acc: 89.42% || Val Loss: 0.1760 (Cls: 0.0442, Reg: 0.2667) | Val Acc: 99.31%\n",
      "Epoch [151/300] | Train Loss: 0.8565 (Cls: 0.3493, Reg: 1.0144) | Train Acc: 89.87% || Val Loss: 0.1726 (Cls: 0.0400, Reg: 0.2682) | Val Acc: 99.43%\n",
      "Epoch [152/300] | Train Loss: 0.8695 (Cls: 0.3580, Reg: 1.0230) | Train Acc: 89.52% || Val Loss: 0.1784 (Cls: 0.0394, Reg: 0.2807) | Val Acc: 99.47%\n",
      "Epoch [153/300] | Train Loss: 0.8727 (Cls: 0.3609, Reg: 1.0236) | Train Acc: 89.39% || Val Loss: 0.1635 (Cls: 0.0409, Reg: 0.2484) | Val Acc: 99.41%\n",
      "Epoch [154/300] | Train Loss: 0.8794 (Cls: 0.3627, Reg: 1.0334) | Train Acc: 89.35% || Val Loss: 0.1618 (Cls: 0.0392, Reg: 0.2480) | Val Acc: 99.45%\n",
      "Epoch [155/300] | Train Loss: 0.8489 (Cls: 0.3437, Reg: 1.0105) | Train Acc: 89.92% || Val Loss: 0.1750 (Cls: 0.0415, Reg: 0.2705) | Val Acc: 99.33%\n",
      "Epoch [156/300] | Train Loss: 0.8574 (Cls: 0.3523, Reg: 1.0104) | Train Acc: 89.56% || Val Loss: 0.1566 (Cls: 0.0393, Reg: 0.2384) | Val Acc: 99.45%\n",
      "Epoch [157/300] | Train Loss: 0.8565 (Cls: 0.3528, Reg: 1.0073) | Train Acc: 89.59% || Val Loss: 0.1648 (Cls: 0.0393, Reg: 0.2546) | Val Acc: 99.43%\n",
      "Epoch [158/300] | Train Loss: 0.8562 (Cls: 0.3506, Reg: 1.0111) | Train Acc: 89.53% || Val Loss: 0.1580 (Cls: 0.0403, Reg: 0.2392) | Val Acc: 99.45%\n",
      "Epoch [159/300] | Train Loss: 0.8599 (Cls: 0.3558, Reg: 1.0081) | Train Acc: 89.78% || Val Loss: 0.1633 (Cls: 0.0386, Reg: 0.2529) | Val Acc: 99.49%\n",
      "Epoch [160/300] | Train Loss: 0.8691 (Cls: 0.3629, Reg: 1.0124) | Train Acc: 89.67% || Val Loss: 0.1678 (Cls: 0.0402, Reg: 0.2584) | Val Acc: 99.49%\n",
      "Epoch [161/300] | Train Loss: 0.8328 (Cls: 0.3332, Reg: 0.9991) | Train Acc: 90.23% || Val Loss: 0.1549 (Cls: 0.0380, Reg: 0.2372) | Val Acc: 99.51%\n",
      "Epoch [162/300] | Train Loss: 0.8355 (Cls: 0.3330, Reg: 1.0050) | Train Acc: 90.44% || Val Loss: 0.1622 (Cls: 0.0393, Reg: 0.2489) | Val Acc: 99.51%\n",
      "Epoch [163/300] | Train Loss: 0.8289 (Cls: 0.3366, Reg: 0.9846) | Train Acc: 90.13% || Val Loss: 0.1560 (Cls: 0.0381, Reg: 0.2393) | Val Acc: 99.55%\n",
      "Epoch [164/300] | Train Loss: 0.8503 (Cls: 0.3477, Reg: 1.0051) | Train Acc: 89.98% || Val Loss: 0.1579 (Cls: 0.0384, Reg: 0.2424) | Val Acc: 99.55%\n",
      "Epoch [165/300] | Train Loss: 0.8385 (Cls: 0.3403, Reg: 0.9964) | Train Acc: 90.23% || Val Loss: 0.1650 (Cls: 0.0384, Reg: 0.2562) | Val Acc: 99.51%\n",
      "Epoch [166/300] | Train Loss: 0.8372 (Cls: 0.3373, Reg: 0.9999) | Train Acc: 89.99% || Val Loss: 0.1608 (Cls: 0.0395, Reg: 0.2463) | Val Acc: 99.45%\n",
      "Epoch [167/300] | Train Loss: 0.8215 (Cls: 0.3264, Reg: 0.9903) | Train Acc: 90.33% || Val Loss: 0.1622 (Cls: 0.0397, Reg: 0.2486) | Val Acc: 99.47%\n",
      "Epoch [168/300] | Train Loss: 0.8473 (Cls: 0.3437, Reg: 1.0072) | Train Acc: 90.02% || Val Loss: 0.1607 (Cls: 0.0402, Reg: 0.2446) | Val Acc: 99.47%\n",
      "Early stop at epoch 168\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "#alpha = 0.1\n",
    "k = 0.4  # 基礎平衡係數\n",
    "min_alpha = 0.01\n",
    "max_alpha = 0.5\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "# -----------------------\n",
    "# COORDINATES 字典：將分類標籤轉為 (X, Y) 座標\n",
    "# -----------------------\n",
    "COORDINATES = {\n",
    "    # 下邊界 (1-10 和 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    # 左邊界 (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    # 上邊界 (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    # 右邊界 (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    # 中間點 (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        # 將 0-index 轉換成 1-index (例如 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 訓練參數與 Early Stopping 設定\n",
    "# -----------------------\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_best_model_0324_reg_class_std_balance_04_05.pth\"\n",
    "epochs = 300\n",
    "patience = 25\n",
    "counter = 0  \n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# -----------------------\n",
    "# 訓練迴圈 (分類與回歸雙輸出)\n",
    "# -----------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_class_loss = 0.0\n",
    "    train_reg_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 每個 batch 返回 (amp_inputs, rssi_inputs, labels)\n",
    "    # 其中 labels 為 one-hot 編碼 (用以計算分類損失)\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # 分類目標：one-hot -> class index\n",
    "        target_class = torch.argmax(labels, dim=1)\n",
    "        loss_class = criterion(class_out, target_class)\n",
    "        \n",
    "        # 回歸目標：根據 target_class 透過 COORDINATES 字典取得 (X, Y) 座標\n",
    "        true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "        loss_reg = criterion_reg(reg_out, true_coords)\n",
    "        \n",
    "        loss = loss_class + alpha * loss_reg\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size_curr = amp_inputs.size(0)\n",
    "        train_loss += loss.item() * batch_size_curr\n",
    "        train_class_loss += loss_class.item() * batch_size_curr\n",
    "        train_reg_loss += loss_reg.item() * batch_size_curr\n",
    "        _, predicted = torch.max(class_out, 1)\n",
    "        total_train += batch_size_curr\n",
    "        train_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_class_loss = train_class_loss / len(train_loader.dataset)\n",
    "    avg_train_reg_loss = train_reg_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_class_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            loss_class = criterion(class_out, target_class)\n",
    "            true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "            loss_reg = criterion_reg(reg_out, true_coords)\n",
    "            \n",
    "            # 計算平衡係數 alpha 每個 epoch 動態調整\n",
    "            ratio = loss_class.item() / (loss_reg.item() + 1e-8)\n",
    "            alpha = k / ratio\n",
    "            alpha = max(min_alpha, min(max_alpha, alpha))\n",
    "            loss = loss_class + alpha * loss_reg\n",
    "            \n",
    "            batch_size_curr = amp_inputs.size(0)\n",
    "            val_loss += loss.item() * batch_size_curr\n",
    "            val_class_loss += loss_class.item() * batch_size_curr\n",
    "            val_reg_loss += loss_reg.item() * batch_size_curr\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total_val += batch_size_curr\n",
    "            val_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_class_loss = val_class_loss / len(val_loader.dataset)\n",
    "    avg_val_reg_loss = val_reg_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_class_loss:.4f}, Reg: {avg_train_reg_loss:.4f}) | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% || \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} (Cls: {avg_val_class_loss:.4f}, Reg: {avg_val_reg_loss:.4f}) | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 自定義apha 0.7最好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 測試損失: 0.0558, 測試準確率: 99.43%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "\n",
    "# 載入最佳模型\n",
    "model.load_state_dict(torch.load(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN/models_save/rssi_csi_best_model_0506_reg_class_07_std.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 測試模型\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_reg = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for amp_inputs, rssi_input, labels in test_loader:\n",
    "        amp_inputs, rssi_inputs, labels = amp_inputs.to(device), rssi_input.to(device), labels.to(device)\n",
    "        outputs, reg = model(amp_inputs, rssi_inputs)\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "        test_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "        # 儲存真實標籤與預測標籤\n",
    "        all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_reg.extend(reg.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"📊 測試損失: {test_loss:.4f}, 測試準確率: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance error: 0.012163687323484332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adjusted_predictions = np.array(all_predictions) + 1\n",
    "adjusted_labels = np.array(all_labels) + 1\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # 取出對應的座標\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # 計算歐氏距離\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) , errors\n",
    "\n",
    "mean_error, errors = compute_mean_distance_error(adjusted_labels, adjusted_predictions, COORDINATES)\n",
    "print(\"Mean distance error:\", mean_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8\n",
      "1.2\n",
      "1.2000000000000002\n",
      "0.5999999999999996\n",
      "1.7999999999999998\n",
      "5.909314681077663\n",
      "0.5999999999999996\n",
      "1.3416407864998738\n",
      "0.5999999999999999\n",
      "1.7999999999999998\n",
      "6.708203932499369\n",
      "3.8418745424597094\n",
      "0.5999999999999996\n",
      "1.7999999999999998\n"
     ]
    }
   ],
   "source": [
    "for i in errors:\n",
    "    if i > 0.0:\n",
    "        print(i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Distance Error (Regression): 0.5729004140644297\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def compute_mean_distance_error_reg(y_true, y_pred, coord_dict):\n",
    "    \"\"\"\n",
    "    y_true: 一維的 NumPy 陣列，存放真實的標籤 (整數)\n",
    "    y_pred: 二維的 NumPy 陣列，形狀 (N,2)，存放回歸輸出的 (X, Y) 座標\n",
    "    coord_dict: dict, 將標籤轉換為 (X, Y) 座標，鍵應該為 1~49\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "    # 假設 y_true[i] 對應於 y_pred[i]\n",
    "    for true_label, pred_coord in zip(y_true, y_pred):\n",
    "        # 若你的分類 head 輸出是 0~48，則需要將其轉成 1~49\n",
    "        true_label = int(true_label) + 1  # 轉換 0-index 為 1-index\n",
    "        if true_label not in coord_dict:\n",
    "            continue\n",
    "        true_coord = np.array(coord_dict[true_label])\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) if errors else None\n",
    "\n",
    "# 假設 all_labels 為真實分類標籤 (0-index, 例如 0 ~ 48)\n",
    "# all_reg 為模型回歸輸出的座標 (shape: (N,2))\n",
    "mean_distance_error = compute_mean_distance_error_reg(np.array(all_labels), np.array(all_reg), COORDINATES)\n",
    "print(\"Mean Distance Error (Regression):\", mean_distance_error)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6\n",
      "1.2\n",
      "1.2\n",
      "1.6971\n",
      "1.8\n",
      "1.8\n",
      "2.6833\n",
      "3.0\n",
      "3.0594\n",
      "3.2311\n",
      "3.6\n",
      "5.4\n",
      "5.4332\n",
      "5.9397\n",
      "6.2642\n",
      "7.3239\n"
     ]
    }
   ],
   "source": [
    "sorted_errors = np.sort(errors1)\n",
    "\n",
    "for error in sorted_errors:\n",
    "    if error > 0:\n",
    "        print(error.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 以下為  MultiTaskUncertaintyLoss 測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 508,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOSS REWEIGHT\n",
    "\n",
    "# -----------------------\n",
    "# 自適應多任務損失函數：Uncertainty-based Weighting\n",
    "# -----------------------\n",
    "class MultiTaskUncertaintyLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiTaskUncertaintyLoss, self).__init__()\n",
    "        self.log_sigma_cls = nn.Parameter(torch.tensor(0.0))\n",
    "        self.log_sigma_reg = nn.Parameter(torch.tensor(0.0))\n",
    "\n",
    "    def forward(self, loss_cls, loss_reg):\n",
    "        precision_cls = torch.exp(-self.log_sigma_cls)\n",
    "        precision_reg = torch.exp(-self.log_sigma_reg)\n",
    "        loss = precision_cls * loss_cls + precision_reg * loss_reg\n",
    "        loss += self.log_sigma_cls + self.log_sigma_reg  # regularization term\n",
    "        return loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 511,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300] | Train Loss: 5.7598 (Cls: 2.4583, Reg: 3.3014) | Train Acc: 27.57% || Val Loss: 3.0014 (Cls: 1.2298, Reg: 1.7715) | Val Acc: 81.63%\n",
      "✅ 儲存最佳模型 (Val Loss: 3.0014) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [2/300] | Train Loss: 3.8250 (Cls: 1.4312, Reg: 2.3938) | Train Acc: 55.12% || Val Loss: 1.6459 (Cls: 0.4913, Reg: 1.1545) | Val Acc: 94.55%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.6459) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [3/300] | Train Loss: 2.9902 (Cls: 0.9897, Reg: 2.0005) | Train Acc: 68.54% || Val Loss: 1.1354 (Cls: 0.2848, Reg: 0.8506) | Val Acc: 94.20%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.1354) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [4/300] | Train Loss: 2.5543 (Cls: 0.7860, Reg: 1.7683) | Train Acc: 75.17% || Val Loss: 1.2491 (Cls: 0.2470, Reg: 1.0021) | Val Acc: 95.80%\n",
      "Epoch [5/300] | Train Loss: 2.2054 (Cls: 0.6512, Reg: 1.5542) | Train Acc: 79.40% || Val Loss: 1.0546 (Cls: 0.1761, Reg: 0.8785) | Val Acc: 96.55%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.0546) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [6/300] | Train Loss: 2.0256 (Cls: 0.5848, Reg: 1.4408) | Train Acc: 81.30% || Val Loss: 1.0222 (Cls: 0.1419, Reg: 0.8803) | Val Acc: 97.57%\n",
      "✅ 儲存最佳模型 (Val Loss: 1.0222) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [7/300] | Train Loss: 1.8613 (Cls: 0.5271, Reg: 1.3342) | Train Acc: 83.35% || Val Loss: 0.7497 (Cls: 0.1227, Reg: 0.6270) | Val Acc: 97.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.7497) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [8/300] | Train Loss: 1.7175 (Cls: 0.4747, Reg: 1.2428) | Train Acc: 85.18% || Val Loss: 0.5896 (Cls: 0.0981, Reg: 0.4915) | Val Acc: 98.27%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5896) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [9/300] | Train Loss: 1.5921 (Cls: 0.4421, Reg: 1.1499) | Train Acc: 86.12% || Val Loss: 0.7081 (Cls: 0.1106, Reg: 0.5976) | Val Acc: 98.78%\n",
      "Epoch [10/300] | Train Loss: 1.5190 (Cls: 0.4193, Reg: 1.0996) | Train Acc: 86.83% || Val Loss: 0.6488 (Cls: 0.0828, Reg: 0.5660) | Val Acc: 99.22%\n",
      "Epoch [11/300] | Train Loss: 1.4544 (Cls: 0.3973, Reg: 1.0572) | Train Acc: 87.95% || Val Loss: 0.5280 (Cls: 0.0753, Reg: 0.4527) | Val Acc: 99.16%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5280) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [12/300] | Train Loss: 1.3858 (Cls: 0.3764, Reg: 1.0094) | Train Acc: 88.95% || Val Loss: 0.4373 (Cls: 0.0596, Reg: 0.3776) | Val Acc: 99.22%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4373) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [13/300] | Train Loss: 1.3310 (Cls: 0.3650, Reg: 0.9660) | Train Acc: 88.98% || Val Loss: 0.4292 (Cls: 0.0526, Reg: 0.3766) | Val Acc: 99.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4292) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [14/300] | Train Loss: 1.2484 (Cls: 0.3319, Reg: 0.9165) | Train Acc: 89.96% || Val Loss: 0.4229 (Cls: 0.0662, Reg: 0.3567) | Val Acc: 99.08%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4229) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [15/300] | Train Loss: 1.2498 (Cls: 0.3450, Reg: 0.9049) | Train Acc: 89.59% || Val Loss: 0.6894 (Cls: 0.0792, Reg: 0.6102) | Val Acc: 98.71%\n",
      "Epoch [16/300] | Train Loss: 1.1701 (Cls: 0.3184, Reg: 0.8517) | Train Acc: 90.57% || Val Loss: 0.3413 (Cls: 0.0465, Reg: 0.2947) | Val Acc: 99.39%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3413) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [17/300] | Train Loss: 1.1429 (Cls: 0.3127, Reg: 0.8302) | Train Acc: 90.68% || Val Loss: 0.4546 (Cls: 0.0629, Reg: 0.3917) | Val Acc: 99.08%\n",
      "Epoch [18/300] | Train Loss: 1.1001 (Cls: 0.2941, Reg: 0.8061) | Train Acc: 91.28% || Val Loss: 0.2689 (Cls: 0.0444, Reg: 0.2245) | Val Acc: 99.35%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2689) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [19/300] | Train Loss: 1.1024 (Cls: 0.3013, Reg: 0.8011) | Train Acc: 91.06% || Val Loss: 0.2981 (Cls: 0.0384, Reg: 0.2597) | Val Acc: 99.59%\n",
      "Epoch [20/300] | Train Loss: 1.0615 (Cls: 0.2875, Reg: 0.7740) | Train Acc: 91.57% || Val Loss: 0.3365 (Cls: 0.0521, Reg: 0.2844) | Val Acc: 99.08%\n",
      "Epoch [21/300] | Train Loss: 1.0663 (Cls: 0.2906, Reg: 0.7757) | Train Acc: 91.36% || Val Loss: 0.2633 (Cls: 0.0432, Reg: 0.2201) | Val Acc: 99.51%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2633) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [22/300] | Train Loss: 1.0267 (Cls: 0.2709, Reg: 0.7558) | Train Acc: 91.94% || Val Loss: 0.2726 (Cls: 0.0401, Reg: 0.2325) | Val Acc: 99.43%\n",
      "Epoch [23/300] | Train Loss: 1.0134 (Cls: 0.2760, Reg: 0.7374) | Train Acc: 91.88% || Val Loss: 0.2005 (Cls: 0.0346, Reg: 0.1659) | Val Acc: 99.45%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2005) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [24/300] | Train Loss: 1.0010 (Cls: 0.2655, Reg: 0.7355) | Train Acc: 92.24% || Val Loss: 0.2217 (Cls: 0.0384, Reg: 0.1833) | Val Acc: 99.57%\n",
      "Epoch [25/300] | Train Loss: 0.9835 (Cls: 0.2619, Reg: 0.7215) | Train Acc: 92.22% || Val Loss: 0.2399 (Cls: 0.0320, Reg: 0.2079) | Val Acc: 99.51%\n",
      "Epoch [26/300] | Train Loss: 0.9915 (Cls: 0.2677, Reg: 0.7237) | Train Acc: 92.28% || Val Loss: 0.2471 (Cls: 0.0376, Reg: 0.2095) | Val Acc: 99.29%\n",
      "Epoch [27/300] | Train Loss: 0.9772 (Cls: 0.2535, Reg: 0.7236) | Train Acc: 92.65% || Val Loss: 0.2742 (Cls: 0.0436, Reg: 0.2306) | Val Acc: 99.33%\n",
      "Epoch [28/300] | Train Loss: 0.9636 (Cls: 0.2571, Reg: 0.7065) | Train Acc: 92.59% || Val Loss: 0.2000 (Cls: 0.0278, Reg: 0.1721) | Val Acc: 99.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2000) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [29/300] | Train Loss: 0.9327 (Cls: 0.2518, Reg: 0.6809) | Train Acc: 92.61% || Val Loss: 0.2608 (Cls: 0.0350, Reg: 0.2257) | Val Acc: 99.55%\n",
      "Epoch [30/300] | Train Loss: 0.9032 (Cls: 0.2365, Reg: 0.6667) | Train Acc: 93.13% || Val Loss: 0.1671 (Cls: 0.0262, Reg: 0.1409) | Val Acc: 99.69%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1671) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [31/300] | Train Loss: 0.9084 (Cls: 0.2380, Reg: 0.6704) | Train Acc: 92.94% || Val Loss: 0.2238 (Cls: 0.0371, Reg: 0.1867) | Val Acc: 99.59%\n",
      "Epoch [32/300] | Train Loss: 0.9155 (Cls: 0.2447, Reg: 0.6708) | Train Acc: 93.01% || Val Loss: 0.2416 (Cls: 0.0636, Reg: 0.1780) | Val Acc: 99.08%\n",
      "Epoch [33/300] | Train Loss: 0.8862 (Cls: 0.2360, Reg: 0.6502) | Train Acc: 92.93% || Val Loss: 0.1451 (Cls: 0.0267, Reg: 0.1184) | Val Acc: 99.67%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1451) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [34/300] | Train Loss: 0.8697 (Cls: 0.2292, Reg: 0.6404) | Train Acc: 93.28% || Val Loss: 0.1580 (Cls: 0.0304, Reg: 0.1276) | Val Acc: 99.59%\n",
      "Epoch [35/300] | Train Loss: 0.8481 (Cls: 0.2174, Reg: 0.6307) | Train Acc: 93.68% || Val Loss: 0.1883 (Cls: 0.0290, Reg: 0.1592) | Val Acc: 99.51%\n",
      "Epoch [36/300] | Train Loss: 0.8794 (Cls: 0.2307, Reg: 0.6487) | Train Acc: 93.28% || Val Loss: 0.1533 (Cls: 0.0303, Reg: 0.1231) | Val Acc: 99.49%\n",
      "Epoch [37/300] | Train Loss: 0.8654 (Cls: 0.2286, Reg: 0.6369) | Train Acc: 93.59% || Val Loss: 0.2042 (Cls: 0.0293, Reg: 0.1749) | Val Acc: 99.45%\n",
      "Epoch [38/300] | Train Loss: 0.8497 (Cls: 0.2243, Reg: 0.6253) | Train Acc: 93.51% || Val Loss: 0.1872 (Cls: 0.0374, Reg: 0.1499) | Val Acc: 99.57%\n",
      "Epoch [39/300] | Train Loss: 0.8876 (Cls: 0.2381, Reg: 0.6496) | Train Acc: 93.04% || Val Loss: 0.1739 (Cls: 0.0268, Reg: 0.1472) | Val Acc: 99.67%\n",
      "Epoch [40/300] | Train Loss: 0.8189 (Cls: 0.2060, Reg: 0.6129) | Train Acc: 94.11% || Val Loss: 0.1449 (Cls: 0.0274, Reg: 0.1176) | Val Acc: 99.59%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1449) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [41/300] | Train Loss: 0.8330 (Cls: 0.2129, Reg: 0.6201) | Train Acc: 93.83% || Val Loss: 0.1482 (Cls: 0.0204, Reg: 0.1278) | Val Acc: 99.67%\n",
      "Epoch [42/300] | Train Loss: 0.8305 (Cls: 0.2217, Reg: 0.6088) | Train Acc: 93.67% || Val Loss: 0.1902 (Cls: 0.0322, Reg: 0.1579) | Val Acc: 99.57%\n",
      "Epoch [43/300] | Train Loss: 0.8219 (Cls: 0.2207, Reg: 0.6012) | Train Acc: 93.87% || Val Loss: 0.1560 (Cls: 0.0373, Reg: 0.1187) | Val Acc: 99.55%\n",
      "Epoch [44/300] | Train Loss: 0.8054 (Cls: 0.2119, Reg: 0.5935) | Train Acc: 93.93% || Val Loss: 0.1435 (Cls: 0.0280, Reg: 0.1155) | Val Acc: 99.53%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1435) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [45/300] | Train Loss: 0.8211 (Cls: 0.2155, Reg: 0.6056) | Train Acc: 93.92% || Val Loss: 0.1708 (Cls: 0.0328, Reg: 0.1380) | Val Acc: 99.41%\n",
      "Epoch [46/300] | Train Loss: 0.7811 (Cls: 0.1989, Reg: 0.5823) | Train Acc: 94.24% || Val Loss: 0.1252 (Cls: 0.0236, Reg: 0.1015) | Val Acc: 99.65%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.1252) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [47/300] | Train Loss: 0.8336 (Cls: 0.2228, Reg: 0.6108) | Train Acc: 93.68% || Val Loss: 0.1763 (Cls: 0.0285, Reg: 0.1477) | Val Acc: 99.39%\n",
      "Epoch [48/300] | Train Loss: 0.7823 (Cls: 0.2005, Reg: 0.5818) | Train Acc: 94.48% || Val Loss: 0.1332 (Cls: 0.0253, Reg: 0.1079) | Val Acc: 99.69%\n",
      "Epoch [49/300] | Train Loss: 0.8029 (Cls: 0.2099, Reg: 0.5930) | Train Acc: 93.86% || Val Loss: 0.1439 (Cls: 0.0302, Reg: 0.1138) | Val Acc: 99.63%\n",
      "Epoch [50/300] | Train Loss: 0.7727 (Cls: 0.1995, Reg: 0.5733) | Train Acc: 94.16% || Val Loss: 0.1264 (Cls: 0.0212, Reg: 0.1052) | Val Acc: 99.67%\n",
      "Epoch [51/300] | Train Loss: 0.7681 (Cls: 0.1996, Reg: 0.5685) | Train Acc: 94.34% || Val Loss: 0.1792 (Cls: 0.0297, Reg: 0.1495) | Val Acc: 99.45%\n",
      "Epoch [52/300] | Train Loss: 0.8052 (Cls: 0.2185, Reg: 0.5867) | Train Acc: 93.73% || Val Loss: 0.1341 (Cls: 0.0275, Reg: 0.1066) | Val Acc: 99.73%\n",
      "Epoch [53/300] | Train Loss: 0.7745 (Cls: 0.2031, Reg: 0.5714) | Train Acc: 94.19% || Val Loss: 0.0991 (Cls: 0.0223, Reg: 0.0768) | Val Acc: 99.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0991) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [54/300] | Train Loss: 0.7722 (Cls: 0.2082, Reg: 0.5640) | Train Acc: 94.12% || Val Loss: 0.1342 (Cls: 0.0335, Reg: 0.1007) | Val Acc: 99.59%\n",
      "Epoch [55/300] | Train Loss: 0.7674 (Cls: 0.2033, Reg: 0.5641) | Train Acc: 94.12% || Val Loss: 0.1171 (Cls: 0.0273, Reg: 0.0898) | Val Acc: 99.67%\n",
      "Epoch [56/300] | Train Loss: 0.7520 (Cls: 0.1902, Reg: 0.5618) | Train Acc: 94.44% || Val Loss: 0.1290 (Cls: 0.0354, Reg: 0.0936) | Val Acc: 99.55%\n",
      "Epoch [57/300] | Train Loss: 0.7661 (Cls: 0.2027, Reg: 0.5634) | Train Acc: 94.13% || Val Loss: 0.1043 (Cls: 0.0289, Reg: 0.0754) | Val Acc: 99.65%\n",
      "Epoch [58/300] | Train Loss: 0.7665 (Cls: 0.1924, Reg: 0.5741) | Train Acc: 94.36% || Val Loss: 0.1519 (Cls: 0.0323, Reg: 0.1196) | Val Acc: 99.59%\n",
      "Epoch [59/300] | Train Loss: 0.7685 (Cls: 0.2054, Reg: 0.5631) | Train Acc: 94.20% || Val Loss: 0.1414 (Cls: 0.0345, Reg: 0.1069) | Val Acc: 99.51%\n",
      "Epoch [60/300] | Train Loss: 0.7492 (Cls: 0.1922, Reg: 0.5570) | Train Acc: 94.73% || Val Loss: 0.1299 (Cls: 0.0279, Reg: 0.1020) | Val Acc: 99.61%\n",
      "Epoch [61/300] | Train Loss: 0.7560 (Cls: 0.1957, Reg: 0.5603) | Train Acc: 94.38% || Val Loss: 0.1366 (Cls: 0.0272, Reg: 0.1094) | Val Acc: 99.61%\n",
      "Epoch [62/300] | Train Loss: 0.7539 (Cls: 0.1946, Reg: 0.5593) | Train Acc: 94.54% || Val Loss: 0.2110 (Cls: 0.0375, Reg: 0.1735) | Val Acc: 99.24%\n",
      "Epoch [63/300] | Train Loss: 0.7456 (Cls: 0.1882, Reg: 0.5574) | Train Acc: 94.76% || Val Loss: 0.1626 (Cls: 0.0241, Reg: 0.1386) | Val Acc: 99.59%\n",
      "Epoch [64/300] | Train Loss: 0.7402 (Cls: 0.1907, Reg: 0.5495) | Train Acc: 94.75% || Val Loss: 0.1110 (Cls: 0.0230, Reg: 0.0880) | Val Acc: 99.63%\n",
      "Epoch [65/300] | Train Loss: 0.7452 (Cls: 0.1838, Reg: 0.5613) | Train Acc: 94.52% || Val Loss: 0.1228 (Cls: 0.0274, Reg: 0.0954) | Val Acc: 99.65%\n",
      "Epoch [66/300] | Train Loss: 0.7317 (Cls: 0.1831, Reg: 0.5486) | Train Acc: 94.72% || Val Loss: 0.1211 (Cls: 0.0278, Reg: 0.0934) | Val Acc: 99.45%\n",
      "Epoch [67/300] | Train Loss: 0.7103 (Cls: 0.1736, Reg: 0.5366) | Train Acc: 95.04% || Val Loss: 0.1505 (Cls: 0.0257, Reg: 0.1247) | Val Acc: 99.49%\n",
      "Epoch [68/300] | Train Loss: 0.7195 (Cls: 0.1847, Reg: 0.5348) | Train Acc: 94.92% || Val Loss: 0.1345 (Cls: 0.0341, Reg: 0.1004) | Val Acc: 99.43%\n",
      "Epoch [69/300] | Train Loss: 0.7427 (Cls: 0.1945, Reg: 0.5482) | Train Acc: 94.85% || Val Loss: 0.1814 (Cls: 0.0389, Reg: 0.1425) | Val Acc: 99.33%\n",
      "Epoch [70/300] | Train Loss: 0.6679 (Cls: 0.1643, Reg: 0.5036) | Train Acc: 95.54% || Val Loss: 0.1025 (Cls: 0.0259, Reg: 0.0767) | Val Acc: 99.67%\n",
      "Epoch [71/300] | Train Loss: 0.6440 (Cls: 0.1542, Reg: 0.4897) | Train Acc: 95.76% || Val Loss: 0.1002 (Cls: 0.0214, Reg: 0.0788) | Val Acc: 99.65%\n",
      "Epoch [72/300] | Train Loss: 0.6371 (Cls: 0.1516, Reg: 0.4855) | Train Acc: 95.86% || Val Loss: 0.0913 (Cls: 0.0209, Reg: 0.0704) | Val Acc: 99.67%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0913) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [73/300] | Train Loss: 0.6397 (Cls: 0.1505, Reg: 0.4891) | Train Acc: 95.84% || Val Loss: 0.0949 (Cls: 0.0204, Reg: 0.0745) | Val Acc: 99.78%\n",
      "Epoch [74/300] | Train Loss: 0.6273 (Cls: 0.1485, Reg: 0.4788) | Train Acc: 95.86% || Val Loss: 0.0987 (Cls: 0.0209, Reg: 0.0778) | Val Acc: 99.71%\n",
      "Epoch [75/300] | Train Loss: 0.6226 (Cls: 0.1460, Reg: 0.4767) | Train Acc: 95.97% || Val Loss: 0.0887 (Cls: 0.0246, Reg: 0.0641) | Val Acc: 99.61%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0887) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [76/300] | Train Loss: 0.6344 (Cls: 0.1500, Reg: 0.4844) | Train Acc: 95.83% || Val Loss: 0.0954 (Cls: 0.0198, Reg: 0.0756) | Val Acc: 99.67%\n",
      "Epoch [77/300] | Train Loss: 0.6123 (Cls: 0.1422, Reg: 0.4701) | Train Acc: 95.84% || Val Loss: 0.1008 (Cls: 0.0218, Reg: 0.0790) | Val Acc: 99.67%\n",
      "Epoch [78/300] | Train Loss: 0.6138 (Cls: 0.1424, Reg: 0.4714) | Train Acc: 96.06% || Val Loss: 0.1004 (Cls: 0.0199, Reg: 0.0805) | Val Acc: 99.67%\n",
      "Epoch [79/300] | Train Loss: 0.6296 (Cls: 0.1501, Reg: 0.4795) | Train Acc: 95.78% || Val Loss: 0.1025 (Cls: 0.0219, Reg: 0.0806) | Val Acc: 99.69%\n",
      "Epoch [80/300] | Train Loss: 0.6363 (Cls: 0.1524, Reg: 0.4838) | Train Acc: 95.70% || Val Loss: 0.0847 (Cls: 0.0200, Reg: 0.0647) | Val Acc: 99.71%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0847) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [81/300] | Train Loss: 0.6115 (Cls: 0.1381, Reg: 0.4734) | Train Acc: 96.14% || Val Loss: 0.0934 (Cls: 0.0217, Reg: 0.0717) | Val Acc: 99.67%\n",
      "Epoch [82/300] | Train Loss: 0.5958 (Cls: 0.1321, Reg: 0.4637) | Train Acc: 96.29% || Val Loss: 0.1192 (Cls: 0.0208, Reg: 0.0985) | Val Acc: 99.59%\n",
      "Epoch [83/300] | Train Loss: 0.6063 (Cls: 0.1366, Reg: 0.4697) | Train Acc: 96.15% || Val Loss: 0.1230 (Cls: 0.0217, Reg: 0.1013) | Val Acc: 99.65%\n",
      "Epoch [84/300] | Train Loss: 0.6107 (Cls: 0.1407, Reg: 0.4700) | Train Acc: 96.08% || Val Loss: 0.0826 (Cls: 0.0185, Reg: 0.0642) | Val Acc: 99.73%\n",
      "✅ 儲存最佳模型 (Val Loss: 0.0826) 至 ./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\n",
      "Epoch [85/300] | Train Loss: 0.6119 (Cls: 0.1423, Reg: 0.4696) | Train Acc: 96.04% || Val Loss: 0.0900 (Cls: 0.0267, Reg: 0.0633) | Val Acc: 99.61%\n",
      "Epoch [86/300] | Train Loss: 0.6025 (Cls: 0.1415, Reg: 0.4610) | Train Acc: 96.03% || Val Loss: 0.0859 (Cls: 0.0199, Reg: 0.0660) | Val Acc: 99.71%\n",
      "Epoch [87/300] | Train Loss: 0.6009 (Cls: 0.1358, Reg: 0.4652) | Train Acc: 96.14% || Val Loss: 0.0850 (Cls: 0.0250, Reg: 0.0600) | Val Acc: 99.69%\n",
      "Epoch [88/300] | Train Loss: 0.6085 (Cls: 0.1405, Reg: 0.4680) | Train Acc: 96.09% || Val Loss: 0.0938 (Cls: 0.0208, Reg: 0.0730) | Val Acc: 99.69%\n",
      "Epoch [89/300] | Train Loss: 0.6113 (Cls: 0.1438, Reg: 0.4675) | Train Acc: 95.94% || Val Loss: 0.0941 (Cls: 0.0228, Reg: 0.0712) | Val Acc: 99.63%\n",
      "Epoch [90/300] | Train Loss: 0.6011 (Cls: 0.1398, Reg: 0.4612) | Train Acc: 95.95% || Val Loss: 0.0875 (Cls: 0.0239, Reg: 0.0636) | Val Acc: 99.69%\n",
      "Epoch [91/300] | Train Loss: 0.5969 (Cls: 0.1350, Reg: 0.4619) | Train Acc: 96.43% || Val Loss: 0.0894 (Cls: 0.0252, Reg: 0.0641) | Val Acc: 99.69%\n",
      "Epoch [92/300] | Train Loss: 0.5941 (Cls: 0.1372, Reg: 0.4569) | Train Acc: 96.34% || Val Loss: 0.1003 (Cls: 0.0241, Reg: 0.0762) | Val Acc: 99.65%\n",
      "Epoch [93/300] | Train Loss: 0.5933 (Cls: 0.1385, Reg: 0.4549) | Train Acc: 96.15% || Val Loss: 0.0960 (Cls: 0.0246, Reg: 0.0714) | Val Acc: 99.69%\n",
      "Epoch [94/300] | Train Loss: 0.5902 (Cls: 0.1345, Reg: 0.4557) | Train Acc: 96.35% || Val Loss: 0.0979 (Cls: 0.0245, Reg: 0.0734) | Val Acc: 99.63%\n",
      "Epoch [95/300] | Train Loss: 0.5989 (Cls: 0.1393, Reg: 0.4595) | Train Acc: 96.09% || Val Loss: 0.0910 (Cls: 0.0253, Reg: 0.0657) | Val Acc: 99.65%\n",
      "Epoch [96/300] | Train Loss: 0.5803 (Cls: 0.1332, Reg: 0.4471) | Train Acc: 96.29% || Val Loss: 0.1560 (Cls: 0.0311, Reg: 0.1249) | Val Acc: 99.53%\n",
      "Epoch [97/300] | Train Loss: 0.5772 (Cls: 0.1315, Reg: 0.4457) | Train Acc: 96.17% || Val Loss: 0.0875 (Cls: 0.0237, Reg: 0.0638) | Val Acc: 99.71%\n",
      "Epoch [98/300] | Train Loss: 0.5667 (Cls: 0.1230, Reg: 0.4438) | Train Acc: 96.43% || Val Loss: 0.0962 (Cls: 0.0216, Reg: 0.0746) | Val Acc: 99.73%\n",
      "Epoch [99/300] | Train Loss: 0.5809 (Cls: 0.1292, Reg: 0.4517) | Train Acc: 96.45% || Val Loss: 0.0872 (Cls: 0.0223, Reg: 0.0649) | Val Acc: 99.73%\n",
      "Epoch [100/300] | Train Loss: 0.5836 (Cls: 0.1316, Reg: 0.4520) | Train Acc: 96.41% || Val Loss: 0.0998 (Cls: 0.0206, Reg: 0.0792) | Val Acc: 99.73%\n",
      "Epoch [101/300] | Train Loss: 0.5680 (Cls: 0.1334, Reg: 0.4346) | Train Acc: 96.52% || Val Loss: 0.0934 (Cls: 0.0216, Reg: 0.0717) | Val Acc: 99.69%\n",
      "Epoch [102/300] | Train Loss: 0.5537 (Cls: 0.1170, Reg: 0.4367) | Train Acc: 96.63% || Val Loss: 0.0858 (Cls: 0.0244, Reg: 0.0615) | Val Acc: 99.67%\n",
      "Epoch [103/300] | Train Loss: 0.5534 (Cls: 0.1206, Reg: 0.4328) | Train Acc: 96.82% || Val Loss: 0.0874 (Cls: 0.0240, Reg: 0.0634) | Val Acc: 99.65%\n",
      "Epoch [104/300] | Train Loss: 0.5515 (Cls: 0.1193, Reg: 0.4322) | Train Acc: 96.85% || Val Loss: 0.0877 (Cls: 0.0271, Reg: 0.0606) | Val Acc: 99.69%\n",
      "Early stop at epoch 104\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion_cls = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "criterion = MultiTaskUncertaintyLoss().to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n",
    "\n",
    "# -----------------------\n",
    "# COORDINATES 字典：將分類標籤轉為 (X, Y) 座標\n",
    "# -----------------------\n",
    "COORDINATES = {\n",
    "    # 下邊界 (1-10 和 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    # 左邊界 (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    # 上邊界 (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "    # 右邊界 (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    # 中間點 (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        # 將 0-index 轉換成 1-index (例如 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "\n",
    "# -----------------------\n",
    "# 訓練參數與 Early Stopping 設定\n",
    "# -----------------------\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"./models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\"\n",
    "epochs = 300\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "# -----------------------\n",
    "# 訓練迴圈 (分類與回歸雙輸出)\n",
    "# -----------------------\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_class_loss = 0.0\n",
    "    train_reg_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # 每個 batch 返回 (amp_inputs, rssi_inputs, labels)\n",
    "    # 其中 labels 為 one-hot 編碼 (用以計算分類損失)\n",
    "    for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        amp_inputs = amp_inputs.to(device)\n",
    "        rssi_inputs = rssi_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "        \n",
    "        # 分類目標：one-hot -> class index\n",
    "        target_class = torch.argmax(labels, dim=1)\n",
    "        loss_class = criterion_cls(class_out, target_class)\n",
    "        \n",
    "        # 回歸目標：根據 target_class 透過 COORDINATES 字典取得 (X, Y) 座標\n",
    "        true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "        loss_reg = criterion_reg(reg_out, true_coords)\n",
    "        \n",
    "        loss = criterion(loss_class, loss_reg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        batch_size_curr = amp_inputs.size(0)\n",
    "        train_loss += loss.item() * batch_size_curr\n",
    "        train_class_loss += loss_class.item() * batch_size_curr\n",
    "        train_reg_loss += loss_reg.item() * batch_size_curr\n",
    "        _, predicted = torch.max(class_out, 1)\n",
    "        total_train += batch_size_curr\n",
    "        train_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    avg_train_class_loss = train_class_loss / len(train_loader.dataset)\n",
    "    avg_train_reg_loss = train_reg_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_class_loss = 0.0\n",
    "    val_reg_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "            amp_inputs = amp_inputs.to(device)\n",
    "            rssi_inputs = rssi_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            loss_class = criterion_cls(class_out, target_class)\n",
    "            true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "            loss_reg = criterion_reg(reg_out, true_coords)\n",
    "            loss = loss_class + alpha * loss_reg\n",
    "            \n",
    "            batch_size_curr = amp_inputs.size(0)\n",
    "            val_loss += loss.item() * batch_size_curr\n",
    "            val_class_loss += loss_class.item() * batch_size_curr\n",
    "            val_reg_loss += loss_reg.item() * batch_size_curr\n",
    "            _, predicted = torch.max(class_out, 1)\n",
    "            total_val += batch_size_curr\n",
    "            val_correct += (predicted == target_class).sum().item()\n",
    "    \n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    avg_val_class_loss = val_class_loss / len(val_loader.dataset)\n",
    "    avg_val_reg_loss = val_reg_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    scheduler.step(avg_val_loss)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} (Cls: {avg_train_class_loss:.4f}, Reg: {avg_train_reg_loss:.4f}) | \"\n",
    "          f\"Train Acc: {train_acc:.2f}% || \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} (Cls: {avg_val_class_loss:.4f}, Reg: {avg_val_reg_loss:.4f}) | \"\n",
    "          f\"Val Acc: {val_acc:.2f}%\")\n",
    "    \n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 512,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 測試損失: 0.0840, 測試準確率: 99.59%\n"
     ]
    }
   ],
   "source": [
    "# -----------------------\n",
    "# 損失函數、優化器與學習率調整器設定\n",
    "# -----------------------\n",
    "criterion_cls = nn.CrossEntropyLoss()  # 分類損失：target 為 class index\n",
    "criterion_reg = nn.MSELoss()         # 回歸損失：target 為 (X, Y)\n",
    "criterion = MultiTaskUncertaintyLoss().to(device)\n",
    "# 載入最佳模型\n",
    "model.load_state_dict(torch.load(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN/models_save/rssi_csi_0521_reg_class_MTUL_nostd_rssi32.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 測試模型\n",
    "test_loss = 0.0\n",
    "correct = 0\n",
    "total = 0\n",
    "all_labels = []\n",
    "all_predictions = []\n",
    "all_reg = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for amp_inputs, rssi_input, labels in test_loader:\n",
    "        amp_inputs, rssi_inputs, labels = amp_inputs.to(device), rssi_input.to(device), labels.to(device)\n",
    "        outputs, reg = model(amp_inputs, rssi_inputs)\n",
    "        loss = criterion_cls(outputs, torch.argmax(labels, dim=1))\n",
    "            \n",
    "        test_loss += loss.item() * amp_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "        # 儲存真實標籤與預測標籤\n",
    "        all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "        all_predictions.extend(predicted.cpu().numpy())\n",
    "        all_reg.extend(reg.cpu().numpy())\n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "accuracy = 100 * correct / total\n",
    "\n",
    "print(f\"📊 測試損失: {test_loss:.4f}, 測試準確率: {accuracy:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean distance error: 0.015009651128307246\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "adjusted_predictions = np.array(all_predictions) + 1\n",
    "adjusted_labels = np.array(all_labels) + 1\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # 取出對應的座標\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # 計算歐氏距離\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) , errors\n",
    "\n",
    "mean_error, errors = compute_mean_distance_error(adjusted_labels, adjusted_predictions, COORDINATES)\n",
    "print(\"Mean distance error:\", mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重複測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def split_data(amp, rssi, rp_labels, seed=42, batch_size=32, augment=True, csi_noise_std=0.00, rssi_mask_prob=0.05):\n",
    "    \"\"\"\n",
    "    切分資料為 7:2:1 並回傳 train/val/test DataLoader 與 encoder\n",
    "    \"\"\"\n",
    "    indices = amp.index\n",
    "\n",
    "    # 切分 70% 訓練 + 30% 臨時\n",
    "    train_idx, temp_idx = train_test_split(indices, test_size=0.3, random_state=seed)\n",
    "\n",
    "    # 臨時中的 1/3 為 val，2/3 為 test → 最終 10% + 20%\n",
    "    val_idx, test_idx = train_test_split(temp_idx, test_size=1/3, random_state=seed)\n",
    "\n",
    "    # 根據 index 取出資料\n",
    "    amp_train, amp_val, amp_test = amp.loc[train_idx], amp.loc[val_idx], amp.loc[test_idx]\n",
    "    rssi_train, rssi_val, rssi_test = rssi.loc[train_idx], rssi.loc[val_idx], rssi.loc[test_idx]\n",
    "    y_train, y_val, y_test = rp_labels.loc[train_idx], rp_labels.loc[val_idx], rp_labels.loc[test_idx]\n",
    "\n",
    "    # One-hot 編碼\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    y_train_oh = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "    y_val_oh   = encoder.transform(np.array(y_val).reshape(-1, 1))\n",
    "    y_test_oh  = encoder.transform(np.array(y_test).reshape(-1, 1))\n",
    "\n",
    "    # 建立 Dataset\n",
    "    train_dataset = CSIRSSIDataset(np.array(amp_train), np.array(rssi_train), y_train_oh,\n",
    "                                   augment=augment, csi_noise_std=csi_noise_std, rssi_mask_prob=rssi_mask_prob)\n",
    "    val_dataset   = CSIRSSIDataset(np.array(amp_val), np.array(rssi_val), y_val_oh)\n",
    "    test_dataset  = CSIRSSIDataset(np.array(amp_test), np.array(rssi_test), y_test_oh)\n",
    "\n",
    "    # 建立 DataLoader\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "    test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    return train_loader, val_loader, test_loader, encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/5 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run 1: test acc = 99.71%, test MDE = 0.0086, wrong samples = 7\n",
      "\n",
      "=== Run 2/5 ===\n",
      "Run 2: test acc = 99.63%, test MDE = 0.0154, wrong samples = 9\n",
      "\n",
      "=== Run 3/5 ===\n",
      "Run 3: test acc = 99.67%, test MDE = 0.0117, wrong samples = 8\n",
      "\n",
      "=== Run 4/5 ===\n",
      "Run 4: test acc = 99.67%, test MDE = 0.0119, wrong samples = 8\n",
      "\n",
      "=== Run 5/5 ===\n",
      "Run 5: test acc = 99.67%, test MDE = 0.0114, wrong samples = 8\n",
      "\n",
      "=== Summary ===\n",
      "   run   accuracy       mde                                       wrong_errors\n",
      "0    1  99.714286  0.008574  [1.2000000000000002, 2.6832815729997477, 6.0, ...\n",
      "1    2  99.632653  0.015438  [6.26418390534633, 1.2000000000000002, 6.70820...\n",
      "2    3  99.673469  0.011713  [6.26418390534633, 1.2000000000000002, 5.36656...\n",
      "3    4  99.673469  0.011892  [3.2310988842807027, 1.2000000000000002, 4.242...\n",
      "4    5  99.673469  0.011404  [3.2310988842807027, 1.2000000000000002, 6.462...\n",
      "Saved results to experiment_results.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "\n",
    "# 假設前面已經定義好：\n",
    "# CSIRSSI_DualHead, labels_to_coords, compute_mean_distance_error\n",
    "# train_loader, val_loader, test_loader, COORDINATES 等\n",
    "\n",
    "# hyper-params\n",
    "num_runs = 5\n",
    "alpha = 0.2\n",
    "epochs = 200\n",
    "patience = 20\n",
    "batch_size = 32\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 用來存結果\n",
    "test_accs = []\n",
    "test_mdes = []\n",
    "test_wrong_errors = []\n",
    "\n",
    "for run in range(num_runs):\n",
    "    print(f\"\\n=== Run {run+1}/{num_runs} ===\")\n",
    "    # 1) 重置 model / optimizer / scheduler / early-stop\n",
    "    model = CSIRSSI_DualHead(num_classes=49, rssi_dim=4).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, mode='min', factor=0.5, patience=15, verbose=False\n",
    "    )\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    # 2) 訓練\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "            amp_inputs, rssi_inputs, labels = (\n",
    "                amp_inputs.to(device),\n",
    "                rssi_inputs.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            optimizer.zero_grad()\n",
    "            class_out, reg_out = model(amp_inputs, rssi_inputs)\n",
    "\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            loss_class = nn.CrossEntropyLoss()(class_out, target_class)\n",
    "            true_coords = labels_to_coords(target_class, COORDINATES)\n",
    "            loss_reg = nn.MSELoss()(reg_out, true_coords)\n",
    "\n",
    "            loss = loss_class + alpha * loss_reg\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # validation\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for amp_inputs, rssi_inputs, labels in val_loader:\n",
    "                amp_inputs, rssi_inputs, labels = (\n",
    "                    amp_inputs.to(device),\n",
    "                    rssi_inputs.to(device),\n",
    "                    labels.to(device),\n",
    "                )\n",
    "                co, ro = model(amp_inputs, rssi_inputs)\n",
    "                tc = torch.argmax(labels, dim=1)\n",
    "                lc = nn.CrossEntropyLoss()(co, tc)\n",
    "                rc = nn.MSELoss()(ro, labels_to_coords(tc, COORDINATES))\n",
    "                val_loss += (lc + alpha * rc).item() * amp_inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # 3) 測試並計算指標\n",
    "    model.eval()\n",
    "    all_labels, all_preds = [], []\n",
    "    with torch.no_grad():\n",
    "        for amp_inputs, rssi_inputs, labels in test_loader:\n",
    "            amp_inputs, rssi_inputs, labels = (\n",
    "                amp_inputs.to(device),\n",
    "                rssi_inputs.to(device),\n",
    "                labels.to(device),\n",
    "            )\n",
    "            co, _ = model(amp_inputs, rssi_inputs)\n",
    "            preds = torch.argmax(co, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(torch.argmax(labels, dim=1).cpu().numpy())\n",
    "\n",
    "    # +1 轉回 1~49\n",
    "    y_true = np.array(all_labels) + 1\n",
    "    y_pred = np.array(all_preds)  + 1\n",
    "    acc = 100 * np.mean(y_true == y_pred)\n",
    "    mde, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "    test_accs.append(acc)\n",
    "    test_mdes.append(mde)\n",
    "    # 只保留錯誤樣本的 distance error (>0)\n",
    "    wrong = [e for e in errors if e > 0]\n",
    "    test_wrong_errors.append(wrong)\n",
    "\n",
    "    print(f\"Run {run+1}: test acc = {acc:.2f}%, test MDE = {mde:.4f}, wrong samples = {len(wrong)}\")\n",
    "\n",
    "# 4) 整理成 DataFrame，並存成 CSV\n",
    "results = pd.DataFrame({\n",
    "    'run': list(range(1, num_runs+1)),\n",
    "    'accuracy': test_accs,\n",
    "    'mde': test_mdes,\n",
    "    'wrong_errors': test_wrong_errors\n",
    "})\n",
    "\n",
    "print(\"\\n=== Summary ===\")\n",
    "print(results)\n",
    "\n",
    "# 存檔\n",
    "results.to_csv('./repeat/02/experiment_results02.csv', index=False)\n",
    "print(\"Saved results to experiment_results.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
