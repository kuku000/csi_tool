{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN Ê®°ÂûãÈ†êÊ∏¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csidataset import *\n",
    "import data_loader\n",
    "from data_loader import *\n",
    "sys.path.append(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool\")\n",
    "import denoise\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/1223_phone/5G/20MHz/csv/all\"\n",
    "#base_path2 = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/1222_phone/5G/20MHz/csv/all\"\n",
    "#base_path3 = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/1223_phone/5G/20MHz/csv/all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_points = {}\n",
    "reference_points2 = {}\n",
    "reference_points3 = {}\n",
    "spacing = 0.6  # ÊØèÈöî 0.6m\n",
    "\n",
    "for ref_id, coord in data_loader.COORDINATES.items():\n",
    "    folder_path = os.path.join(base_path, f\"reference_point_{ref_id}.xlsx\")\n",
    "    reference_points[folder_path] = (ref_id, coord)\n",
    "\n",
    "# for ref_id, coord in data_loader.COORDINATES.items():\n",
    "#     folder_path = os.path.join(base_path2, f\"reference_point_{ref_id}.xlsx\")\n",
    "#     reference_points2[folder_path] = (ref_id, coord)\n",
    "\n",
    "\n",
    "# for ref_id, coord in data_loader.COORDINATES.items():\n",
    "#     folder_path = os.path.join(base_path3, f\"reference_point_{ref_id}.xlsx\")\n",
    "#     reference_points3[folder_path] = (ref_id, coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, rp_labels, coord_labels = load_data(reference_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(24500, 98)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Âêà‰ΩµÂæåË≥áÊñôÂûãÂà•Ôºö\n",
      "data_filtered: <class 'pandas.core.frame.DataFrame'> (19649, 98)\n",
      "rp_labels_filtered: <class 'numpy.ndarray'> (19649,)\n",
      "coord_labels_filtered: <class 'pandas.core.frame.DataFrame'> (19649, 2)\n",
      "\n",
      "ÊØèÂÄã label Á≠ÜÊï∏ÔºàÊáâË©≤ÈÉΩÁÇ∫ 401ÔºâÔºö\n",
      "1     401\n",
      "2     401\n",
      "3     401\n",
      "4     401\n",
      "5     401\n",
      "6     401\n",
      "7     401\n",
      "8     401\n",
      "9     401\n",
      "10    401\n",
      "11    401\n",
      "12    401\n",
      "13    401\n",
      "14    401\n",
      "15    401\n",
      "16    401\n",
      "17    401\n",
      "18    401\n",
      "19    401\n",
      "20    401\n",
      "21    401\n",
      "22    401\n",
      "23    401\n",
      "24    401\n",
      "25    401\n",
      "26    401\n",
      "27    401\n",
      "28    401\n",
      "29    401\n",
      "30    401\n",
      "31    401\n",
      "32    401\n",
      "33    401\n",
      "34    401\n",
      "35    401\n",
      "36    401\n",
      "37    401\n",
      "38    401\n",
      "39    401\n",
      "40    401\n",
      "41    401\n",
      "42    401\n",
      "43    401\n",
      "44    401\n",
      "45    401\n",
      "46    401\n",
      "47    401\n",
      "48    401\n",
      "49    401\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_count = 401\n",
    "unique_labels = np.unique(rp_labels)\n",
    "\n",
    "filtered_data = []\n",
    "filtered_rp_labels = []\n",
    "filtered_coord_labels = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    indices = np.where(rp_labels == label)[0]\n",
    "\n",
    "    if len(indices) < target_count:\n",
    "        print(f\"Label {label} only has {len(indices)} samples, skipped.\")\n",
    "        continue\n",
    "\n",
    "    selected_indices = indices[:target_count]  # ‚úÖ Âõ∫ÂÆöÂèñÊúÄÂâçÈù¢ÁöÑ 401 Á≠Ü\n",
    "\n",
    "    filtered_data.append(data.iloc[selected_indices])\n",
    "    filtered_rp_labels.append(rp_labels[selected_indices])\n",
    "    filtered_coord_labels.append(coord_labels.iloc[selected_indices])\n",
    "\n",
    "# ‚úÖ Âêà‰ΩµÔºåÁ¢∫‰øùÂûãÂà•‰∏ÄËá¥\n",
    "data_filtered = pd.concat(filtered_data, ignore_index=True)  # DataFrame\n",
    "rp_labels_filtered = np.concatenate(filtered_rp_labels, axis=0)  # ndarray\n",
    "coord_labels_filtered = pd.concat(filtered_coord_labels, ignore_index=True)  # DataFrame\n",
    "\n",
    "# ‚úÖ È©óË≠â\n",
    "print(\"‚úÖ Âêà‰ΩµÂæåË≥áÊñôÂûãÂà•Ôºö\")\n",
    "print(\"data_filtered:\", type(data_filtered), data_filtered.shape)\n",
    "print(\"rp_labels_filtered:\", type(rp_labels_filtered), rp_labels_filtered.shape)\n",
    "print(\"coord_labels_filtered:\", type(coord_labels_filtered), coord_labels_filtered.shape)\n",
    "\n",
    "print(\"\\nÊØèÂÄã label Á≠ÜÊï∏ÔºàÊáâË©≤ÈÉΩÁÇ∫ 401ÔºâÔºö\")\n",
    "print(pd.Series(rp_labels_filtered).value_counts().sort_index())\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>893.573164</td>\n",
       "      <td>888.473973</td>\n",
       "      <td>871.023536</td>\n",
       "      <td>836.316328</td>\n",
       "      <td>841.770753</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387518</td>\n",
       "      <td>-1.581549</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>-2.080869</td>\n",
       "      <td>-2.287379</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>672.521375</td>\n",
       "      <td>687.674342</td>\n",
       "      <td>780.946221</td>\n",
       "      <td>803.560203</td>\n",
       "      <td>863.103702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.677159</td>\n",
       "      <td>-2.877730</td>\n",
       "      <td>-3.051501</td>\n",
       "      <td>2.738820</td>\n",
       "      <td>2.503722</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>713.950278</td>\n",
       "      <td>736.619983</td>\n",
       "      <td>852.877482</td>\n",
       "      <td>846.442556</td>\n",
       "      <td>880.032954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176051</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.251636</td>\n",
       "      <td>-0.776139</td>\n",
       "      <td>-1.023850</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>800.870776</td>\n",
       "      <td>791.982323</td>\n",
       "      <td>778.923616</td>\n",
       "      <td>763.872372</td>\n",
       "      <td>766.167084</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.031820</td>\n",
       "      <td>-1.194379</td>\n",
       "      <td>-1.338067</td>\n",
       "      <td>-1.666445</td>\n",
       "      <td>-1.878363</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>875.416472</td>\n",
       "      <td>878.300632</td>\n",
       "      <td>854.687077</td>\n",
       "      <td>857.886939</td>\n",
       "      <td>843.664033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445534</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-1.090942</td>\n",
       "      <td>-1.286103</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>381.980366</td>\n",
       "      <td>398.527289</td>\n",
       "      <td>370.951479</td>\n",
       "      <td>372.108855</td>\n",
       "      <td>352.346420</td>\n",
       "      <td>359.233907</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>413.997585</td>\n",
       "      <td>420.138073</td>\n",
       "      <td>454.862617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.475208</td>\n",
       "      <td>-0.668036</td>\n",
       "      <td>-0.872238</td>\n",
       "      <td>-1.396551</td>\n",
       "      <td>-1.667962</td>\n",
       "      <td>-1.977215</td>\n",
       "      <td>-2.212040</td>\n",
       "      <td>-2.563840</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>737.651679</td>\n",
       "      <td>730.928861</td>\n",
       "      <td>707.471554</td>\n",
       "      <td>673.190909</td>\n",
       "      <td>665.090219</td>\n",
       "      <td>689.385233</td>\n",
       "      <td>679.574131</td>\n",
       "      <td>666.066813</td>\n",
       "      <td>646.891026</td>\n",
       "      <td>636.792745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.602202</td>\n",
       "      <td>0.417030</td>\n",
       "      <td>0.233346</td>\n",
       "      <td>-0.227937</td>\n",
       "      <td>-0.474118</td>\n",
       "      <td>-0.747335</td>\n",
       "      <td>-1.038664</td>\n",
       "      <td>-1.386254</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>364.329521</td>\n",
       "      <td>391.318285</td>\n",
       "      <td>383.775976</td>\n",
       "      <td>384.610192</td>\n",
       "      <td>367.828765</td>\n",
       "      <td>339.330223</td>\n",
       "      <td>346.236913</td>\n",
       "      <td>376.766506</td>\n",
       "      <td>421.283752</td>\n",
       "      <td>448.486343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877905</td>\n",
       "      <td>-1.877905</td>\n",
       "      <td>2.906870</td>\n",
       "      <td>1.246783</td>\n",
       "      <td>0.451070</td>\n",
       "      <td>-0.370119</td>\n",
       "      <td>-1.260481</td>\n",
       "      <td>-2.098871</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>396.021464</td>\n",
       "      <td>409.890229</td>\n",
       "      <td>392.439804</td>\n",
       "      <td>391.154701</td>\n",
       "      <td>374.786606</td>\n",
       "      <td>380.443163</td>\n",
       "      <td>384.193961</td>\n",
       "      <td>434.991954</td>\n",
       "      <td>450.813709</td>\n",
       "      <td>472.956658</td>\n",
       "      <td>...</td>\n",
       "      <td>1.654569</td>\n",
       "      <td>1.488145</td>\n",
       "      <td>1.336005</td>\n",
       "      <td>0.979841</td>\n",
       "      <td>0.765983</td>\n",
       "      <td>0.532844</td>\n",
       "      <td>0.208257</td>\n",
       "      <td>-0.127813</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>734.994558</td>\n",
       "      <td>726.389703</td>\n",
       "      <td>717.432227</td>\n",
       "      <td>688.447529</td>\n",
       "      <td>683.183723</td>\n",
       "      <td>684.211225</td>\n",
       "      <td>688.442445</td>\n",
       "      <td>673.167141</td>\n",
       "      <td>681.950145</td>\n",
       "      <td>663.717560</td>\n",
       "      <td>...</td>\n",
       "      <td>-3.017652</td>\n",
       "      <td>2.908532</td>\n",
       "      <td>2.559763</td>\n",
       "      <td>1.732479</td>\n",
       "      <td>1.298008</td>\n",
       "      <td>0.859337</td>\n",
       "      <td>0.389644</td>\n",
       "      <td>-0.102262</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19649 rows √ó 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "0      919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
       "1      698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
       "2      745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
       "3      830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
       "4      906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "19644  381.980366  398.527289  370.951479  372.108855  352.346420  359.233907   \n",
       "19645  737.651679  730.928861  707.471554  673.190909  665.090219  689.385233   \n",
       "19646  364.329521  391.318285  383.775976  384.610192  367.828765  339.330223   \n",
       "19647  396.021464  409.890229  392.439804  391.154701  374.786606  380.443163   \n",
       "19648  734.994558  726.389703  717.432227  688.447529  683.183723  684.211225   \n",
       "\n",
       "               6           7           8           9   ...        88  \\\n",
       "0      888.473973  871.023536  836.316328  841.770753  ... -1.387518   \n",
       "1      687.674342  780.946221  803.560203  863.103702  ... -2.677159   \n",
       "2      736.619983  852.877482  846.442556  880.032954  ...  0.176051   \n",
       "3      791.982323  778.923616  763.872372  766.167084  ... -1.031820   \n",
       "4      878.300632  854.687077  857.886939  843.664033  ... -0.445534   \n",
       "...           ...         ...         ...         ...  ...       ...   \n",
       "19644  370.000000  413.997585  420.138073  454.862617  ... -0.475208   \n",
       "19645  679.574131  666.066813  646.891026  636.792745  ...  0.602202   \n",
       "19646  346.236913  376.766506  421.283752  448.486343  ... -1.877905   \n",
       "19647  384.193961  434.991954  450.813709  472.956658  ...  1.654569   \n",
       "19648  688.442445  673.167141  681.950145  663.717560  ... -3.017652   \n",
       "\n",
       "             89        90        91        92        93        94        95  \\\n",
       "0     -1.581549 -1.708760 -2.080869 -2.287379 -2.467120 -2.666997 -2.982241   \n",
       "1     -2.877730 -3.051501  2.738820  2.503722  2.256683  2.035376  1.737005   \n",
       "2     -0.063704 -0.251636 -0.776139 -1.023850 -1.275624 -1.535994 -1.899522   \n",
       "3     -1.194379 -1.338067 -1.666445 -1.878363 -2.100073 -2.369892 -2.666982   \n",
       "4     -0.593750 -0.783166 -1.090942 -1.286103 -1.497279 -1.784787 -2.076610   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "19644 -0.668036 -0.872238 -1.396551 -1.667962 -1.977215 -2.212040 -2.563840   \n",
       "19645  0.417030  0.233346 -0.227937 -0.474118 -0.747335 -1.038664 -1.386254   \n",
       "19646 -1.877905  2.906870  1.246783  0.451070 -0.370119 -1.260481 -2.098871   \n",
       "19647  1.488145  1.336005  0.979841  0.765983  0.532844  0.208257 -0.127813   \n",
       "19648  2.908532  2.559763  1.732479  1.298008  0.859337  0.389644 -0.102262   \n",
       "\n",
       "         96     97  \n",
       "0     -50.0  136.0  \n",
       "1     -49.0  148.0  \n",
       "2     -50.0  224.0  \n",
       "3     -51.0  136.0  \n",
       "4     -50.0  136.0  \n",
       "...     ...    ...  \n",
       "19644 -56.0  148.0  \n",
       "19645 -58.0  136.0  \n",
       "19646 -57.0  148.0  \n",
       "19647 -58.0  224.0  \n",
       "19648 -58.0  136.0  \n",
       "\n",
       "[19649 rows x 98 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_data = np.array(data_filtered.iloc[:, :48])  \n",
    "phase_data = np.array(data_filtered.iloc[:, 48:-2]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# amp_data1_2 = np.concatenate((amp_data, amp_data2), axis=0)\n",
    "# phase_data1_2 = np.concatenate((phase_data, phase_data2), axis=0)\n",
    "# rp_labels1_2 = np.concatenate((rp_labels, rp_labels2), axis=0)\n",
    "# coord_labels1_2 = np.concatenate((coord_labels, coord_labels2), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#amp_d = denoise.preprocess_csi_for_fingerprint2(amp_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels = encoder.fit_transform(np.array(rp_labels_filtered).reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_train, amp_temp, y_train, y_temp = train_test_split(amp_data, one_hot_labels, test_size=0.3, random_state=42)\n",
    "amp_val, amp_test, y_val, y_test = train_test_split(amp_temp, y_temp, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13754, 48)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÂâµÂª∫ Dataset Âíå DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CSIDataset(amp_train, y_train)\n",
    "val_dataset = CSIDataset(amp_val, y_val)\n",
    "test_dataset = CSIDataset(amp_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãË®≠ÂÆö\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "            Conv1d-1               [-1, 64, 48]             256\n",
      "       BatchNorm1d-2               [-1, 64, 48]             128\n",
      "         MaxPool1d-3               [-1, 64, 24]               0\n",
      "            Conv1d-4              [-1, 128, 24]          24,704\n",
      "       BatchNorm1d-5              [-1, 128, 24]             256\n",
      "         MaxPool1d-6              [-1, 128, 12]               0\n",
      "            Linear-7                  [-1, 128]         196,736\n",
      "           Dropout-8                  [-1, 128]               0\n",
      "            Linear-9                   [-1, 64]           8,256\n",
      "          Dropout-10                   [-1, 64]               0\n",
      "           Linear-11                   [-1, 49]           3,185\n",
      "================================================================\n",
      "Total params: 233,521\n",
      "Trainable params: 233,521\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 0.12\n",
      "Params size (MB): 0.89\n",
      "Estimated Total Size (MB): 1.01\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "class CNNClassifier(nn.Module):\n",
    "    def __init__(self, num_classes=49):\n",
    "        super(CNNClassifier, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.flatten_dim = 128 * 12  # 48 -> 24 -> 12 after pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        self.fc3 = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure input is in the shape (batch_size, channels, length)\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "\n",
    "        if x.shape[1] != 1:\n",
    "            x = x.permute(0, 2, 1)  # (batch_size, 1, 48)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Initialize the model\n",
    "model = CNNClassifier(num_classes=49).to(device)\n",
    "\n",
    "# Print model summary\n",
    "summary(model, input_size=(1, 48))  # Input shape: (channels, length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ê®°ÂûãË®ìÁ∑¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# ÊêçÂ§±ÂáΩÊï∏\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# ÂÑ™ÂåñÂô®\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Â≠∏ÁøíÁéáË™øÊï¥Âô®\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Â§öÊ¨°Ê∏¨Ë©¶ cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Â§öÊ¨°Ê∏¨Ë©¶new + best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    \"\"\"\n",
    "    y_true, y_pred: ‰∏ÄÁ∂≠ÁöÑ NumPy Èô£ÂàóÔºåÂàÜÂà•Â≠òÊîæÁúüÂØ¶ÂíåÈ†êÊ∏¨ÁöÑ labelÔºàÊï¥Êï∏Ôºâ\n",
    "    coordinates: dict, label -> (x, y)\n",
    "    \"\"\"\n",
    "    errors = []\n",
    "\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        # ÂèñÂá∫Â∞çÊáâÁöÑÂ∫ßÊ®ô\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            # Ëã•ÊüêÂÄã label ‰∏çÂú®Â∫ßÊ®ôÂ≠óÂÖ∏ÂÖßÔºåÂ∞±Ë∑≥ÈÅéÔºàÊàñË¶ñÈúÄÊ±ÇËôïÁêÜÔºâ\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        # Ë®àÁÆóÊ≠êÊ∞èË∑ùÈõ¢\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors) , errors\n",
    "\n",
    "COORDINATES = {\n",
    "    # ‰∏ãÈÇäÁïå (1-10 Âíå 40-31)\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "\n",
    "    # Â∑¶ÈÇäÁïå (1-11)\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "    6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "\n",
    "    # ‰∏äÈÇäÁïå (11-21)\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "\n",
    "    # Âè≥ÈÇäÁïå (21-31)\n",
    "    22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "    26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "\n",
    "    # ‰∏≠ÈñìÈªû (41-49)\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "    44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "    47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/5 ===\n",
      "‚úÖ Run 1: Acc = 99.59%, MDE = 0.0141\n",
      "\n",
      "=== Run 2/5 ===\n",
      "‚úÖ Run 2: Acc = 99.64%, MDE = 0.0132\n",
      "\n",
      "=== Run 3/5 ===\n",
      "‚úÖ Run 3: Acc = 98.93%, MDE = 0.0308\n",
      "\n",
      "=== Run 4/5 ===\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0169\n",
      "\n",
      "=== Run 5/5 ===\n",
      "‚úÖ Run 5: Acc = 99.34%, MDE = 0.0269\n",
      "üìÅ ÊâÄÊúâÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥ repeat_copy/00/\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from torchsummary import summary\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ÂÅáË®≠‰Ω†Â∑≤ÂÆöÁæ© CNNClassifier È°ûÂà•\n",
    "# ÂÅáË®≠ COORDINATES Ëàá compute_mean_distance_error Â∑≤Ê≠£Á¢∫ÂÆöÁæ©\n",
    "\n",
    "num_runs = 5\n",
    "epochs = 200\n",
    "patience = 20\n",
    "\n",
    "test_accs = []\n",
    "test_mdes = []\n",
    "all_run_errors = []\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    print(f\"\\n=== Run {run}/{num_runs} ===\")\n",
    "    model = CNNClassifier(num_classes=49).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    best_val_loss = float('inf')\n",
    "    counter = 0\n",
    "\n",
    "    best_model_path = \"best_model_tmp.pth\"\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for csi_inputs, labels in train_loader:\n",
    "            csi_inputs, labels = csi_inputs.to(device), labels.to(device)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            optimizer.zero_grad()\n",
    "            class_out = model(csi_inputs)\n",
    "            loss = criterion(class_out, target_class)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # È©óË≠â\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for csi_inputs, labels in val_loader:\n",
    "                csi_inputs, labels = csi_inputs.to(device), labels.to(device)\n",
    "                target_class = torch.argmax(labels, dim=1)\n",
    "                class_out = model(csi_inputs)\n",
    "                val_loss += criterion(class_out, target_class).item() * csi_inputs.size(0)\n",
    "\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            torch.save(model.state_dict(), best_model_path)\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Ê∏¨Ë©¶\n",
    "    model.load_state_dict(torch.load(best_model_path))\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for csi_inputs, labels in test_loader:\n",
    "            csi_inputs, labels = csi_inputs.to(device), labels.to(device)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            class_out = model(csi_inputs)\n",
    "            pred = torch.argmax(class_out, dim=1)\n",
    "            all_pred.extend(pred.cpu().numpy())\n",
    "            all_true.extend(target_class.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(all_true) + 1\n",
    "    y_pred = np.array(all_pred) + 1\n",
    "    acc = 100 * np.mean(y_true == y_pred)\n",
    "    mde, error = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "    test_accs.append(acc)\n",
    "    test_mdes.append(mde)\n",
    "    all_run_errors.append(error)\n",
    "\n",
    "    print(f\"‚úÖ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "# === ÂÑ≤Â≠òÊâÄÊúâÁµêÊûú ===\n",
    "output_folder = \"repeat_copy/00\"\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# 1. ÊØè run ÁöÑ acc/mde\n",
    "df = pd.DataFrame({\n",
    "    \"run\": list(range(1, num_runs + 1)),\n",
    "    \"accuracy\": test_accs,\n",
    "    \"mde\": test_mdes\n",
    "})\n",
    "df.to_csv(f\"{output_folder}/csicls_results00_b.csv\", index=False)\n",
    "\n",
    "# 2. ÊâÄÊúâÊ∏¨Ë©¶Ë™§Â∑ÆÔºàÈï∑Ê†ºÂºèÔºâ\n",
    "errors_flat = []\n",
    "for run_idx, errors in enumerate(all_run_errors):\n",
    "    for sample_idx, e in enumerate(errors):\n",
    "        errors_flat.append({\n",
    "            \"run\": run_idx + 1,\n",
    "            \"sample_idx\": sample_idx + 1,\n",
    "            \"error\": e\n",
    "        })\n",
    "df_errors = pd.DataFrame(errors_flat)\n",
    "df_errors.to_csv(f\"{output_folder}/csicls_all_errors00_b.csv\", index=False)\n",
    "\n",
    "# 3. SummaryÔºàÂπ≥ÂùáËàáÊ®ôÊ∫ñÂ∑ÆÔºâ\n",
    "summary_df = pd.DataFrame([{\n",
    "    \"avg_acc\": np.mean(test_accs),\n",
    "    \"std_acc\": np.std(test_accs),\n",
    "    \"avg_mde\": np.mean(test_mdes),\n",
    "    \"std_mde\": np.std(test_mdes)\n",
    "}])\n",
    "summary_df.to_csv(f\"{output_folder}/csicls_summary00_b.csv\", index=False)\n",
    "\n",
    "print(\"üìÅ ÊâÄÊúâÁµêÊûúÂ∑≤ÂÑ≤Â≠òËá≥ repeat_copy/00/\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as npv\n",
    "\n",
    "# ÂÅáË®≠ errors ÊòØÊâÄÊúâÊ∏¨Ë©¶ error (‰∏ÄÁ∂≠ array)\n",
    "sorted_errors = np.sort(errors)\n",
    "cdf = np.arange(1, len(sorted_errors) + 1) / len(sorted_errors)\n",
    "\n",
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(sorted_errors, cdf, label=\"CDF\")\n",
    "plt.xlabel(\"Localization Error (m)\")\n",
    "plt.ylabel(\"Cumulative Probability\")\n",
    "plt.title(\"CDF (Zoom in: 99-100%)\")\n",
    "plt.ylim(0.99, 1.00)        # ÈáçÈªûÔºöÂ£ìÁ∏Æ Y Ëª∏ÔºåÂè™È°ØÁ§∫ 0.99~1.00\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"repeat/00/csicls_error_cdf_zoom99.png\", dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Â§öËº∏Âá∫"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CNN_DualHead(nn.Module):\n",
    "    def __init__(self, num_classes=49, regression_dim=2):\n",
    "        super(CNN_DualHead, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "\n",
    "        self.flatten_dim = 128 * 12  # 48 -> 24 -> 12 after pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(self.flatten_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        # Two heads: classification & regression\n",
    "        self.class_head = nn.Linear(64, num_classes)     # ÂàÜÈ°û head\n",
    "        self.reg_head = nn.Linear(64, regression_dim)    # ÂõûÊ≠∏ head (È†êË®≠ 2 Á∂≠)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if x.dim() == 2:\n",
    "            x = x.unsqueeze(1)\n",
    "        if x.shape[1] != 1:\n",
    "            x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        class_out = self.class_head(x)  # ÂàÜÈ°ûËº∏Âá∫ (logits)\n",
    "        reg_out = self.reg_head(x)      # ÂõûÊ≠∏Ëº∏Âá∫ (ÈÄöÂ∏∏ 2 Á∂≠)\n",
    "\n",
    "        return class_out, reg_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "ÂàÜÈ°ûËº∏Âá∫ (class_out): torch.Size([32, 49])\n",
      "ÂõûÊ≠∏Ëº∏Âá∫ (reg_out): torch.Size([32, 2])\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model = CNN_DualHead(num_classes=49).to(device)\n",
    "\n",
    "# ÂÅáË®≠‰Ω†ÁöÑËº∏ÂÖ•ÊòØ (batch_size, 48)\n",
    "dummy_input = torch.randn(32, 48).to(device)\n",
    "cls_out, reg_out = model(dummy_input)\n",
    "\n",
    "print(\"ÂàÜÈ°ûËº∏Âá∫ (class_out):\", cls_out.shape)  # ÊáâÁÇ∫ (32, 49)\n",
    "print(\"ÂõûÊ≠∏Ëº∏Âá∫ (reg_out):\", reg_out.shape)    # ÊáâÁÇ∫ (32, 2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary  # torchinfo ÂèØÁî®‰æÜÈ°ØÁ§∫Â§öËº∏ÂÖ•Ê®°ÂûãÊëòË¶Å\n",
    "import matplotlib.pyplot as plt\n",
    "# -----------------------\n",
    "# ÊêçÂ§±ÂáΩÊï∏„ÄÅÂÑ™ÂåñÂô®ËàáÂ≠∏ÁøíÁéáË™øÊï¥Âô®Ë®≠ÂÆö\n",
    "# -----------------------\n",
    "criterion = nn.CrossEntropyLoss()  # ÂàÜÈ°ûÊêçÂ§±Ôºötarget ÁÇ∫ class index\n",
    "criterion_reg = nn.MSELoss()         # ÂõûÊ≠∏ÊêçÂ§±Ôºötarget ÁÇ∫ (X, Y)\n",
    "alpha = 0.5  # ÂõûÊ≠∏ÊêçÂ§±Ê¨äÈáçÔºåÂèØÊ†πÊìöÈúÄË¶ÅË™øÊï¥\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred: ‰∏ÄÁ∂≠ÁöÑ NumPy Èô£ÂàóÔºåÂàÜÂà•Â≠òÊîæÁúüÂØ¶ÂíåÈ†êÊ∏¨ÁöÑ labelÔºàÊï¥Êï∏Ôºâ\n",
    "#     coordinates: dict, label -> (x, y)\n",
    "#     \"\"\"\n",
    "#     errors = []\n",
    "\n",
    "#     for true_label, pred_label in zip(y_true, y_pred):\n",
    "#         # ÂèñÂá∫Â∞çÊáâÁöÑÂ∫ßÊ®ô\n",
    "#         if true_label not in coordinates or pred_label not in coordinates:\n",
    "#             # Ëã•ÊüêÂÄã label ‰∏çÂú®Â∫ßÊ®ôÂ≠óÂÖ∏ÂÖßÔºåÂ∞±Ë∑≥ÈÅéÔºàÊàñË¶ñÈúÄÊ±ÇËôïÁêÜÔºâ\n",
    "#             print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "#             continue\n",
    "#         true_coord = np.array(coordinates[true_label])\n",
    "#         pred_coord = np.array(coordinates[pred_label])\n",
    "#         # Ë®àÁÆóÊ≠êÊ∞èË∑ùÈõ¢\n",
    "#         error = np.linalg.norm(pred_coord - true_coord)\n",
    "#         errors.append(error)\n",
    "#     return np.mean(errors) , errors\n",
    "\n",
    "# COORDINATES = {\n",
    "#     # ‰∏ãÈÇäÁïå (1-10 Âíå 40-31)\n",
    "#     1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "#     36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "#     # Â∑¶ÈÇäÁïå (1-11)\n",
    "#     2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "#     6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "#     # ‰∏äÈÇäÁïå (11-21)\n",
    "#     12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "#     16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "#     20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "#     # Âè≥ÈÇäÁïå (21-31)\n",
    "#     22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "#     26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "#     # ‰∏≠ÈñìÈªû (41-49)\n",
    "#     41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "#     44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "#     47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "# }\n",
    "\n",
    "# def labels_to_coords(label_tensor, coord_dict):\n",
    "#     coords = []\n",
    "#     for label in label_tensor:\n",
    "#         # Â∞á 0-index ËΩâÊèõÊàê 1-index (‰æãÂ¶Ç 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "#         coords.append(coord_dict[label.item() + 1])\n",
    "#     return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "# # ---------- 3. Ë®ìÁ∑¥Ë®≠ÂÆö ----------\n",
    "# device   = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# criterion_cls  = nn.CrossEntropyLoss()\n",
    "# criterion_reg  = nn.MSELoss()\n",
    "\n",
    "# alphas   = np.arange(0.1, 1.1, 0.1)\n",
    "# num_runs = 5\n",
    "# epochs   = 200\n",
    "# patience = 20\n",
    "\n",
    "# summary_results = []\n",
    "\n",
    "# # ÂÅáË®≠ train_loader / val_loader / test_loader ÂÉÖÂõûÂÇ≥ (csi_inputs, labels)\n",
    "# for alpha in alphas:\n",
    "#     run_acc, run_mde, run_mde_std = [], [], []\n",
    "\n",
    "#     print(f\"\\n[Alpha = {alpha:.1f}]  Start {num_runs} runs\")\n",
    "#     for run in range(1, num_runs + 1):\n",
    "#         print(f\"Run {run}/{num_runs}\")\n",
    "#         model = CNN_DualHead(num_classes=49).to(device)\n",
    "#         optimzr = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#         sched   = optim.lr_scheduler.ReduceLROnPlateau(optimzr, 'min', 0.5, patience=15)\n",
    "#         best_val, wait = float('inf'), 0\n",
    "\n",
    "#         # ----- training with early-stop -----\n",
    "#         for epoch in range(epochs):\n",
    "#             model.train()\n",
    "#             for csi_x, labels in train_loader:\n",
    "#                 csi_x, labels = csi_x.to(device), labels.to(device)\n",
    "#                 target = torch.argmax(labels, 1)\n",
    "\n",
    "#                 cls_out, reg_out = model(csi_x)\n",
    "#                 loss_cls = criterion_cls(cls_out, target)\n",
    "#                 loss_reg = criterion_reg(reg_out, labels_to_coords(target, COORDINATES))\n",
    "#                 loss = loss_cls + alpha * loss_reg\n",
    "\n",
    "#                 optimzr.zero_grad(); loss.backward(); optimzr.step()\n",
    "\n",
    "#             # ----- validation -----\n",
    "#             model.eval(); val_loss = 0\n",
    "#             with torch.no_grad():\n",
    "#                 for csi_x, labels in val_loader:\n",
    "#                     csi_x, labels = csi_x.to(device), labels.to(device)\n",
    "#                     target = torch.argmax(labels, 1)\n",
    "#                     cls_out, reg_out = model(csi_x)\n",
    "#                     v_loss = criterion_cls(cls_out, target) \\\n",
    "#                            + alpha * criterion_reg(reg_out, labels_to_coords(target, COORDINATES))\n",
    "#                     val_loss += v_loss.item() * csi_x.size(0)\n",
    "\n",
    "#             val_loss /= len(val_loader.dataset)\n",
    "#             sched.step(val_loss)\n",
    "\n",
    "#             if val_loss < best_val:\n",
    "#                 best_val, wait = val_loss, 0\n",
    "#             else:\n",
    "#                 wait += 1\n",
    "#                 if wait >= patience:\n",
    "#                     break\n",
    "\n",
    "#         # ----- testing -----\n",
    "#         model.eval(); y_true, y_pred = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for csi_x, labels in test_loader:\n",
    "#                 csi_x, labels = csi_x.to(device), labels.to(device)\n",
    "#                 target = torch.argmax(labels, 1)\n",
    "#                 cls_out, _ = model(csi_x)\n",
    "#                 pred = torch.argmax(cls_out, 1)\n",
    "#                 y_true.extend((target.cpu().numpy() + 1))   # ËΩâ 1-index\n",
    "#                 y_pred.extend((pred.cpu().numpy()   + 1))\n",
    "\n",
    "#         acc            = 100 * np.mean(np.array(y_true) == np.array(y_pred))\n",
    "#         mde_mean, mde_sd = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "#         run_acc.append(acc); run_mde.append(mde_mean); run_mde_std.append(mde_sd)\n",
    "#         print(f\"‚úÖ Run {run}: Acc = {acc:.2f}%, MDE = {mde_mean:.4f} ¬± {mde_sd:.4f}\")\n",
    "\n",
    "#     # ----- ÂÑ≤Â≠òÊØèÂÄã Œ± ÁöÑÁµêÊûú -----\n",
    "#     alpha_id = int(round(alpha * 10))\n",
    "#     folder   = f\"repeat_2_4/{alpha_id:02d}\"\n",
    "#     os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "#     pd.DataFrame({\n",
    "#         'run':     range(1, num_runs + 1),\n",
    "#         'accuracy':run_acc,\n",
    "#         'mde':     run_mde,\n",
    "#         'mde_std': run_mde_std\n",
    "#     }).to_csv(f\"{folder}/csi_cls_reg_results{alpha_id:02d}.csv\", index=False)\n",
    "\n",
    "#     summary_results.append({\n",
    "#         'alpha':   alpha,\n",
    "#         'avg_acc': np.mean(run_acc),\n",
    "#         'std_acc': np.std(run_acc),\n",
    "#         'avg_mde': np.mean(run_mde),\n",
    "#         'std_mde': np.std(run_mde)\n",
    "#     })\n",
    "\n",
    "# # ÊúÄÂæåÊï¥ÂêàÂêÑ Œ± Áµ±Ë®à\n",
    "# summary_df = pd.DataFrame(summary_results)\n",
    "# # summary_df.to_csv(\"csi_alpha_comparison_summary.csv\", index=False)\n",
    "# print(\"\\n=== DONE ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ÈáçË§áË∑ë Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Alpha = 0.1] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.64%, MDE = 0.0156 ¬± 0.2946\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.64%, MDE = 0.0154 ¬± 0.2780\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.59%, MDE = 0.0161 ¬± 0.2769\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0128 ¬± 0.2529\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.64%, MDE = 0.0133 ¬± 0.2521\n",
      "\n",
      "[Alpha = 0.2] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.69%, MDE = 0.0083 ¬± 0.1565\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.64%, MDE = 0.0131 ¬± 0.2554\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.69%, MDE = 0.0134 ¬± 0.2632\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.64%, MDE = 0.0105 ¬± 0.1873\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.59%, MDE = 0.0186 ¬± 0.3215\n",
      "\n",
      "[Alpha = 0.3] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.59%, MDE = 0.0197 ¬± 0.3251\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.59%, MDE = 0.0193 ¬± 0.3268\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.64%, MDE = 0.0140 ¬± 0.2371\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.64%, MDE = 0.0101 ¬± 0.2108\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.64%, MDE = 0.0145 ¬± 0.2707\n",
      "\n",
      "[Alpha = 0.4] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.59%, MDE = 0.0150 ¬± 0.2510\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.59%, MDE = 0.0149 ¬± 0.2737\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.54%, MDE = 0.0148 ¬± 0.2473\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0156 ¬± 0.2723\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.69%, MDE = 0.0123 ¬± 0.2522\n",
      "\n",
      "[Alpha = 0.5] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.69%, MDE = 0.0115 ¬± 0.2342\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.59%, MDE = 0.0134 ¬± 0.2547\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.54%, MDE = 0.0140 ¬± 0.2481\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0156 ¬± 0.2502\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.59%, MDE = 0.0141 ¬± 0.2635\n",
      "\n",
      "[Alpha = 0.6] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.69%, MDE = 0.0142 ¬± 0.2865\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.54%, MDE = 0.0176 ¬± 0.2985\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.64%, MDE = 0.0124 ¬± 0.2229\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0165 ¬± 0.2795\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.64%, MDE = 0.0163 ¬± 0.2995\n",
      "\n",
      "[Alpha = 0.7] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.64%, MDE = 0.0162 ¬± 0.3115\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.64%, MDE = 0.0094 ¬± 0.1996\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.64%, MDE = 0.0159 ¬± 0.2828\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0163 ¬± 0.2867\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.59%, MDE = 0.0146 ¬± 0.2599\n",
      "\n",
      "[Alpha = 0.8] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.59%, MDE = 0.0156 ¬± 0.2595\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.59%, MDE = 0.0215 ¬± 0.3564\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.59%, MDE = 0.0148 ¬± 0.2703\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.44%, MDE = 0.0203 ¬± 0.3165\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.59%, MDE = 0.0213 ¬± 0.3661\n",
      "\n",
      "[Alpha = 0.9] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.75%, MDE = 0.0114 ¬± 0.2519\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.29%, MDE = 0.0222 ¬± 0.2904\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.54%, MDE = 0.0148 ¬± 0.2495\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.64%, MDE = 0.0131 ¬± 0.2333\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.39%, MDE = 0.0211 ¬± 0.2873\n",
      "\n",
      "[Alpha = 1.0] Start 5 runs\n",
      "Run 1/5\n",
      "‚úÖ Run 1: Acc = 99.54%, MDE = 0.0179 ¬± 0.2888\n",
      "Run 2/5\n",
      "‚úÖ Run 2: Acc = 99.59%, MDE = 0.0136 ¬± 0.2395\n",
      "Run 3/5\n",
      "‚úÖ Run 3: Acc = 99.44%, MDE = 0.0184 ¬± 0.2910\n",
      "Run 4/5\n",
      "‚úÖ Run 4: Acc = 99.59%, MDE = 0.0168 ¬± 0.2898\n",
      "Run 5/5\n",
      "‚úÖ Run 5: Acc = 99.49%, MDE = 0.0174 ¬± 0.2963\n",
      "\n",
      "=== Alpha Summary (ACC / MDE ¬± STD) ===\n",
      "   alpha  avg_acc  std_acc  avg_mde  std_mde\n",
      "0    0.1  99.6234   0.0249   0.0146   0.0013\n",
      "1    0.2  99.6539   0.0381   0.0128   0.0034\n",
      "2    0.3  99.6234   0.0249   0.0155   0.0036\n",
      "3    0.4  99.6031   0.0499   0.0145   0.0011\n",
      "4    0.5  99.6031   0.0499   0.0137   0.0013\n",
      "5    0.6  99.6234   0.0519   0.0154   0.0019\n",
      "6    0.7  99.6234   0.0249   0.0145   0.0026\n",
      "7    0.8  99.5623   0.0611   0.0187   0.0029\n",
      "8    0.9  99.5216   0.1660   0.0165   0.0044\n",
      "9    1.0  99.5318   0.0593   0.0168   0.0017\n",
      "üìÅ Summary saved to repeat_copy/00/csi_alpha_comparison_summary.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "COORDINATES = {\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4), 6: (0, 3.0),\n",
    "    7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0), 22: (6.0, 5.4), 23: (6.0, 4.8),\n",
    "    24: (6.0, 4.2), 25: (6.0, 3.6), 26: (6.0, 3.0), 27: (6.0, 2.4),\n",
    "    28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8), 44: (3.0, 2.4),\n",
    "    45: (3.0, 3.0), 46: (3.0, 3.6), 47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "criterion_cls = nn.CrossEntropyLoss()\n",
    "criterion_reg = nn.MSELoss()\n",
    "\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "num_runs = 5\n",
    "epochs = 200\n",
    "patience = 20\n",
    "\n",
    "summary_results = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    run_acc, run_mde, run_mde_std = [], [], []\n",
    "    all_run_errors = []\n",
    "\n",
    "    print(f\"\\n[Alpha = {alpha:.1f}] Start {num_runs} runs\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Run {run}/{num_runs}\")\n",
    "        model = CNN_DualHead(num_classes=49).to(device)\n",
    "        optimzr = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        sched = optim.lr_scheduler.ReduceLROnPlateau(optimzr, 'min', 0.5, patience=15)\n",
    "        best_val, wait = float('inf'), 0\n",
    "\n",
    "        temp_model_path = \"best_csi_reg_temp.pth\"\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for csi_x, labels in train_loader:\n",
    "                csi_x, labels = csi_x.to(device), labels.to(device)\n",
    "                target = torch.argmax(labels, 1)\n",
    "\n",
    "                cls_out, reg_out = model(csi_x)\n",
    "                loss_cls = criterion_cls(cls_out, target)\n",
    "                loss_reg = criterion_reg(reg_out, labels_to_coords(target, COORDINATES))\n",
    "                loss = loss_cls + alpha * loss_reg\n",
    "\n",
    "                optimzr.zero_grad(); loss.backward(); optimzr.step()\n",
    "\n",
    "            model.eval(); val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for csi_x, labels in val_loader:\n",
    "                    csi_x, labels = csi_x.to(device), labels.to(device)\n",
    "                    target = torch.argmax(labels, 1)\n",
    "                    cls_out, reg_out = model(csi_x)\n",
    "                    v_loss = criterion_cls(cls_out, target) + alpha * criterion_reg(reg_out, labels_to_coords(target, COORDINATES))\n",
    "                    val_loss += v_loss.item() * csi_x.size(0)\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            sched.step(val_loss)\n",
    "\n",
    "            if val_loss < best_val:\n",
    "                best_val, wait = val_loss, 0\n",
    "                torch.save(model.state_dict(), temp_model_path)\n",
    "            else:\n",
    "                wait += 1\n",
    "                if wait >= patience:\n",
    "                    break\n",
    "\n",
    "        model.load_state_dict(torch.load(temp_model_path))\n",
    "        model.eval(); y_true, y_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for csi_x, labels in test_loader:\n",
    "                csi_x, labels = csi_x.to(device), labels.to(device)\n",
    "                target = torch.argmax(labels, 1)\n",
    "                cls_out, _ = model(csi_x)\n",
    "                pred = torch.argmax(cls_out, 1)\n",
    "                y_true.extend((target.cpu().numpy() + 1))\n",
    "                y_pred.extend((pred.cpu().numpy() + 1))\n",
    "\n",
    "        acc = 100 * np.mean(np.array(y_true) == np.array(y_pred))\n",
    "        mde_mean, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "        run_acc.append(acc)\n",
    "        run_mde.append(mde_mean)\n",
    "        run_mde_std.append(np.std(errors))\n",
    "        all_run_errors.append(errors)\n",
    "\n",
    "        print(f\"‚úÖ Run {run}: Acc = {acc:.2f}%, MDE = {mde_mean:.4f} ¬± {np.std(errors):.4f}\")\n",
    "\n",
    "    # ÂÑ≤Â≠òÊØèÂÄã alpha ÁöÑÂÄãÂà• run ÁµêÊûú\n",
    "    alpha_id = int(round(alpha * 10))\n",
    "    folder = f\"repeat_copy/{alpha_id:02d}\"\n",
    "    os.makedirs(folder, exist_ok=True)\n",
    "\n",
    "    pd.DataFrame({\n",
    "        'run': range(1, num_runs + 1),\n",
    "        'accuracy': run_acc,\n",
    "        'mde': run_mde,\n",
    "        'mde_std': run_mde_std\n",
    "    }).to_csv(f\"{folder}/csi_cls_reg_results{alpha_id:02d}_error2_b.csv\", index=False)\n",
    "\n",
    "    # ÂÑ≤Â≠ò error Èï∑Ê†ºÂºè\n",
    "    error_records = []\n",
    "    for run_idx, errors in enumerate(all_run_errors):\n",
    "        for sample_idx, e in enumerate(errors):\n",
    "            error_records.append({\n",
    "                \"run\": run_idx + 1,\n",
    "                \"sample_idx\": sample_idx + 1,\n",
    "                \"error\": e\n",
    "            })\n",
    "    df_errors = pd.DataFrame(error_records)\n",
    "    df_errors.to_csv(f\"{folder}/csi_cls_reg_all_errors2_{alpha_id:02d}.csv\", index=False)\n",
    "\n",
    "    # Âä†ÂÖ• summary Áµ±Ë®à\n",
    "    summary_results.append({\n",
    "        'alpha': alpha,\n",
    "        'avg_acc': np.mean(run_acc),\n",
    "        'std_acc': np.std(run_acc),\n",
    "        'avg_mde': np.mean(run_mde),\n",
    "        'std_mde': np.std(run_mde)\n",
    "    })\n",
    "\n",
    "# ÊúÄÂæåÁµ±Êï¥ÊâÄÊúâ alpha ÁöÑÁµêÊûúÂ≠òÂà∞ repeat_copy/00/\n",
    "summary_df = pd.DataFrame(summary_results)\n",
    "os.makedirs(\"repeat_copy/00\", exist_ok=True)\n",
    "summary_df.to_csv(\"repeat_copy/00/csi_alpha_comparison_summary.csv\", index=False)\n",
    "\n",
    "print(\"\\n=== Alpha Summary (ACC / MDE ¬± STD) ===\")\n",
    "print(summary_df.round(4))\n",
    "print(\"üìÅ Summary saved to repeat_copy/00/csi_alpha_comparison_summary.csv\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
