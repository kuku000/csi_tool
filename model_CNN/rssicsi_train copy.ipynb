{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN for RSSI + CSI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當前工作目錄: /media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN\n",
      "這裡有哪些檔案: ['best_model_tmp.pth', '__init__.py', 'training_results.txt', 'csidataset.py', 'train.ipynb', 'csi_val_acc.png', 'repeat_copy', 'data_loader.py', 'csi_train_loss.png', 'train_soft_label.ipynb', 'rssicsi_train.ipynb', 'best_csirssi_classifier_bn_temp.pth', 'best_csirssi_reg_temp.pth', 'repeat_for_zero_shot', 'repeat', 'csi_train_acc.png', 'models_save', 'model_save_soft_label', '__pycache__', 'csi_val_loss.png', 'model.py', 'rssicsi_train copy.ipynb', 'train copy.ipynb', 'zero_shot_test.ipynb', 'best_csi_reg_temp.pth', 'train_mirco.ipynb', 'soft_labels.npz']\n",
      "當前工作目錄: /media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"當前工作目錄:\", os.getcwd())\n",
    "print(\"這裡有哪些檔案:\", os.listdir())\n",
    "os.chdir('/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN')\n",
    "print(\"當前工作目錄:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "sys.path.append('./')\n",
    "from csidataset import *\n",
    "import data_loader\n",
    "from data_loader import *\n",
    "sys.path.append(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool\")\n",
    "import denoise\n",
    "from model import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#   CSI資料載入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "csi_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/1223_phone/5G/20MHz/csv/all\"\n",
    "rssi_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/RSSI/timestamp_allignment_Balanced_2024_12_14_rtt_logs.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data CSI\n",
    "reference_points = {}\n",
    "spacing = 0.6  # 每隔 0.6mh origin\n",
    "\n",
    "for ref_id, coord in data_loader.COORDINATES.items():\n",
    "    folder_path = os.path.join(csi_path, f\"reference_point_{ref_id}.xlsx\")\n",
    "    reference_points[folder_path] = (ref_id, coord)\n",
    "\n",
    "data, rp_labels, coord_labels = load_data(reference_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>88</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>893.573164</td>\n",
       "      <td>888.473973</td>\n",
       "      <td>871.023536</td>\n",
       "      <td>836.316328</td>\n",
       "      <td>841.770753</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.387518</td>\n",
       "      <td>-1.581549</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>-2.080869</td>\n",
       "      <td>-2.287379</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>672.521375</td>\n",
       "      <td>687.674342</td>\n",
       "      <td>780.946221</td>\n",
       "      <td>803.560203</td>\n",
       "      <td>863.103702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.677159</td>\n",
       "      <td>-2.877730</td>\n",
       "      <td>-3.051501</td>\n",
       "      <td>2.738820</td>\n",
       "      <td>2.503722</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>713.950278</td>\n",
       "      <td>736.619983</td>\n",
       "      <td>852.877482</td>\n",
       "      <td>846.442556</td>\n",
       "      <td>880.032954</td>\n",
       "      <td>...</td>\n",
       "      <td>0.176051</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.251636</td>\n",
       "      <td>-0.776139</td>\n",
       "      <td>-1.023850</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>800.870776</td>\n",
       "      <td>791.982323</td>\n",
       "      <td>778.923616</td>\n",
       "      <td>763.872372</td>\n",
       "      <td>766.167084</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.031820</td>\n",
       "      <td>-1.194379</td>\n",
       "      <td>-1.338067</td>\n",
       "      <td>-1.666445</td>\n",
       "      <td>-1.878363</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>875.416472</td>\n",
       "      <td>878.300632</td>\n",
       "      <td>854.687077</td>\n",
       "      <td>857.886939</td>\n",
       "      <td>843.664033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.445534</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-1.090942</td>\n",
       "      <td>-1.286103</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24495</th>\n",
       "      <td>397.615392</td>\n",
       "      <td>416.889674</td>\n",
       "      <td>395.588170</td>\n",
       "      <td>413.019370</td>\n",
       "      <td>382.733589</td>\n",
       "      <td>381.136459</td>\n",
       "      <td>393.814677</td>\n",
       "      <td>434.337426</td>\n",
       "      <td>465.842248</td>\n",
       "      <td>483.735465</td>\n",
       "      <td>...</td>\n",
       "      <td>2.183710</td>\n",
       "      <td>2.005190</td>\n",
       "      <td>1.826107</td>\n",
       "      <td>1.364322</td>\n",
       "      <td>1.150287</td>\n",
       "      <td>0.872978</td>\n",
       "      <td>0.573063</td>\n",
       "      <td>0.255977</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24496</th>\n",
       "      <td>643.708785</td>\n",
       "      <td>646.260010</td>\n",
       "      <td>627.624888</td>\n",
       "      <td>621.498994</td>\n",
       "      <td>605.152873</td>\n",
       "      <td>628.458431</td>\n",
       "      <td>617.272225</td>\n",
       "      <td>592.763865</td>\n",
       "      <td>581.986254</td>\n",
       "      <td>587.911558</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.494557</td>\n",
       "      <td>-0.782703</td>\n",
       "      <td>-1.053078</td>\n",
       "      <td>-1.631902</td>\n",
       "      <td>-1.969954</td>\n",
       "      <td>-2.328378</td>\n",
       "      <td>-2.758960</td>\n",
       "      <td>3.087458</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24497</th>\n",
       "      <td>762.685387</td>\n",
       "      <td>745.553486</td>\n",
       "      <td>706.884007</td>\n",
       "      <td>695.708272</td>\n",
       "      <td>698.951357</td>\n",
       "      <td>710.169698</td>\n",
       "      <td>720.184004</td>\n",
       "      <td>675.370269</td>\n",
       "      <td>654.120020</td>\n",
       "      <td>669.209982</td>\n",
       "      <td>...</td>\n",
       "      <td>0.621717</td>\n",
       "      <td>0.498336</td>\n",
       "      <td>0.356470</td>\n",
       "      <td>-0.131807</td>\n",
       "      <td>-0.426306</td>\n",
       "      <td>-0.748795</td>\n",
       "      <td>-1.031706</td>\n",
       "      <td>-1.339351</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24498</th>\n",
       "      <td>401.220638</td>\n",
       "      <td>416.688133</td>\n",
       "      <td>388.561449</td>\n",
       "      <td>390.436935</td>\n",
       "      <td>370.411933</td>\n",
       "      <td>382.498366</td>\n",
       "      <td>394.452786</td>\n",
       "      <td>436.664631</td>\n",
       "      <td>446.643034</td>\n",
       "      <td>495.258518</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.801749</td>\n",
       "      <td>-2.120022</td>\n",
       "      <td>-2.410138</td>\n",
       "      <td>3.138034</td>\n",
       "      <td>2.751942</td>\n",
       "      <td>2.306627</td>\n",
       "      <td>1.868269</td>\n",
       "      <td>1.432071</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24499</th>\n",
       "      <td>753.160010</td>\n",
       "      <td>739.957431</td>\n",
       "      <td>724.996552</td>\n",
       "      <td>704.264865</td>\n",
       "      <td>701.806954</td>\n",
       "      <td>717.674717</td>\n",
       "      <td>715.455100</td>\n",
       "      <td>674.985185</td>\n",
       "      <td>658.325907</td>\n",
       "      <td>679.605768</td>\n",
       "      <td>...</td>\n",
       "      <td>1.746490</td>\n",
       "      <td>1.586924</td>\n",
       "      <td>1.427713</td>\n",
       "      <td>1.118018</td>\n",
       "      <td>0.870126</td>\n",
       "      <td>0.650771</td>\n",
       "      <td>0.353386</td>\n",
       "      <td>0.054227</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>24500 rows × 98 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "0      919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
       "1      698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
       "2      745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
       "3      830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
       "4      906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "24495  397.615392  416.889674  395.588170  413.019370  382.733589  381.136459   \n",
       "24496  643.708785  646.260010  627.624888  621.498994  605.152873  628.458431   \n",
       "24497  762.685387  745.553486  706.884007  695.708272  698.951357  710.169698   \n",
       "24498  401.220638  416.688133  388.561449  390.436935  370.411933  382.498366   \n",
       "24499  753.160010  739.957431  724.996552  704.264865  701.806954  717.674717   \n",
       "\n",
       "               6           7           8           9   ...        88  \\\n",
       "0      888.473973  871.023536  836.316328  841.770753  ... -1.387518   \n",
       "1      687.674342  780.946221  803.560203  863.103702  ... -2.677159   \n",
       "2      736.619983  852.877482  846.442556  880.032954  ...  0.176051   \n",
       "3      791.982323  778.923616  763.872372  766.167084  ... -1.031820   \n",
       "4      878.300632  854.687077  857.886939  843.664033  ... -0.445534   \n",
       "...           ...         ...         ...         ...  ...       ...   \n",
       "24495  393.814677  434.337426  465.842248  483.735465  ...  2.183710   \n",
       "24496  617.272225  592.763865  581.986254  587.911558  ... -0.494557   \n",
       "24497  720.184004  675.370269  654.120020  669.209982  ...  0.621717   \n",
       "24498  394.452786  436.664631  446.643034  495.258518  ... -1.801749   \n",
       "24499  715.455100  674.985185  658.325907  679.605768  ...  1.746490   \n",
       "\n",
       "             89        90        91        92        93        94        95  \\\n",
       "0     -1.581549 -1.708760 -2.080869 -2.287379 -2.467120 -2.666997 -2.982241   \n",
       "1     -2.877730 -3.051501  2.738820  2.503722  2.256683  2.035376  1.737005   \n",
       "2     -0.063704 -0.251636 -0.776139 -1.023850 -1.275624 -1.535994 -1.899522   \n",
       "3     -1.194379 -1.338067 -1.666445 -1.878363 -2.100073 -2.369892 -2.666982   \n",
       "4     -0.593750 -0.783166 -1.090942 -1.286103 -1.497279 -1.784787 -2.076610   \n",
       "...         ...       ...       ...       ...       ...       ...       ...   \n",
       "24495  2.005190  1.826107  1.364322  1.150287  0.872978  0.573063  0.255977   \n",
       "24496 -0.782703 -1.053078 -1.631902 -1.969954 -2.328378 -2.758960  3.087458   \n",
       "24497  0.498336  0.356470 -0.131807 -0.426306 -0.748795 -1.031706 -1.339351   \n",
       "24498 -2.120022 -2.410138  3.138034  2.751942  2.306627  1.868269  1.432071   \n",
       "24499  1.586924  1.427713  1.118018  0.870126  0.650771  0.353386  0.054227   \n",
       "\n",
       "         96     97  \n",
       "0     -50.0  136.0  \n",
       "1     -49.0  148.0  \n",
       "2     -50.0  224.0  \n",
       "3     -51.0  136.0  \n",
       "4     -50.0  136.0  \n",
       "...     ...    ...  \n",
       "24495 -56.0  148.0  \n",
       "24496 -59.0  136.0  \n",
       "24497 -58.0  136.0  \n",
       "24498 -56.0  148.0  \n",
       "24499 -58.0  136.0  \n",
       "\n",
       "[24500 rows x 98 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 合併後資料型別：\n",
      "data_filtered: <class 'pandas.core.frame.DataFrame'> (19649, 99)\n",
      "rp_labels_filtered: <class 'numpy.ndarray'> (19649,)\n",
      "coord_labels_filtered: <class 'pandas.core.frame.DataFrame'> (19649, 2)\n",
      "\n",
      "每個 label 筆數（應該都為 401）：\n",
      "1     401\n",
      "2     401\n",
      "3     401\n",
      "4     401\n",
      "5     401\n",
      "6     401\n",
      "7     401\n",
      "8     401\n",
      "9     401\n",
      "10    401\n",
      "11    401\n",
      "12    401\n",
      "13    401\n",
      "14    401\n",
      "15    401\n",
      "16    401\n",
      "17    401\n",
      "18    401\n",
      "19    401\n",
      "20    401\n",
      "21    401\n",
      "22    401\n",
      "23    401\n",
      "24    401\n",
      "25    401\n",
      "26    401\n",
      "27    401\n",
      "28    401\n",
      "29    401\n",
      "30    401\n",
      "31    401\n",
      "32    401\n",
      "33    401\n",
      "34    401\n",
      "35    401\n",
      "36    401\n",
      "37    401\n",
      "38    401\n",
      "39    401\n",
      "40    401\n",
      "41    401\n",
      "42    401\n",
      "43    401\n",
      "44    401\n",
      "45    401\n",
      "46    401\n",
      "47    401\n",
      "48    401\n",
      "49    401\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_count = 401\n",
    "unique_labels = np.unique(rp_labels)\n",
    "\n",
    "filtered_data = []\n",
    "filtered_rp_labels = []\n",
    "filtered_coord_labels = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    indices = np.where(rp_labels == label)[0]\n",
    "\n",
    "    if len(indices) < target_count:\n",
    "        print(f\"Label {label} only has {len(indices)} samples, skipped.\")\n",
    "        continue\n",
    "\n",
    "    selected_indices = indices[:target_count]  # ✅ 固定取最前面的 401 筆\n",
    "\n",
    "    filtered_data.append(data.iloc[selected_indices])\n",
    "    filtered_rp_labels.append(rp_labels[selected_indices])\n",
    "    filtered_coord_labels.append(coord_labels.iloc[selected_indices])\n",
    "\n",
    "# ✅ 合併，確保型別一致\n",
    "data_filtered = pd.concat(filtered_data, ignore_index=True)  # DataFrame\n",
    "rp_labels_filtered = np.concatenate(filtered_rp_labels, axis=0)  # ndarray\n",
    "coord_labels_filtered = pd.concat(filtered_coord_labels, ignore_index=True)  # DataFrame\n",
    "\n",
    "# ✅ 驗證\n",
    "print(\"✅ 合併後資料型別：\")\n",
    "print(\"data_filtered:\", type(data_filtered), data_filtered.shape)\n",
    "print(\"rp_labels_filtered:\", type(rp_labels_filtered), rp_labels_filtered.shape)\n",
    "print(\"coord_labels_filtered:\", type(coord_labels_filtered), coord_labels_filtered.shape)\n",
    "\n",
    "print(\"\\n每個 label 筆數（應該都為 401）：\")\n",
    "print(pd.Series(rp_labels_filtered).value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>89</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>893.573164</td>\n",
       "      <td>888.473973</td>\n",
       "      <td>871.023536</td>\n",
       "      <td>836.316328</td>\n",
       "      <td>841.770753</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.581549</td>\n",
       "      <td>-1.708760</td>\n",
       "      <td>-2.080869</td>\n",
       "      <td>-2.287379</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>672.521375</td>\n",
       "      <td>687.674342</td>\n",
       "      <td>780.946221</td>\n",
       "      <td>803.560203</td>\n",
       "      <td>863.103702</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.877730</td>\n",
       "      <td>-3.051501</td>\n",
       "      <td>2.738820</td>\n",
       "      <td>2.503722</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>713.950278</td>\n",
       "      <td>736.619983</td>\n",
       "      <td>852.877482</td>\n",
       "      <td>846.442556</td>\n",
       "      <td>880.032954</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.063704</td>\n",
       "      <td>-0.251636</td>\n",
       "      <td>-0.776139</td>\n",
       "      <td>-1.023850</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>800.870776</td>\n",
       "      <td>791.982323</td>\n",
       "      <td>778.923616</td>\n",
       "      <td>763.872372</td>\n",
       "      <td>766.167084</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.194379</td>\n",
       "      <td>-1.338067</td>\n",
       "      <td>-1.666445</td>\n",
       "      <td>-1.878363</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>875.416472</td>\n",
       "      <td>878.300632</td>\n",
       "      <td>854.687077</td>\n",
       "      <td>857.886939</td>\n",
       "      <td>843.664033</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.593750</td>\n",
       "      <td>-0.783166</td>\n",
       "      <td>-1.090942</td>\n",
       "      <td>-1.286103</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>381.980366</td>\n",
       "      <td>398.527289</td>\n",
       "      <td>370.951479</td>\n",
       "      <td>372.108855</td>\n",
       "      <td>352.346420</td>\n",
       "      <td>359.233907</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>413.997585</td>\n",
       "      <td>420.138073</td>\n",
       "      <td>454.862617</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.668036</td>\n",
       "      <td>-0.872238</td>\n",
       "      <td>-1.396551</td>\n",
       "      <td>-1.667962</td>\n",
       "      <td>-1.977215</td>\n",
       "      <td>-2.212040</td>\n",
       "      <td>-2.563840</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>737.651679</td>\n",
       "      <td>730.928861</td>\n",
       "      <td>707.471554</td>\n",
       "      <td>673.190909</td>\n",
       "      <td>665.090219</td>\n",
       "      <td>689.385233</td>\n",
       "      <td>679.574131</td>\n",
       "      <td>666.066813</td>\n",
       "      <td>646.891026</td>\n",
       "      <td>636.792745</td>\n",
       "      <td>...</td>\n",
       "      <td>0.417030</td>\n",
       "      <td>0.233346</td>\n",
       "      <td>-0.227937</td>\n",
       "      <td>-0.474118</td>\n",
       "      <td>-0.747335</td>\n",
       "      <td>-1.038664</td>\n",
       "      <td>-1.386254</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>364.329521</td>\n",
       "      <td>391.318285</td>\n",
       "      <td>383.775976</td>\n",
       "      <td>384.610192</td>\n",
       "      <td>367.828765</td>\n",
       "      <td>339.330223</td>\n",
       "      <td>346.236913</td>\n",
       "      <td>376.766506</td>\n",
       "      <td>421.283752</td>\n",
       "      <td>448.486343</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.877905</td>\n",
       "      <td>2.906870</td>\n",
       "      <td>1.246783</td>\n",
       "      <td>0.451070</td>\n",
       "      <td>-0.370119</td>\n",
       "      <td>-1.260481</td>\n",
       "      <td>-2.098871</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>396.021464</td>\n",
       "      <td>409.890229</td>\n",
       "      <td>392.439804</td>\n",
       "      <td>391.154701</td>\n",
       "      <td>374.786606</td>\n",
       "      <td>380.443163</td>\n",
       "      <td>384.193961</td>\n",
       "      <td>434.991954</td>\n",
       "      <td>450.813709</td>\n",
       "      <td>472.956658</td>\n",
       "      <td>...</td>\n",
       "      <td>1.488145</td>\n",
       "      <td>1.336005</td>\n",
       "      <td>0.979841</td>\n",
       "      <td>0.765983</td>\n",
       "      <td>0.532844</td>\n",
       "      <td>0.208257</td>\n",
       "      <td>-0.127813</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>734.994558</td>\n",
       "      <td>726.389703</td>\n",
       "      <td>717.432227</td>\n",
       "      <td>688.447529</td>\n",
       "      <td>683.183723</td>\n",
       "      <td>684.211225</td>\n",
       "      <td>688.442445</td>\n",
       "      <td>673.167141</td>\n",
       "      <td>681.950145</td>\n",
       "      <td>663.717560</td>\n",
       "      <td>...</td>\n",
       "      <td>2.908532</td>\n",
       "      <td>2.559763</td>\n",
       "      <td>1.732479</td>\n",
       "      <td>1.298008</td>\n",
       "      <td>0.859337</td>\n",
       "      <td>0.389644</td>\n",
       "      <td>-0.102262</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19649 rows × 99 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "0      919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
       "1      698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
       "2      745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
       "3      830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
       "4      906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "19644  381.980366  398.527289  370.951479  372.108855  352.346420  359.233907   \n",
       "19645  737.651679  730.928861  707.471554  673.190909  665.090219  689.385233   \n",
       "19646  364.329521  391.318285  383.775976  384.610192  367.828765  339.330223   \n",
       "19647  396.021464  409.890229  392.439804  391.154701  374.786606  380.443163   \n",
       "19648  734.994558  726.389703  717.432227  688.447529  683.183723  684.211225   \n",
       "\n",
       "                6           7           8           9  ...        89  \\\n",
       "0      888.473973  871.023536  836.316328  841.770753  ... -1.581549   \n",
       "1      687.674342  780.946221  803.560203  863.103702  ... -2.877730   \n",
       "2      736.619983  852.877482  846.442556  880.032954  ... -0.063704   \n",
       "3      791.982323  778.923616  763.872372  766.167084  ... -1.194379   \n",
       "4      878.300632  854.687077  857.886939  843.664033  ... -0.593750   \n",
       "...           ...         ...         ...         ...  ...       ...   \n",
       "19644  370.000000  413.997585  420.138073  454.862617  ... -0.668036   \n",
       "19645  679.574131  666.066813  646.891026  636.792745  ...  0.417030   \n",
       "19646  346.236913  376.766506  421.283752  448.486343  ... -1.877905   \n",
       "19647  384.193961  434.991954  450.813709  472.956658  ...  1.488145   \n",
       "19648  688.442445  673.167141  681.950145  663.717560  ...  2.908532   \n",
       "\n",
       "             90        91        92        93        94        95    96  \\\n",
       "0     -1.708760 -2.080869 -2.287379 -2.467120 -2.666997 -2.982241 -50.0   \n",
       "1     -3.051501  2.738820  2.503722  2.256683  2.035376  1.737005 -49.0   \n",
       "2     -0.251636 -0.776139 -1.023850 -1.275624 -1.535994 -1.899522 -50.0   \n",
       "3     -1.338067 -1.666445 -1.878363 -2.100073 -2.369892 -2.666982 -51.0   \n",
       "4     -0.783166 -1.090942 -1.286103 -1.497279 -1.784787 -2.076610 -50.0   \n",
       "...         ...       ...       ...       ...       ...       ...   ...   \n",
       "19644 -0.872238 -1.396551 -1.667962 -1.977215 -2.212040 -2.563840 -56.0   \n",
       "19645  0.233346 -0.227937 -0.474118 -0.747335 -1.038664 -1.386254 -58.0   \n",
       "19646  2.906870  1.246783  0.451070 -0.370119 -1.260481 -2.098871 -57.0   \n",
       "19647  1.336005  0.979841  0.765983  0.532844  0.208257 -0.127813 -58.0   \n",
       "19648  2.559763  1.732479  1.298008  0.859337  0.389644 -0.102262 -58.0   \n",
       "\n",
       "          97  Label  \n",
       "0      136.0      1  \n",
       "1      148.0      1  \n",
       "2      224.0      1  \n",
       "3      136.0      1  \n",
       "4      136.0      1  \n",
       "...      ...    ...  \n",
       "19644  148.0     49  \n",
       "19645  136.0     49  \n",
       "19646  148.0     49  \n",
       "19647  224.0     49  \n",
       "19648  136.0     49  \n",
       "\n",
       "[19649 rows x 99 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ csi_filtered shape: (19649, 99)\n",
      "每個 Label 的筆數（應該都為 401）：\n",
      "Label\n",
      "1     401\n",
      "2     401\n",
      "3     401\n",
      "4     401\n",
      "5     401\n",
      "6     401\n",
      "7     401\n",
      "8     401\n",
      "9     401\n",
      "10    401\n",
      "11    401\n",
      "12    401\n",
      "13    401\n",
      "14    401\n",
      "15    401\n",
      "16    401\n",
      "17    401\n",
      "18    401\n",
      "19    401\n",
      "20    401\n",
      "21    401\n",
      "22    401\n",
      "23    401\n",
      "24    401\n",
      "25    401\n",
      "26    401\n",
      "27    401\n",
      "28    401\n",
      "29    401\n",
      "30    401\n",
      "31    401\n",
      "32    401\n",
      "33    401\n",
      "34    401\n",
      "35    401\n",
      "36    401\n",
      "37    401\n",
      "38    401\n",
      "39    401\n",
      "40    401\n",
      "41    401\n",
      "42    401\n",
      "43    401\n",
      "44    401\n",
      "45    401\n",
      "46    401\n",
      "47    401\n",
      "48    401\n",
      "49    401\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "target_count = 401\n",
    "unique_labels = csi[\"Label\"].unique()\n",
    "\n",
    "filtered_csi_parts = []\n",
    "\n",
    "for label in unique_labels:\n",
    "    group = csi[csi[\"Label\"] == label]\n",
    "    if len(group) < target_count:\n",
    "        print(f\"Label {label} only has {len(group)} samples, skipped.\")\n",
    "        continue\n",
    "    filtered = group.iloc[:target_count]  # ✅ 固定取最前面 401 筆\n",
    "    filtered_csi_parts.append(filtered)\n",
    "\n",
    "# ✅ 合併\n",
    "csi_filtered = pd.concat(filtered_csi_parts, ignore_index=True)\n",
    "\n",
    "# ✅ 驗證\n",
    "print(\"✅ csi_filtered shape:\", csi_filtered.shape)\n",
    "print(\"每個 Label 的筆數（應該都為 401）：\")\n",
    "print(csi_filtered[\"Label\"].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data RSSI\n",
    "\n",
    "label_to_coordinates_swapped = {\n",
    "    \"1-1\":  (0, 0),   \"1-2\":  (0, 0.6),   \"1-3\":  (0, 1.2),   \"1-4\":  (0, 1.8),\n",
    "    \"1-5\":  (0, 2.4), \"1-6\":  (0, 3.0),   \"1-7\":  (0, 3.6),   \"1-8\":  (0, 4.2),\n",
    "    \"1-9\":  (0, 4.8), \"1-10\": (0, 5.4),   \"1-11\": (0, 6.0),\n",
    "\n",
    "    \"2-1\":  (0.6, 0),  \"2-11\": (0.6, 6.0),\n",
    "\n",
    "    \"3-1\":  (1.2, 0),  \"3-11\": (1.2, 6.0),\n",
    "\n",
    "    \"4-1\":  (1.8, 0),  \"4-11\": (1.8, 6.0),\n",
    "\n",
    "    \"5-1\":  (2.4, 0),  \"5-11\": (2.4, 6.0),\n",
    "\n",
    "    \"6-1\":  (3.0, 0),  \"6-2\":  (3.0, 0.6),  \"6-3\":  (3.0, 1.2),  \"6-4\":  (3.0, 1.8),\n",
    "    \"6-5\":  (3.0, 2.4), \"6-6\":  (3.0, 3.0),  \"6-7\":  (3.0, 3.6),  \"6-8\":  (3.0, 4.2),\n",
    "    \"6-9\":  (3.0, 4.8), \"6-10\": (3.0, 5.4),  \"6-11\": (3.0, 6.0),\n",
    "\n",
    "    \"7-1\":  (3.6, 0),  \"7-11\": (3.6, 6.0),\n",
    "\n",
    "    \"8-1\":  (4.2, 0),  \"8-11\": (4.2, 6.0),\n",
    "\n",
    "    \"9-1\":  (4.8, 0),  \"9-11\": (4.8, 6.0),\n",
    "\n",
    "    \"10-1\": (5.4, 0),  \"10-11\": (5.4, 6.0),\n",
    "\n",
    "    \"11-1\": (6.0, 0),  \"11-2\": (6.0, 0.6), \"11-3\": (6.0, 1.2), \"11-4\": (6.0, 1.8),\n",
    "    \"11-5\": (6.0, 2.4),\"11-6\": (6.0, 3.0), \"11-7\": (6.0, 3.6), \"11-8\": (6.0, 4.2),\n",
    "    \"11-9\": (6.0, 4.8),\"11-10\":(6.0, 5.4), \"11-11\":(6.0, 6.0)\n",
    "}\n",
    "\n",
    "\n",
    "coordinates = {\n",
    "        1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "        36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "        2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "        6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "        12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "        16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "        20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "        22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "        26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "        41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "        44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "        47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "    }\n",
    "coordinate_to_label2 = {value: key for key, value in coordinates.items()}\n",
    "\n",
    "# label_to_coordinates:  \"1-1\" -> (0, 0)\n",
    "# coordinates:          1      -> (0, 0)\n",
    "\n",
    "# 1) 先建立一個「座標 -> 數字」的反轉字典\n",
    "coord_to_num = {value: key for key, value in coordinates.items()}\n",
    "# coord_to_num[(0,0)] = 1,  coord_to_num[(0.6,0)] = 40, ...\n",
    "\n",
    "# 2) 建立 \"1-1\" -> 整數標籤 的 map\n",
    "label_str_to_num = {}\n",
    "for str_label, coord in label_to_coordinates_swapped.items():\n",
    "    if coord in coord_to_num:  # 確保在座標字典中找得到\n",
    "        label_str_to_num[str_label] = coord_to_num[coord]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssi = pd.read_csv(rssi_path)\n",
    "\n",
    "rssi = rssi.drop(columns=['timeStemp', 'AP1_Distance (mm)', 'AP2_Distance (mm)', 'AP3_Distance (mm)', 'AP4_Distance (mm)', 'AP1_StdDev (mm)','AP2_StdDev (mm)', 'AP3_StdDev (mm)', 'AP4_StdDev (mm)'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        1-11\n",
       "1        1-11\n",
       "2        1-11\n",
       "3        1-11\n",
       "4        1-11\n",
       "         ... \n",
       "19644     7-1\n",
       "19645     7-1\n",
       "19646     7-1\n",
       "19647     7-1\n",
       "19648     7-1\n",
       "Name: Label, Length: 19649, dtype: object"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rssi[\"Label\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "rssi[\"Label\"] = (\n",
    "    rssi[\"Label\"]\n",
    "    .map(label_to_coordinates_swapped)   # 由 \"1-1\" → (0,0)\n",
    "    .map(coordinate_to_label2)   # 由 (0,0) → 1 (或其他整數標籤)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11</td>\n",
       "      <td>-60.0</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>11</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-69.0</td>\n",
       "      <td>-63.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-70.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>35</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>35</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-75.0</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>35</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>35</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-59.0</td>\n",
       "      <td>-58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>35</td>\n",
       "      <td>-68.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19649 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Label  AP1_Rssi  AP2_Rssi  AP3_Rssi  AP4_Rssi\n",
       "0         11     -60.0     -68.0     -61.0     -59.0\n",
       "1         11     -62.0     -70.0     -59.0     -59.0\n",
       "2         11     -61.0     -69.0     -62.0     -55.0\n",
       "3         11     -62.0     -69.0     -63.0     -55.0\n",
       "4         11     -61.0     -70.0     -62.0     -55.0\n",
       "...      ...       ...       ...       ...       ...\n",
       "19644     35     -64.0     -66.0     -61.0     -58.0\n",
       "19645     35     -67.0     -75.0     -62.0     -59.0\n",
       "19646     35     -64.0     -66.0     -61.0     -57.0\n",
       "19647     35     -66.0     -65.0     -59.0     -58.0\n",
       "19648     35     -68.0     -64.0     -61.0     -59.0\n",
       "\n",
       "[19649 rows x 5 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rssi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ rssi_filtered shape: (19649, 5)\n",
      "各 Label 筆數（應該都為 401）：\n",
      "Label\n",
      "1     401\n",
      "2     401\n",
      "3     401\n",
      "4     401\n",
      "5     401\n",
      "6     401\n",
      "7     401\n",
      "8     401\n",
      "9     401\n",
      "10    401\n",
      "11    401\n",
      "12    401\n",
      "13    401\n",
      "14    401\n",
      "15    401\n",
      "16    401\n",
      "17    401\n",
      "18    401\n",
      "19    401\n",
      "20    401\n",
      "21    401\n",
      "22    401\n",
      "23    401\n",
      "24    401\n",
      "25    401\n",
      "26    401\n",
      "27    401\n",
      "28    401\n",
      "29    401\n",
      "30    401\n",
      "31    401\n",
      "32    401\n",
      "33    401\n",
      "34    401\n",
      "35    401\n",
      "36    401\n",
      "37    401\n",
      "38    401\n",
      "39    401\n",
      "40    401\n",
      "41    401\n",
      "42    401\n",
      "43    401\n",
      "44    401\n",
      "45    401\n",
      "46    401\n",
      "47    401\n",
      "48    401\n",
      "49    401\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "target_count = 401\n",
    "filtered_rssi_parts = []\n",
    "\n",
    "for label in rssi[\"Label\"].unique():\n",
    "    group = rssi[rssi[\"Label\"] == label]\n",
    "    if len(group) >= target_count:\n",
    "        filtered = group.iloc[:target_count]  # 固定取前 401 筆\n",
    "        filtered_rssi_parts.append(filtered)\n",
    "    else:\n",
    "        print(f\"Label {label} 只有 {len(group)} 筆，不足 {target_count} 筆，略過\")\n",
    "\n",
    "# 合併所有符合條件的 group\n",
    "rssi_filtered = pd.concat(filtered_rssi_parts, ignore_index=True)\n",
    "\n",
    "print(\"✅ rssi_filtered shape:\", rssi_filtered.shape)\n",
    "print(\"各 Label 筆數（應該都為 401）：\")\n",
    "print(rssi_filtered[\"Label\"].value_counts().sort_index())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 合併成功，總筆數： 19649 ，欄位數： 103\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2600420/471310652.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(\"Label\", group_keys=False).apply(lambda x: x.reset_index(drop=True))\n",
      "/tmp/ipykernel_2600420/471310652.py:20: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  return df.groupby(\"Label\", group_keys=False).apply(lambda x: x.reset_index(drop=True))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def safe_label_merge(data_filtered, rssi_filtered):\n",
    "    # 防呆檢查欄位\n",
    "    if \"Label\" not in data_filtered.columns or \"Label\" not in rssi_filtered.columns:\n",
    "        raise ValueError(\"兩個 DataFrame 都必須包含 'Label' 欄位\")\n",
    "\n",
    "    # 防呆檢查每個 Label 的筆數\n",
    "    label_counts_data = data_filtered[\"Label\"].value_counts().sort_index()\n",
    "    label_counts_rssi = rssi_filtered[\"Label\"].value_counts().sort_index()\n",
    "\n",
    "    if not label_counts_data.equals(label_counts_rssi):\n",
    "        print(\"❌ Label 筆數不一致：\")\n",
    "        print(\"data_filtered:\\n\", label_counts_data)\n",
    "        print(\"rssi_filtered:\\n\", label_counts_rssi)\n",
    "        raise ValueError(\"每個 Label 的筆數在兩個資料集之間不一致\")\n",
    "\n",
    "    # 加入 group-wise index：0~400 每組\n",
    "    def add_group_index(df):\n",
    "        return df.groupby(\"Label\", group_keys=False).apply(lambda x: x.reset_index(drop=True))\n",
    "\n",
    "    data_sorted = add_group_index(data_filtered.copy())\n",
    "    rssi_sorted = add_group_index(rssi_filtered.copy())\n",
    "\n",
    "    # 防呆檢查 Label 是否完全一樣 & 順序一致\n",
    "    if not (data_sorted[\"Label\"].values == rssi_sorted[\"Label\"].values).all():\n",
    "        raise ValueError(\"兩個資料集的 Label 值或順序不一致，無法安全合併\")\n",
    "\n",
    "    # 移除 rssi 裡的 Label 欄位，避免重複\n",
    "    rssi_only = rssi_sorted.drop(columns=[\"Label\"])\n",
    "\n",
    "    # 合併\n",
    "    combined = pd.concat([data_sorted.reset_index(drop=True), rssi_only.reset_index(drop=True)], axis=1)\n",
    "\n",
    "    print(\"✅ 合併成功，總筆數：\", combined.shape[0], \"，欄位數：\", combined.shape[1])\n",
    "    return combined\n",
    "combined = safe_label_merge(data_filtered, rssi_filtered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>Label</th>\n",
       "      <th>AP1_Rssi</th>\n",
       "      <th>AP2_Rssi</th>\n",
       "      <th>AP3_Rssi</th>\n",
       "      <th>AP4_Rssi</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>919.592301</td>\n",
       "      <td>929.595611</td>\n",
       "      <td>877.760787</td>\n",
       "      <td>898.203763</td>\n",
       "      <td>851.400023</td>\n",
       "      <td>893.573164</td>\n",
       "      <td>888.473973</td>\n",
       "      <td>871.023536</td>\n",
       "      <td>836.316328</td>\n",
       "      <td>841.770753</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.467120</td>\n",
       "      <td>-2.666997</td>\n",
       "      <td>-2.982241</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-54.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>698.204841</td>\n",
       "      <td>732.963846</td>\n",
       "      <td>684.079674</td>\n",
       "      <td>694.257877</td>\n",
       "      <td>651.079872</td>\n",
       "      <td>672.521375</td>\n",
       "      <td>687.674342</td>\n",
       "      <td>780.946221</td>\n",
       "      <td>803.560203</td>\n",
       "      <td>863.103702</td>\n",
       "      <td>...</td>\n",
       "      <td>2.256683</td>\n",
       "      <td>2.035376</td>\n",
       "      <td>1.737005</td>\n",
       "      <td>-49.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-56.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>745.268408</td>\n",
       "      <td>768.188128</td>\n",
       "      <td>706.606680</td>\n",
       "      <td>713.252410</td>\n",
       "      <td>681.482208</td>\n",
       "      <td>713.950278</td>\n",
       "      <td>736.619983</td>\n",
       "      <td>852.877482</td>\n",
       "      <td>846.442556</td>\n",
       "      <td>880.032954</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.275624</td>\n",
       "      <td>-1.535994</td>\n",
       "      <td>-1.899522</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-62.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>830.278267</td>\n",
       "      <td>829.860832</td>\n",
       "      <td>804.005597</td>\n",
       "      <td>796.492310</td>\n",
       "      <td>786.787138</td>\n",
       "      <td>800.870776</td>\n",
       "      <td>791.982323</td>\n",
       "      <td>778.923616</td>\n",
       "      <td>763.872372</td>\n",
       "      <td>766.167084</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.100073</td>\n",
       "      <td>-2.369892</td>\n",
       "      <td>-2.666982</td>\n",
       "      <td>-51.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-61.0</td>\n",
       "      <td>-73.0</td>\n",
       "      <td>-66.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>906.099884</td>\n",
       "      <td>903.507056</td>\n",
       "      <td>910.843565</td>\n",
       "      <td>896.688352</td>\n",
       "      <td>892.235955</td>\n",
       "      <td>875.416472</td>\n",
       "      <td>878.300632</td>\n",
       "      <td>854.687077</td>\n",
       "      <td>857.886939</td>\n",
       "      <td>843.664033</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.497279</td>\n",
       "      <td>-1.784787</td>\n",
       "      <td>-2.076610</td>\n",
       "      <td>-50.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>1</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-72.0</td>\n",
       "      <td>-67.0</td>\n",
       "      <td>-55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19644</th>\n",
       "      <td>381.980366</td>\n",
       "      <td>398.527289</td>\n",
       "      <td>370.951479</td>\n",
       "      <td>372.108855</td>\n",
       "      <td>352.346420</td>\n",
       "      <td>359.233907</td>\n",
       "      <td>370.000000</td>\n",
       "      <td>413.997585</td>\n",
       "      <td>420.138073</td>\n",
       "      <td>454.862617</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.977215</td>\n",
       "      <td>-2.212040</td>\n",
       "      <td>-2.563840</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>49</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-57.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19645</th>\n",
       "      <td>737.651679</td>\n",
       "      <td>730.928861</td>\n",
       "      <td>707.471554</td>\n",
       "      <td>673.190909</td>\n",
       "      <td>665.090219</td>\n",
       "      <td>689.385233</td>\n",
       "      <td>679.574131</td>\n",
       "      <td>666.066813</td>\n",
       "      <td>646.891026</td>\n",
       "      <td>636.792745</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.747335</td>\n",
       "      <td>-1.038664</td>\n",
       "      <td>-1.386254</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "      <td>-53.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19646</th>\n",
       "      <td>364.329521</td>\n",
       "      <td>391.318285</td>\n",
       "      <td>383.775976</td>\n",
       "      <td>384.610192</td>\n",
       "      <td>367.828765</td>\n",
       "      <td>339.330223</td>\n",
       "      <td>346.236913</td>\n",
       "      <td>376.766506</td>\n",
       "      <td>421.283752</td>\n",
       "      <td>448.486343</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.370119</td>\n",
       "      <td>-1.260481</td>\n",
       "      <td>-2.098871</td>\n",
       "      <td>-57.0</td>\n",
       "      <td>148.0</td>\n",
       "      <td>49</td>\n",
       "      <td>-45.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19647</th>\n",
       "      <td>396.021464</td>\n",
       "      <td>409.890229</td>\n",
       "      <td>392.439804</td>\n",
       "      <td>391.154701</td>\n",
       "      <td>374.786606</td>\n",
       "      <td>380.443163</td>\n",
       "      <td>384.193961</td>\n",
       "      <td>434.991954</td>\n",
       "      <td>450.813709</td>\n",
       "      <td>472.956658</td>\n",
       "      <td>...</td>\n",
       "      <td>0.532844</td>\n",
       "      <td>0.208257</td>\n",
       "      <td>-0.127813</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>224.0</td>\n",
       "      <td>49</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-64.0</td>\n",
       "      <td>-55.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19648</th>\n",
       "      <td>734.994558</td>\n",
       "      <td>726.389703</td>\n",
       "      <td>717.432227</td>\n",
       "      <td>688.447529</td>\n",
       "      <td>683.183723</td>\n",
       "      <td>684.211225</td>\n",
       "      <td>688.442445</td>\n",
       "      <td>673.167141</td>\n",
       "      <td>681.950145</td>\n",
       "      <td>663.717560</td>\n",
       "      <td>...</td>\n",
       "      <td>0.859337</td>\n",
       "      <td>0.389644</td>\n",
       "      <td>-0.102262</td>\n",
       "      <td>-58.0</td>\n",
       "      <td>136.0</td>\n",
       "      <td>49</td>\n",
       "      <td>-54.0</td>\n",
       "      <td>-65.0</td>\n",
       "      <td>-56.0</td>\n",
       "      <td>-59.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>19649 rows × 103 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1           2           3           4           5  \\\n",
       "0      919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
       "1      698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
       "2      745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
       "3      830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
       "4      906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
       "...           ...         ...         ...         ...         ...         ...   \n",
       "19644  381.980366  398.527289  370.951479  372.108855  352.346420  359.233907   \n",
       "19645  737.651679  730.928861  707.471554  673.190909  665.090219  689.385233   \n",
       "19646  364.329521  391.318285  383.775976  384.610192  367.828765  339.330223   \n",
       "19647  396.021464  409.890229  392.439804  391.154701  374.786606  380.443163   \n",
       "19648  734.994558  726.389703  717.432227  688.447529  683.183723  684.211225   \n",
       "\n",
       "                6           7           8           9  ...        93  \\\n",
       "0      888.473973  871.023536  836.316328  841.770753  ... -2.467120   \n",
       "1      687.674342  780.946221  803.560203  863.103702  ...  2.256683   \n",
       "2      736.619983  852.877482  846.442556  880.032954  ... -1.275624   \n",
       "3      791.982323  778.923616  763.872372  766.167084  ... -2.100073   \n",
       "4      878.300632  854.687077  857.886939  843.664033  ... -1.497279   \n",
       "...           ...         ...         ...         ...  ...       ...   \n",
       "19644  370.000000  413.997585  420.138073  454.862617  ... -1.977215   \n",
       "19645  679.574131  666.066813  646.891026  636.792745  ... -0.747335   \n",
       "19646  346.236913  376.766506  421.283752  448.486343  ... -0.370119   \n",
       "19647  384.193961  434.991954  450.813709  472.956658  ...  0.532844   \n",
       "19648  688.442445  673.167141  681.950145  663.717560  ...  0.859337   \n",
       "\n",
       "             94        95    96     97  Label  AP1_Rssi  AP2_Rssi  AP3_Rssi  \\\n",
       "0     -2.666997 -2.982241 -50.0  136.0      1     -62.0     -72.0     -65.0   \n",
       "1      2.035376  1.737005 -49.0  148.0      1     -61.0     -73.0     -66.0   \n",
       "2     -1.535994 -1.899522 -50.0  224.0      1     -62.0     -72.0     -66.0   \n",
       "3     -2.369892 -2.666982 -51.0  136.0      1     -61.0     -73.0     -66.0   \n",
       "4     -1.784787 -2.076610 -50.0  136.0      1     -54.0     -72.0     -67.0   \n",
       "...         ...       ...   ...    ...    ...       ...       ...       ...   \n",
       "19644 -2.212040 -2.563840 -56.0  148.0     49     -54.0     -65.0     -53.0   \n",
       "19645 -1.038664 -1.386254 -58.0  136.0     49     -53.0     -64.0     -55.0   \n",
       "19646 -1.260481 -2.098871 -57.0  148.0     49     -45.0     -64.0     -54.0   \n",
       "19647  0.208257 -0.127813 -58.0  224.0     49     -54.0     -64.0     -55.0   \n",
       "19648  0.389644 -0.102262 -58.0  136.0     49     -54.0     -65.0     -56.0   \n",
       "\n",
       "       AP4_Rssi  \n",
       "0         -54.0  \n",
       "1         -56.0  \n",
       "2         -55.0  \n",
       "3         -55.0  \n",
       "4         -55.0  \n",
       "...         ...  \n",
       "19644     -57.0  \n",
       "19645     -59.0  \n",
       "19646     -59.0  \n",
       "19647     -59.0  \n",
       "19648     -59.0  \n",
       "\n",
       "[19649 rows x 103 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 精簡後的 combined_slim： (19649, 53)\n",
      "            0           1           2           3           4           5  \\\n",
      "0  919.592301  929.595611  877.760787  898.203763  851.400023  893.573164   \n",
      "1  698.204841  732.963846  684.079674  694.257877  651.079872  672.521375   \n",
      "2  745.268408  768.188128  706.606680  713.252410  681.482208  713.950278   \n",
      "3  830.278267  829.860832  804.005597  796.492310  786.787138  800.870776   \n",
      "4  906.099884  903.507056  910.843565  896.688352  892.235955  875.416472   \n",
      "\n",
      "            6           7           8           9  ...          43  \\\n",
      "0  888.473973  871.023536  836.316328  841.770753  ...  587.824804   \n",
      "1  687.674342  780.946221  803.560203  863.103702  ...  433.706122   \n",
      "2  736.619983  852.877482  846.442556  880.032954  ...  458.224836   \n",
      "3  791.982323  778.923616  763.872372  766.167084  ...  544.488751   \n",
      "4  878.300632  854.687077  857.886939  843.664033  ...  634.679447   \n",
      "\n",
      "           44          45          46          47  Label  AP1_Rssi  AP2_Rssi  \\\n",
      "0  586.164653  581.278763  612.719348  623.904640      1     -62.0     -72.0   \n",
      "1  461.807319  448.402721  479.860396  465.413794      1     -61.0     -73.0   \n",
      "2  473.003171  464.070038  517.313251  498.703319      1     -62.0     -72.0   \n",
      "3  535.111203  538.710497  567.860018  571.126081      1     -61.0     -73.0   \n",
      "4  623.080252  612.654878  602.747874  604.723077      1     -54.0     -72.0   \n",
      "\n",
      "   AP3_Rssi  AP4_Rssi  \n",
      "0     -65.0     -54.0  \n",
      "1     -66.0     -56.0  \n",
      "2     -66.0     -55.0  \n",
      "3     -66.0     -55.0  \n",
      "4     -67.0     -55.0  \n",
      "\n",
      "[5 rows x 53 columns]\n"
     ]
    }
   ],
   "source": [
    "columns_to_keep = list(range(48)) + [\"Label\", \"AP1_Rssi\", \"AP2_Rssi\", \"AP3_Rssi\", \"AP4_Rssi\"]\n",
    "combined_slim = combined[columns_to_keep]\n",
    "\n",
    "print(\"✅ 精簡後的 combined_slim：\", combined_slim.shape)\n",
    "print(combined_slim.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "rp_labels = combined['Label']\n",
    "\n",
    "# 移除 Label 和 RSSI 欄位，只留下 CSI 數據\n",
    "csi = combined.drop(columns=['Label', 'AP1_Rssi', 'AP2_Rssi', 'AP3_Rssi', 'AP4_Rssi'])\n",
    "\n",
    "# 取前 48 欄當作 amplitude（如果這是你定義的）\n",
    "amp = combined.iloc[:, :48]\n",
    "\n",
    "# 取出 RSSI 四個欄位\n",
    "rssi = combined[['AP1_Rssi', 'AP2_Rssi', 'AP3_Rssi', 'AP4_Rssi']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "csi shape: (19649, 4)\n",
      "amp shape: (19649, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"csi shape:\", rssi.shape)\n",
    "print(\"amp shape:\", amp.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 精簡資料集已儲存至：/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/combined_csi_rssi_slim.csv\n"
     ]
    }
   ],
   "source": [
    "# 選擇要保留的欄位\n",
    "columns_to_keep = list(range(48)) + ['AP1_Rssi', 'AP2_Rssi', 'AP3_Rssi', 'AP4_Rssi', 'Label']\n",
    "\n",
    "# 建立精簡資料集\n",
    "combined_slim = combined[columns_to_keep]\n",
    "\n",
    "# 儲存\n",
    "save_path_slim = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/combined_csi_rssi_slim.csv\"\n",
    "combined_slim.to_csv(save_path_slim, index=False)\n",
    "print(f\"✅ 精簡資料集已儲存至：{save_path_slim}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 資料集已儲存至：/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/combined_csi_rssi_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "save_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/combined_csi_rssi_dataset.csv\"  # 可自行改路徑\n",
    "combined.to_csv(save_path, index=False)\n",
    "print(f\"✅ 資料集已儲存至：{save_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ 讀取成功，資料維度： (19649, 53)\n",
      "補完 NaN 之後，還有 NaN 嗎？ 0\n",
      "------------------\n",
      "✅ [amp] 切出來： [610.26223871 613.77520315 619.31332942 619.26569419 605.18592185]\n",
      "✅ [amp] 原始 df： [610.26223871 613.77520315 619.31332942 619.26569419 605.18592185]\n",
      "✅ [rssi] 切出來： [-62. -69. -64. -53.]\n",
      "✅ [rssi] 原始 df： [-62. -69. -64. -53.]\n",
      "✅ [label] 切出來： 4\n",
      "✅ [label] 原始 df： 4.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 路徑\n",
    "load_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/rssi/combined_csi_rssi_slim.csv\"\n",
    "\n",
    "# 讀取資料\n",
    "df = pd.read_csv(load_path)\n",
    "print(\"✅ 讀取成功，資料維度：\", df.shape)\n",
    "\n",
    "# 針對 RSSI 欄補 0\n",
    "rssi_cols = [\"AP1_Rssi\", \"AP2_Rssi\", \"AP3_Rssi\", \"AP4_Rssi\"]\n",
    "df[rssi_cols] = df[rssi_cols].fillna(0)\n",
    "\n",
    "# 檢查有沒有補乾淨\n",
    "print(\"補完 NaN 之後，還有 NaN 嗎？\", df[rssi_cols].isnull().sum().sum())  # 應為 0\n",
    "print(\"------------------\")\n",
    "\n",
    "# 保留 index 方便對照\n",
    "df[\"orig_index\"] = df.index\n",
    "\n",
    "# 特徵分割\n",
    "amp = df.iloc[:, 0:48].values\n",
    "rssi = df[rssi_cols].values\n",
    "labels = df[\"Label\"].values\n",
    "orig_idx = df[\"orig_index\"].values\n",
    "\n",
    "# 切 train / temp\n",
    "amp_train, amp_temp, rssi_train, rssi_temp, y_train, y_temp, idx_train, idx_temp = train_test_split(\n",
    "    amp, rssi, labels, orig_idx, test_size=0.3, random_state=42\n",
    ")\n",
    "\n",
    "# 切 val / test\n",
    "amp_val, amp_test, rssi_val, rssi_test, y_val, y_test, idx_val, idx_test = train_test_split(\n",
    "    amp_temp, rssi_temp, y_temp, idx_temp, test_size=1/3, random_state=42\n",
    ")\n",
    "\n",
    "# 檢查一筆對不對齊\n",
    "i = 0\n",
    "print(\"✅ [amp] 切出來：\", amp_train[i][:5])\n",
    "print(\"✅ [amp] 原始 df：\", df.iloc[int(idx_train[i]), 0:5].values)\n",
    "\n",
    "print(\"✅ [rssi] 切出來：\", rssi_train[i])\n",
    "print(\"✅ [rssi] 原始 df：\", df.iloc[int(idx_train[i])][rssi_cols].values)\n",
    "\n",
    "print(\"✅ [label] 切出來：\", y_train[i])\n",
    "print(\"✅ [label] 原始 df：\", df.iloc[int(idx_train[i])][\"Label\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13754, 48)\n",
      "(13754, 4)\n",
      "(3930, 48)\n",
      "(3930, 4)\n",
      "(1965, 48)\n",
      "(1965, 4)\n"
     ]
    }
   ],
   "source": [
    "print(amp_train.shape)\n",
    "print(rssi_train.shape)\n",
    "print(amp_val.shape)\n",
    "print(rssi_val.shape)\n",
    "print(amp_test.shape)\n",
    "print(rssi_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[275.11815643, 270.20177646, 240.10206163, ..., 402.15046935,\n",
       "        410.60077935, 379.40084344],\n",
       "       [270.8671261 , 292.54914117, 277.22193275, ..., 316.07752214,\n",
       "        326.38627422, 308.18500937],\n",
       "       [488.18131877, 514.4171459 , 482.28000166, ..., 112.29425631,\n",
       "        122.58874337, 116.92732786],\n",
       "       ...,\n",
       "       [412.00121359, 422.47011729, 408.88629226, ..., 461.49431199,\n",
       "        471.78172919, 442.92211505],\n",
       "       [512.44511901, 531.16099254, 488.19053657, ..., 408.04411526,\n",
       "        443.66766842, 432.08911118],\n",
       "       [625.51898452, 612.95105841, 599.35381871, ..., 629.05961562,\n",
       "        631.14182241, 606.79238624]])"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "amp_train shape: (13754, 48)\n"
     ]
    }
   ],
   "source": [
    "print(\"amp_train shape:\", amp_train.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 3600x1200 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 將 y_train, y_val, y_test 轉為 Series（如果尚未轉換）\n",
    "y_train_series = pd.Series(y_train)\n",
    "y_val_series = pd.Series(y_val)\n",
    "y_test_series = pd.Series(y_test)\n",
    "\n",
    "# 設定畫圖樣式與解析度\n",
    "plt.figure(figsize=(12, 4), dpi=300)\n",
    "\n",
    "# 畫三張子圖\n",
    "for i, (data, title) in enumerate([\n",
    "    (y_train_series, \"Train Label Distribution\"),\n",
    "    (y_val_series, \"Validation Label Distribution\"),\n",
    "    (y_test_series, \"Test Label Distribution\")\n",
    "]):\n",
    "    plt.subplot(1, 3, i + 1)\n",
    "    vc_prop = data.value_counts(normalize=True).sort_index()\n",
    "    ax = vc_prop.plot(kind='bar')\n",
    "    ax.set_ylim([0, 0.04])  # 可依據實際數據微調\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel('Label')\n",
    "    ax.set_ylabel('Proportion')\n",
    "    ax.tick_params(axis='x', labelrotation=90)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "y_train_oh = encoder.fit_transform(np.array(y_train).reshape(-1, 1))\n",
    "y_val_oh = encoder.transform(np.array(y_val).reshape(-1, 1))\n",
    "y_test_oh = encoder.transform(np.array(y_test).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y_train_oh shape: (13754, 49)\n",
      "y_val_oh shape: (3930, 49)\n",
      "y_test_oh shape: (1965, 49)\n"
     ]
    }
   ],
   "source": [
    "print(\"y_train_oh shape:\", y_train_oh.shape)\n",
    "print(\"y_val_oh shape:\", y_val_oh.shape)\n",
    "print(\"y_test_oh shape:\", y_test_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建 Dataset 和 DataLoader\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_dataset = CSIRSSIDataset(np.array(amp_train), np.array(rssi_train), y_train_oh, augment=True, csi_noise_std=0.00, rssi_mask_prob=0.05)\n",
    "val_dataset = CSIRSSIDataset(np.array(amp_val),np.array(rssi_val), y_val_oh)\n",
    "test_dataset = CSIRSSIDataset(np.array(amp_test), np.array(rssi_test), y_test_oh)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "測試資料數量： 1965\n"
     ]
    }
   ],
   "source": [
    "print(\"測試資料數量：\", len(test_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 1., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 1.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [1., 0., 0.,  ..., 0., 0., 0.]])\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for amp_inputs, rssi_inputs, labels in train_loader:\n",
    "        print(labels)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 重複測試"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "當前工作目錄: /media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN\n",
      "這裡有哪些檔案: ['best_model_tmp.pth', '__init__.py', 'training_results.txt', 'csidataset.py', 'train.ipynb', 'csi_val_acc.png', 'repeat_copy', 'data_loader.py', 'csi_train_loss.png', 'train_soft_label.ipynb', 'rssicsi_train.ipynb', 'best_csirssi_classifier_bn_temp.pth', 'best_csirssi_reg_temp.pth', 'repeat_for_zero_shot', 'repeat', 'csi_train_acc.png', 'models_save', 'model_save_soft_label', '__pycache__', 'csi_val_loss.png', 'model.py', 'rssicsi_train copy.ipynb', 'train copy.ipynb', 'zero_shot_test.ipynb', 'best_csi_reg_temp.pth', 'train_mirco.ipynb', 'soft_labels.npz']\n",
      "當前工作目錄: /media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "print(\"當前工作目錄:\", os.getcwd())\n",
    "print(\"這裡有哪些檔案:\", os.listdir())\n",
    "#os.chdir('/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/model_CNN')\n",
    "print(\"當前工作目錄:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# +BN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# 假設模型已經定義好\n",
    "class CSIRSSI_Classifier_BN(nn.Module):\n",
    "    def __init__(self, num_classes=49, rssi_dim=4):\n",
    "        super(CSIRSSI_Classifier_BN, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        self.flatten_dim = 128 * 12\n",
    "        self.fc1   = nn.Linear(self.flatten_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2   = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc_rssi1 = nn.Linear(rssi_dim, 128)\n",
    "        self.bn_rssi1 = nn.BatchNorm1d(128)\n",
    "        self.dropout_rssi1 = nn.Dropout(0.5)\n",
    "        self.fc_rssi2 = nn.Linear(128, 32)\n",
    "        self.bn_rssi2 = nn.BatchNorm1d(32)\n",
    "        self.dropout_rssi2 = nn.Dropout(0.5)\n",
    "\n",
    "        self.fc_fusion = nn.Linear(64 + 32, 64)\n",
    "        self.dropout_fusion = nn.Dropout(0.3)\n",
    "        self.fc_class = nn.Linear(64, num_classes)\n",
    "\n",
    "    def forward(self, csi_input, rssi_input):\n",
    "        if csi_input.dim() == 2:\n",
    "            csi_input = csi_input.unsqueeze(1)\n",
    "        x = F.relu(self.bn1(self.conv1(csi_input)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        csi_feat = F.relu(self.fc2(x))\n",
    "        csi_feat = self.dropout2(csi_feat)\n",
    "\n",
    "        rssi_feat = F.relu(self.bn_rssi1(self.fc_rssi1(rssi_input)))\n",
    "        rssi_feat = self.dropout_rssi1(rssi_feat)\n",
    "        rssi_feat = F.relu(self.bn_rssi2(self.fc_rssi2(rssi_feat)))\n",
    "        rssi_feat = self.dropout_rssi2(rssi_feat)\n",
    "\n",
    "        fusion = torch.cat([csi_feat, rssi_feat], dim=1)\n",
    "        fusion = F.relu(self.fc_fusion(fusion))\n",
    "        fusion = self.dropout_fusion(fusion)\n",
    "\n",
    "        class_out = self.fc_class(fusion)\n",
    "        return class_out\n",
    "\n",
    "    \n",
    "    def extract_embedding(self, csi_input, rssi_input):\n",
    "        if csi_input.dim() == 2:\n",
    "            csi_input = csi_input.unsqueeze(1)\n",
    "        x = F.relu(self.bn1(self.conv1(csi_input)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        csi_feat = F.relu(self.fc2(x))\n",
    "        rssi_feat = F.relu(self.bn_rssi1(self.fc_rssi1(rssi_input)))\n",
    "        rssi_feat = F.relu(self.bn_rssi2(self.fc_rssi2(rssi_feat)))\n",
    "        fusion = torch.cat([csi_feat, rssi_feat], dim=1)\n",
    "        fusion = F.relu(self.fc_fusion(fusion))\n",
    "        return fusion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#+best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Run 1/5 ===\n",
      "Run 1, Epoch 1, val_loss = 0.678859\n",
      "Run 1, Epoch 2, val_loss = 0.170541\n",
      "Run 1, Epoch 3, val_loss = 0.095648\n",
      "Run 1, Epoch 4, val_loss = 0.065961\n",
      "Run 1, Epoch 5, val_loss = 0.073683\n",
      "Run 1, Epoch 6, val_loss = 0.055937\n",
      "Run 1, Epoch 7, val_loss = 0.043618\n",
      "Run 1, Epoch 8, val_loss = 0.028258\n",
      "Run 1, Epoch 9, val_loss = 0.028968\n",
      "Run 1, Epoch 10, val_loss = 0.023270\n",
      "Run 1, Epoch 11, val_loss = 0.021583\n",
      "Run 1, Epoch 12, val_loss = 0.023642\n",
      "Run 1, Epoch 13, val_loss = 0.022980\n",
      "Run 1, Epoch 14, val_loss = 0.026863\n",
      "Run 1, Epoch 15, val_loss = 0.041865\n",
      "Run 1, Epoch 16, val_loss = 0.031126\n",
      "Run 1, Epoch 17, val_loss = 0.015364\n",
      "Run 1, Epoch 18, val_loss = 0.022832\n",
      "Run 1, Epoch 19, val_loss = 0.018353\n",
      "Run 1, Epoch 20, val_loss = 0.025645\n",
      "Run 1, Epoch 21, val_loss = 0.019921\n",
      "Run 1, Epoch 22, val_loss = 0.026924\n",
      "Run 1, Epoch 23, val_loss = 0.017053\n",
      "Run 1, Epoch 24, val_loss = 0.016713\n",
      "Run 1, Epoch 25, val_loss = 0.015505\n",
      "Run 1, Epoch 26, val_loss = 0.023583\n",
      "Run 1, Epoch 27, val_loss = 0.015204\n",
      "Run 1, Epoch 28, val_loss = 0.016365\n",
      "Run 1, Epoch 29, val_loss = 0.013526\n",
      "Run 1, Epoch 30, val_loss = 0.016711\n",
      "Run 1, Epoch 31, val_loss = 0.014720\n",
      "Run 1, Epoch 32, val_loss = 0.022287\n",
      "Run 1, Epoch 33, val_loss = 0.022845\n",
      "Run 1, Epoch 34, val_loss = 0.019265\n",
      "Run 1, Epoch 35, val_loss = 0.021196\n",
      "Run 1, Epoch 36, val_loss = 0.013103\n",
      "Run 1, Epoch 37, val_loss = 0.009442\n",
      "Run 1, Epoch 38, val_loss = 0.010422\n",
      "Run 1, Epoch 39, val_loss = 0.007176\n",
      "Run 1, Epoch 40, val_loss = 0.009959\n",
      "Run 1, Epoch 41, val_loss = 0.009034\n",
      "Run 1, Epoch 42, val_loss = 0.007020\n",
      "Run 1, Epoch 43, val_loss = 0.006014\n",
      "Run 1, Epoch 44, val_loss = 0.012830\n",
      "Run 1, Epoch 45, val_loss = 0.008874\n",
      "Run 1, Epoch 46, val_loss = 0.009366\n",
      "Run 1, Epoch 47, val_loss = 0.016679\n",
      "Run 1, Epoch 48, val_loss = 0.020930\n",
      "Run 1, Epoch 49, val_loss = 0.013986\n",
      "Run 1, Epoch 50, val_loss = 0.017384\n",
      "Run 1, Epoch 51, val_loss = 0.013954\n",
      "Run 1, Epoch 52, val_loss = 0.012381\n",
      "Run 1, Epoch 53, val_loss = 0.011343\n",
      "Run 1, Epoch 54, val_loss = 0.013368\n",
      "Run 1, Epoch 55, val_loss = 0.018365\n",
      "Run 1, Epoch 56, val_loss = 0.020972\n",
      "Run 1, Epoch 57, val_loss = 0.013494\n",
      "Run 1, Epoch 58, val_loss = 0.017671\n",
      "Run 1, Epoch 59, val_loss = 0.028622\n",
      "Run 1, Epoch 60, val_loss = 0.018004\n",
      "Run 1, Epoch 61, val_loss = 0.021802\n",
      "Run 1, Epoch 62, val_loss = 0.020496\n",
      "Run 1, Epoch 63, val_loss = 0.015895\n",
      "✅ Run 1: Acc = 99.75%, MDE = 0.0101\n",
      "\n",
      "=== Run 2/5 ===\n",
      "Run 2, Epoch 1, val_loss = 0.685704\n",
      "Run 2, Epoch 2, val_loss = 0.244113\n",
      "Run 2, Epoch 3, val_loss = 0.140152\n",
      "Run 2, Epoch 4, val_loss = 0.094170\n",
      "Run 2, Epoch 5, val_loss = 0.077633\n",
      "Run 2, Epoch 6, val_loss = 0.054417\n",
      "Run 2, Epoch 7, val_loss = 0.043362\n",
      "Run 2, Epoch 8, val_loss = 0.068949\n",
      "Run 2, Epoch 9, val_loss = 0.044295\n",
      "Run 2, Epoch 10, val_loss = 0.026319\n",
      "Run 2, Epoch 11, val_loss = 0.031745\n",
      "Run 2, Epoch 12, val_loss = 0.026133\n",
      "Run 2, Epoch 13, val_loss = 0.023759\n",
      "Run 2, Epoch 14, val_loss = 0.030115\n",
      "Run 2, Epoch 15, val_loss = 0.028246\n",
      "Run 2, Epoch 16, val_loss = 0.022258\n",
      "Run 2, Epoch 17, val_loss = 0.019448\n",
      "Run 2, Epoch 18, val_loss = 0.023271\n",
      "Run 2, Epoch 19, val_loss = 0.012114\n",
      "Run 2, Epoch 20, val_loss = 0.031658\n",
      "Run 2, Epoch 21, val_loss = 0.019514\n",
      "Run 2, Epoch 22, val_loss = 0.011137\n",
      "Run 2, Epoch 23, val_loss = 0.014631\n",
      "Run 2, Epoch 24, val_loss = 0.015472\n",
      "Run 2, Epoch 25, val_loss = 0.011830\n",
      "Run 2, Epoch 26, val_loss = 0.015793\n",
      "Run 2, Epoch 27, val_loss = 0.020795\n",
      "Run 2, Epoch 28, val_loss = 0.011100\n",
      "Run 2, Epoch 29, val_loss = 0.016498\n",
      "Run 2, Epoch 30, val_loss = 0.012968\n",
      "Run 2, Epoch 31, val_loss = 0.013158\n",
      "Run 2, Epoch 32, val_loss = 0.014851\n",
      "Run 2, Epoch 33, val_loss = 0.012037\n",
      "Run 2, Epoch 34, val_loss = 0.011669\n",
      "Run 2, Epoch 35, val_loss = 0.013084\n",
      "Run 2, Epoch 36, val_loss = 0.008954\n",
      "Run 2, Epoch 37, val_loss = 0.011099\n",
      "Run 2, Epoch 38, val_loss = 0.012836\n",
      "Run 2, Epoch 39, val_loss = 0.011274\n",
      "Run 2, Epoch 40, val_loss = 0.009296\n",
      "Run 2, Epoch 41, val_loss = 0.014981\n",
      "Run 2, Epoch 42, val_loss = 0.014154\n",
      "Run 2, Epoch 43, val_loss = 0.009401\n",
      "Run 2, Epoch 44, val_loss = 0.013133\n",
      "Run 2, Epoch 45, val_loss = 0.013939\n",
      "Run 2, Epoch 46, val_loss = 0.017588\n",
      "Run 2, Epoch 47, val_loss = 0.012103\n",
      "Run 2, Epoch 48, val_loss = 0.016837\n",
      "Run 2, Epoch 49, val_loss = 0.011610\n",
      "Run 2, Epoch 50, val_loss = 0.012313\n",
      "Run 2, Epoch 51, val_loss = 0.016137\n",
      "Run 2, Epoch 52, val_loss = 0.010534\n",
      "Run 2, Epoch 53, val_loss = 0.014035\n",
      "Run 2, Epoch 54, val_loss = 0.008606\n",
      "Run 2, Epoch 55, val_loss = 0.012508\n",
      "Run 2, Epoch 56, val_loss = 0.010180\n",
      "Run 2, Epoch 57, val_loss = 0.009176\n",
      "Run 2, Epoch 58, val_loss = 0.008293\n",
      "Run 2, Epoch 59, val_loss = 0.010124\n",
      "Run 2, Epoch 60, val_loss = 0.008467\n",
      "Run 2, Epoch 61, val_loss = 0.007590\n",
      "Run 2, Epoch 62, val_loss = 0.007791\n",
      "Run 2, Epoch 63, val_loss = 0.010312\n",
      "Run 2, Epoch 64, val_loss = 0.006884\n",
      "Run 2, Epoch 65, val_loss = 0.010171\n",
      "Run 2, Epoch 66, val_loss = 0.014953\n",
      "Run 2, Epoch 67, val_loss = 0.011573\n",
      "Run 2, Epoch 68, val_loss = 0.017853\n",
      "Run 2, Epoch 69, val_loss = 0.018560\n",
      "Run 2, Epoch 70, val_loss = 0.014859\n",
      "Run 2, Epoch 71, val_loss = 0.006606\n",
      "Run 2, Epoch 72, val_loss = 0.008236\n",
      "Run 2, Epoch 73, val_loss = 0.009957\n",
      "Run 2, Epoch 74, val_loss = 0.011435\n",
      "Run 2, Epoch 75, val_loss = 0.012442\n",
      "Run 2, Epoch 76, val_loss = 0.013137\n",
      "Run 2, Epoch 77, val_loss = 0.012522\n",
      "Run 2, Epoch 78, val_loss = 0.010826\n",
      "Run 2, Epoch 79, val_loss = 0.010751\n",
      "Run 2, Epoch 80, val_loss = 0.009045\n",
      "Run 2, Epoch 81, val_loss = 0.010304\n",
      "Run 2, Epoch 82, val_loss = 0.012107\n",
      "Run 2, Epoch 83, val_loss = 0.012719\n",
      "Run 2, Epoch 84, val_loss = 0.012173\n",
      "Run 2, Epoch 85, val_loss = 0.012705\n",
      "Run 2, Epoch 86, val_loss = 0.013515\n",
      "Run 2, Epoch 87, val_loss = 0.010255\n",
      "Run 2, Epoch 88, val_loss = 0.010812\n",
      "Run 2, Epoch 89, val_loss = 0.010467\n",
      "Run 2, Epoch 90, val_loss = 0.009494\n",
      "Run 2, Epoch 91, val_loss = 0.012435\n",
      "✅ Run 2: Acc = 99.69%, MDE = 0.0107\n",
      "\n",
      "=== Run 3/5 ===\n",
      "Run 3, Epoch 1, val_loss = 0.653215\n",
      "Run 3, Epoch 2, val_loss = 0.188662\n",
      "Run 3, Epoch 3, val_loss = 0.119652\n",
      "Run 3, Epoch 4, val_loss = 0.071580\n",
      "Run 3, Epoch 5, val_loss = 0.064997\n",
      "Run 3, Epoch 6, val_loss = 0.051801\n",
      "Run 3, Epoch 7, val_loss = 0.035045\n",
      "Run 3, Epoch 8, val_loss = 0.044014\n",
      "Run 3, Epoch 9, val_loss = 0.040793\n",
      "Run 3, Epoch 10, val_loss = 0.032684\n",
      "Run 3, Epoch 11, val_loss = 0.032020\n",
      "Run 3, Epoch 12, val_loss = 0.026167\n",
      "Run 3, Epoch 13, val_loss = 0.032850\n",
      "Run 3, Epoch 14, val_loss = 0.035562\n",
      "Run 3, Epoch 15, val_loss = 0.020928\n",
      "Run 3, Epoch 16, val_loss = 0.024922\n",
      "Run 3, Epoch 17, val_loss = 0.041283\n",
      "Run 3, Epoch 18, val_loss = 0.049376\n",
      "Run 3, Epoch 19, val_loss = 0.025791\n",
      "Run 3, Epoch 20, val_loss = 0.024018\n",
      "Run 3, Epoch 21, val_loss = 0.029582\n",
      "Run 3, Epoch 22, val_loss = 0.021231\n",
      "Run 3, Epoch 23, val_loss = 0.023422\n",
      "Run 3, Epoch 24, val_loss = 0.028938\n",
      "Run 3, Epoch 25, val_loss = 0.032376\n",
      "Run 3, Epoch 26, val_loss = 0.020476\n",
      "Run 3, Epoch 27, val_loss = 0.023263\n",
      "Run 3, Epoch 28, val_loss = 0.020914\n",
      "Run 3, Epoch 29, val_loss = 0.020717\n",
      "Run 3, Epoch 30, val_loss = 0.036519\n",
      "Run 3, Epoch 31, val_loss = 0.027989\n",
      "Run 3, Epoch 32, val_loss = 0.018308\n",
      "Run 3, Epoch 33, val_loss = 0.016905\n",
      "Run 3, Epoch 34, val_loss = 0.024581\n",
      "Run 3, Epoch 35, val_loss = 0.011097\n",
      "Run 3, Epoch 36, val_loss = 0.014454\n",
      "Run 3, Epoch 37, val_loss = 0.024165\n",
      "Run 3, Epoch 38, val_loss = 0.017798\n",
      "Run 3, Epoch 39, val_loss = 0.013371\n",
      "Run 3, Epoch 40, val_loss = 0.015911\n",
      "Run 3, Epoch 41, val_loss = 0.024844\n",
      "Run 3, Epoch 42, val_loss = 0.022528\n",
      "Run 3, Epoch 43, val_loss = 0.032205\n",
      "Run 3, Epoch 44, val_loss = 0.030131\n",
      "Run 3, Epoch 45, val_loss = 0.023811\n",
      "Run 3, Epoch 46, val_loss = 0.023772\n",
      "Run 3, Epoch 47, val_loss = 0.029924\n",
      "Run 3, Epoch 48, val_loss = 0.024493\n",
      "Run 3, Epoch 49, val_loss = 0.025725\n",
      "Run 3, Epoch 50, val_loss = 0.024512\n",
      "Run 3, Epoch 51, val_loss = 0.013991\n",
      "Run 3, Epoch 52, val_loss = 0.022309\n",
      "Run 3, Epoch 53, val_loss = 0.015749\n",
      "Run 3, Epoch 54, val_loss = 0.013232\n",
      "Run 3, Epoch 55, val_loss = 0.020369\n",
      "✅ Run 3: Acc = 99.54%, MDE = 0.0090\n",
      "\n",
      "=== Run 4/5 ===\n",
      "Run 4, Epoch 1, val_loss = 0.851469\n",
      "Run 4, Epoch 2, val_loss = 0.261240\n",
      "Run 4, Epoch 3, val_loss = 0.139350\n",
      "Run 4, Epoch 4, val_loss = 0.101518\n",
      "Run 4, Epoch 5, val_loss = 0.076109\n",
      "Run 4, Epoch 6, val_loss = 0.070305\n",
      "Run 4, Epoch 7, val_loss = 0.060053\n",
      "Run 4, Epoch 8, val_loss = 0.056337\n",
      "Run 4, Epoch 9, val_loss = 0.044831\n",
      "Run 4, Epoch 10, val_loss = 0.040624\n",
      "Run 4, Epoch 11, val_loss = 0.035379\n",
      "Run 4, Epoch 12, val_loss = 0.025162\n",
      "Run 4, Epoch 13, val_loss = 0.022297\n",
      "Run 4, Epoch 14, val_loss = 0.032365\n",
      "Run 4, Epoch 15, val_loss = 0.027907\n",
      "Run 4, Epoch 16, val_loss = 0.030352\n",
      "Run 4, Epoch 17, val_loss = 0.024501\n",
      "Run 4, Epoch 18, val_loss = 0.029516\n",
      "Run 4, Epoch 19, val_loss = 0.017118\n",
      "Run 4, Epoch 20, val_loss = 0.023216\n",
      "Run 4, Epoch 21, val_loss = 0.023138\n",
      "Run 4, Epoch 22, val_loss = 0.017352\n",
      "Run 4, Epoch 23, val_loss = 0.021525\n",
      "Run 4, Epoch 24, val_loss = 0.042159\n",
      "Run 4, Epoch 25, val_loss = 0.018375\n",
      "Run 4, Epoch 26, val_loss = 0.022458\n",
      "Run 4, Epoch 27, val_loss = 0.027730\n",
      "Run 4, Epoch 28, val_loss = 0.013424\n",
      "Run 4, Epoch 29, val_loss = 0.015858\n",
      "Run 4, Epoch 30, val_loss = 0.017962\n",
      "Run 4, Epoch 31, val_loss = 0.018310\n",
      "Run 4, Epoch 32, val_loss = 0.015342\n",
      "Run 4, Epoch 33, val_loss = 0.010498\n",
      "Run 4, Epoch 34, val_loss = 0.023643\n",
      "Run 4, Epoch 35, val_loss = 0.024254\n",
      "Run 4, Epoch 36, val_loss = 0.016093\n",
      "Run 4, Epoch 37, val_loss = 0.017446\n",
      "Run 4, Epoch 38, val_loss = 0.022271\n",
      "Run 4, Epoch 39, val_loss = 0.021049\n",
      "Run 4, Epoch 40, val_loss = 0.019735\n",
      "Run 4, Epoch 41, val_loss = 0.018730\n",
      "Run 4, Epoch 42, val_loss = 0.012041\n",
      "Run 4, Epoch 43, val_loss = 0.020190\n",
      "Run 4, Epoch 44, val_loss = 0.012573\n",
      "Run 4, Epoch 45, val_loss = 0.024358\n",
      "Run 4, Epoch 46, val_loss = 0.016972\n",
      "Run 4, Epoch 47, val_loss = 0.023634\n",
      "Run 4, Epoch 48, val_loss = 0.012735\n",
      "Run 4, Epoch 49, val_loss = 0.025158\n",
      "Run 4, Epoch 50, val_loss = 0.018676\n",
      "Run 4, Epoch 51, val_loss = 0.018926\n",
      "Run 4, Epoch 52, val_loss = 0.017990\n",
      "Run 4, Epoch 53, val_loss = 0.018036\n",
      "✅ Run 4: Acc = 99.69%, MDE = 0.0100\n",
      "\n",
      "=== Run 5/5 ===\n",
      "Run 5, Epoch 1, val_loss = 0.717606\n",
      "Run 5, Epoch 2, val_loss = 0.287250\n",
      "Run 5, Epoch 3, val_loss = 0.144010\n",
      "Run 5, Epoch 4, val_loss = 0.152526\n",
      "Run 5, Epoch 5, val_loss = 0.068687\n",
      "Run 5, Epoch 6, val_loss = 0.054667\n",
      "Run 5, Epoch 7, val_loss = 0.052375\n",
      "Run 5, Epoch 8, val_loss = 0.046013\n",
      "Run 5, Epoch 9, val_loss = 0.043487\n",
      "Run 5, Epoch 10, val_loss = 0.044639\n",
      "Run 5, Epoch 11, val_loss = 0.026204\n",
      "Run 5, Epoch 12, val_loss = 0.033898\n",
      "Run 5, Epoch 13, val_loss = 0.040988\n",
      "Run 5, Epoch 14, val_loss = 0.023299\n",
      "Run 5, Epoch 15, val_loss = 0.022902\n",
      "Run 5, Epoch 16, val_loss = 0.034168\n",
      "Run 5, Epoch 17, val_loss = 0.023446\n",
      "Run 5, Epoch 18, val_loss = 0.021224\n",
      "Run 5, Epoch 19, val_loss = 0.021901\n",
      "Run 5, Epoch 20, val_loss = 0.026942\n",
      "Run 5, Epoch 21, val_loss = 0.014044\n",
      "Run 5, Epoch 22, val_loss = 0.021738\n",
      "Run 5, Epoch 23, val_loss = 0.018289\n",
      "Run 5, Epoch 24, val_loss = 0.026146\n",
      "Run 5, Epoch 25, val_loss = 0.024415\n",
      "Run 5, Epoch 26, val_loss = 0.023213\n",
      "Run 5, Epoch 27, val_loss = 0.024520\n",
      "Run 5, Epoch 28, val_loss = 0.028219\n",
      "Run 5, Epoch 29, val_loss = 0.033306\n",
      "Run 5, Epoch 30, val_loss = 0.017631\n",
      "Run 5, Epoch 31, val_loss = 0.026706\n",
      "Run 5, Epoch 32, val_loss = 0.017553\n",
      "Run 5, Epoch 33, val_loss = 0.015053\n",
      "Run 5, Epoch 34, val_loss = 0.013513\n",
      "Run 5, Epoch 35, val_loss = 0.016864\n",
      "Run 5, Epoch 36, val_loss = 0.012114\n",
      "Run 5, Epoch 37, val_loss = 0.023254\n",
      "Run 5, Epoch 38, val_loss = 0.031810\n",
      "Run 5, Epoch 39, val_loss = 0.017208\n",
      "Run 5, Epoch 40, val_loss = 0.017704\n",
      "Run 5, Epoch 41, val_loss = 0.020492\n",
      "Run 5, Epoch 42, val_loss = 0.017009\n",
      "Run 5, Epoch 43, val_loss = 0.028098\n",
      "Run 5, Epoch 44, val_loss = 0.014403\n",
      "Run 5, Epoch 45, val_loss = 0.011482\n",
      "Run 5, Epoch 46, val_loss = 0.014869\n",
      "Run 5, Epoch 47, val_loss = 0.023937\n",
      "Run 5, Epoch 48, val_loss = 0.023646\n",
      "Run 5, Epoch 49, val_loss = 0.029261\n",
      "Run 5, Epoch 50, val_loss = 0.026747\n",
      "Run 5, Epoch 51, val_loss = 0.021166\n",
      "Run 5, Epoch 52, val_loss = 0.024480\n",
      "Run 5, Epoch 53, val_loss = 0.015050\n",
      "Run 5, Epoch 54, val_loss = 0.014867\n",
      "Run 5, Epoch 55, val_loss = 0.019862\n",
      "Run 5, Epoch 56, val_loss = 0.014402\n",
      "Run 5, Epoch 57, val_loss = 0.012646\n",
      "Run 5, Epoch 58, val_loss = 0.016412\n",
      "Run 5, Epoch 59, val_loss = 0.022302\n",
      "Run 5, Epoch 60, val_loss = 0.011228\n",
      "Run 5, Epoch 61, val_loss = 0.015288\n",
      "Run 5, Epoch 62, val_loss = 0.010490\n",
      "Run 5, Epoch 63, val_loss = 0.019433\n",
      "Run 5, Epoch 64, val_loss = 0.016478\n",
      "Run 5, Epoch 65, val_loss = 0.011196\n",
      "Run 5, Epoch 66, val_loss = 0.013853\n",
      "Run 5, Epoch 67, val_loss = 0.023645\n",
      "Run 5, Epoch 68, val_loss = 0.017289\n",
      "Run 5, Epoch 69, val_loss = 0.018561\n",
      "Run 5, Epoch 70, val_loss = 0.015599\n",
      "Run 5, Epoch 71, val_loss = 0.011228\n",
      "Run 5, Epoch 72, val_loss = 0.010957\n",
      "Run 5, Epoch 73, val_loss = 0.016240\n",
      "Run 5, Epoch 74, val_loss = 0.013366\n",
      "Run 5, Epoch 75, val_loss = 0.021679\n",
      "Run 5, Epoch 76, val_loss = 0.017590\n",
      "Run 5, Epoch 77, val_loss = 0.015405\n",
      "Run 5, Epoch 78, val_loss = 0.020019\n",
      "Run 5, Epoch 79, val_loss = 0.011666\n",
      "Run 5, Epoch 80, val_loss = 0.011786\n",
      "Run 5, Epoch 81, val_loss = 0.010117\n",
      "Run 5, Epoch 82, val_loss = 0.015393\n",
      "Run 5, Epoch 83, val_loss = 0.016260\n",
      "Run 5, Epoch 84, val_loss = 0.024714\n",
      "Run 5, Epoch 85, val_loss = 0.018438\n",
      "Run 5, Epoch 86, val_loss = 0.024056\n",
      "Run 5, Epoch 87, val_loss = 0.015004\n",
      "Run 5, Epoch 88, val_loss = 0.012398\n",
      "Run 5, Epoch 89, val_loss = 0.017054\n",
      "Run 5, Epoch 90, val_loss = 0.018863\n",
      "Run 5, Epoch 91, val_loss = 0.017948\n",
      "Run 5, Epoch 92, val_loss = 0.018777\n",
      "Run 5, Epoch 93, val_loss = 0.019121\n",
      "Run 5, Epoch 94, val_loss = 0.020581\n",
      "Run 5, Epoch 95, val_loss = 0.015335\n",
      "Run 5, Epoch 96, val_loss = 0.021015\n",
      "Run 5, Epoch 97, val_loss = 0.028628\n",
      "Run 5, Epoch 98, val_loss = 0.021154\n",
      "Run 5, Epoch 99, val_loss = 0.019288\n",
      "Run 5, Epoch 100, val_loss = 0.018870\n",
      "Run 5, Epoch 101, val_loss = 0.017989\n",
      "✅ Run 5: Acc = 99.69%, MDE = 0.0109\n",
      "📁 Results saved to repeat_copy/00/csirssi_cls_results00_error2_b.csv\n",
      "📁 All errors saved to repeat_copy/00/csirssi_cls_all_errors2_00_b.csv\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "# ===== 你的模型 CSIRSSI_Classifier_BN 定義在這裡（略） =====\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "COORDINATES = {\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4), 6: (0, 3.0),\n",
    "    7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0), 22: (6.0, 5.4), 23: (6.0, 4.8),\n",
    "    24: (6.0, 4.2), 25: (6.0, 3.6), 26: (6.0, 3.0), 27: (6.0, 2.4),\n",
    "    28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8), 44: (3.0, 2.4),\n",
    "    45: (3.0, 3.0), 46: (3.0, 3.6), 47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "num_runs = 5\n",
    "epochs = 200\n",
    "patience = 20\n",
    "\n",
    "test_accs = []\n",
    "test_mdes = []\n",
    "all_run_errors = []\n",
    "\n",
    "for run in range(1, num_runs + 1):\n",
    "    print(f\"\\n=== Run {run}/{num_runs} ===\")\n",
    "\n",
    "    model = CSIRSSI_Classifier_BN(num_classes=49).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "    criterion_cls = nn.CrossEntropyLoss()\n",
    "    best_val_loss = None\n",
    "    best_state_dict = None\n",
    "    counter = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for csi_inputs, rssi_inputs, labels in train_loader:\n",
    "            csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            class_out = model(csi_inputs, rssi_inputs)\n",
    "            loss = criterion_cls(class_out, target_class)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 驗證\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for csi_inputs, rssi_inputs, labels in val_loader:\n",
    "                csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                target_class = torch.argmax(labels, dim=1)\n",
    "                class_out = model(csi_inputs, rssi_inputs)\n",
    "                val_loss += criterion_cls(class_out, target_class).item() * csi_inputs.size(0)\n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        # 印出 val_loss\n",
    "        print(f\"Run {run}, Epoch {epoch+1}, val_loss = {val_loss:.6f}\")\n",
    "\n",
    "        if (best_val_loss is None) or (val_loss < best_val_loss):\n",
    "            best_val_loss = val_loss\n",
    "            counter = 0\n",
    "            best_state_dict = copy.deepcopy(model.state_dict())\n",
    "        else:\n",
    "            counter += 1\n",
    "            if counter >= patience:\n",
    "                break\n",
    "\n",
    "    # 測試\n",
    "    if best_state_dict is None:\n",
    "        raise RuntimeError(\"No best_state_dict found. Check training loop or data.\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    model.eval()\n",
    "    all_true, all_pred = [], []\n",
    "    with torch.no_grad():\n",
    "        for csi_inputs, rssi_inputs, labels in test_loader:\n",
    "            csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "            target_class = torch.argmax(labels, dim=1)\n",
    "            class_out = model(csi_inputs, rssi_inputs)\n",
    "            pred = torch.argmax(class_out, dim=1)\n",
    "            all_pred.extend(pred.cpu().numpy())\n",
    "            all_true.extend(target_class.cpu().numpy())\n",
    "\n",
    "    y_true = np.array(all_true) + 1\n",
    "    y_pred = np.array(all_pred) + 1\n",
    "    acc = 100 * np.mean(y_true == y_pred)\n",
    "    mde, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "    test_accs.append(acc)\n",
    "    test_mdes.append(mde)\n",
    "    all_run_errors.append(errors)\n",
    "    print(f\"✅ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "# 儲存 summary 結果\n",
    "df = pd.DataFrame({\n",
    "    \"run\": list(range(1, num_runs+1)),\n",
    "    \"accuracy\": test_accs,\n",
    "    \"mde\": test_mdes\n",
    "})\n",
    "os.makedirs(\"repeat_copy/00\", exist_ok=True)\n",
    "df.to_csv(\"repeat_copy/00/csirssi_cls_results00_error2_b_2.csv\", index=False)\n",
    "print(\"📁 Results saved to repeat_copy/00/csirssi_cls_results00_error2_b.csv\")\n",
    "\n",
    "# 儲存所有 error（長條格式）\n",
    "error_records = []\n",
    "for run_idx, errors in enumerate(all_run_errors):\n",
    "    for sample_idx, e in enumerate(errors):\n",
    "        error_records.append({\n",
    "            \"run\": run_idx + 1,\n",
    "            \"sample_idx\": sample_idx + 1,\n",
    "            \"error\": e\n",
    "        })\n",
    "df_errors = pd.DataFrame(error_records)\n",
    "df_errors.to_csv(\"repeat_copy/00/csirssi_cls_all_errors2_00_b_2.csv\", index=False)\n",
    "print(\"📁 All errors saved to repeat_copy/00/csirssi_cls_all_errors2_00_b.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 存ERROR版本"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "模型摘要:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "CSIRSSI_DualHead_BN                      [1, 49]                   --\n",
       "├─Conv1d: 1-1                            [1, 64, 48]               256\n",
       "├─BatchNorm1d: 1-2                       [1, 64, 48]               128\n",
       "├─MaxPool1d: 1-3                         [1, 64, 24]               --\n",
       "├─Conv1d: 1-4                            [1, 128, 24]              24,704\n",
       "├─BatchNorm1d: 1-5                       [1, 128, 24]              256\n",
       "├─MaxPool1d: 1-6                         [1, 128, 12]              --\n",
       "├─Linear: 1-7                            [1, 128]                  196,736\n",
       "├─Dropout: 1-8                           [1, 128]                  --\n",
       "├─Linear: 1-9                            [1, 64]                   8,256\n",
       "├─Dropout: 1-10                          [1, 64]                   --\n",
       "├─Linear: 1-11                           [1, 128]                  640\n",
       "├─BatchNorm1d: 1-12                      [1, 128]                  256\n",
       "├─Dropout: 1-13                          [1, 128]                  --\n",
       "├─Linear: 1-14                           [1, 32]                   4,128\n",
       "├─BatchNorm1d: 1-15                      [1, 32]                   64\n",
       "├─Dropout: 1-16                          [1, 32]                   --\n",
       "├─Linear: 1-17                           [1, 64]                   6,208\n",
       "├─Dropout: 1-18                          [1, 64]                   --\n",
       "├─Linear: 1-19                           [1, 49]                   3,185\n",
       "├─Linear: 1-20                           [1, 2]                    130\n",
       "==========================================================================================\n",
       "Total params: 244,947\n",
       "Trainable params: 244,947\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 0.83\n",
       "==========================================================================================\n",
       "Input size (MB): 0.00\n",
       "Forward/backward pass size (MB): 0.10\n",
       "Params size (MB): 0.98\n",
       "Estimated Total Size (MB): 1.08\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchinfo import summary  # torchinfo 可用來顯示多輸入模型摘要\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------\n",
    "# 模型定義：CSIRSSI_DualHead\n",
    "# -----------------------\n",
    "class CSIRSSI_DualHead_BN(nn.Module):\n",
    "    def __init__(self, num_classes=49, rssi_dim=4):\n",
    "        super(CSIRSSI_DualHead_BN, self).__init__()\n",
    "        # ---- CSI 分支 (CNN) ---\n",
    "        # 假設輸入 CSI shape 為 (batch, 1, 48)\n",
    "        self.conv1 = nn.Conv1d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.bn1   = nn.BatchNorm1d(64)\n",
    "        self.pool1 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        self.conv2 = nn.Conv1d(in_channels=64, out_channels=128, kernel_size=3, padding=1)\n",
    "        self.bn2   = nn.BatchNorm1d(128)\n",
    "        self.pool2 = nn.MaxPool1d(kernel_size=2)\n",
    "        \n",
    "        # 輸入長度 48 → 經過兩次 pooling → 48/2=24，再 24/2=12\n",
    "        self.flatten_dim = 128 * 12\n",
    "        \n",
    "        self.fc1   = nn.Linear(self.flatten_dim, 128)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.fc2   = nn.Linear(128, 64)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        # 此時 CSI 分支輸出 64 維特徵\n",
    "        \n",
    "        # ---- RSSI 分支 (MLP) ----\n",
    "        self.fc_rssi1 = nn.Linear(rssi_dim, 128)\n",
    "        self.bn_rssi1 = nn.BatchNorm1d(128)\n",
    "        self.dropout_rssi1 = nn.Dropout(0.5)\n",
    "        self.fc_rssi2 = nn.Linear(128, 32)\n",
    "        self.bn_rssi2 = nn.BatchNorm1d(32)\n",
    "        self.dropout_rssi2 = nn.Dropout(0.5)\n",
    "        # RSSI 分支輸出 32 維特徵\n",
    "        \n",
    "        # ---- 融合層 ----\n",
    "        # 將 CSI (64-d) 與 RSSI (32-d) 連接 → 96-d\n",
    "        self.fc_fusion = nn.Linear(64+32, 64)\n",
    "        self.dropout_fusion = nn.Dropout(0.3)\n",
    "        \n",
    "        # ---- 雙輸出頭 ----\n",
    "        # 分類頭：輸出 num_classes 個類別的 logits\n",
    "        self.fc_class = nn.Linear(64, num_classes)\n",
    "        # 回歸頭：輸出 2 個數值 (X, Y)\n",
    "        self.fc_reg = nn.Linear(64, 2)\n",
    "    \n",
    "    def forward(self, csi_input, rssi_input):\n",
    "        # CSI 分支\n",
    "        # 如果輸入為 (batch, 48)，則擴展成 (batch, 1, 48)\n",
    "        if csi_input.dim() == 2:\n",
    "            csi_input = csi_input.unsqueeze(1)\n",
    "        x = F.relu(self.bn1(self.conv1(csi_input)))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.bn2(self.conv2(x)))\n",
    "        x = self.pool2(x)\n",
    "        x = x.view(x.size(0), -1)  # flatten → (batch, flatten_dim)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        csi_feat = F.relu(self.fc2(x))\n",
    "        csi_feat = self.dropout2(csi_feat)\n",
    "        \n",
    "        # RSSI 分支\n",
    "        rssi_feat = F.relu(self.bn_rssi1(self.fc_rssi1(rssi_input)))\n",
    "        rssi_feat = self.dropout_rssi1(rssi_feat)\n",
    "        rssi_feat = F.relu(self.bn_rssi2(self.fc_rssi2(rssi_feat)))\n",
    "        rssi_feat = self.dropout_rssi2(rssi_feat)\n",
    "        \n",
    "        # 融合特徵\n",
    "        fusion = torch.cat([csi_feat, rssi_feat], dim=1)  # (batch, 96)\n",
    "        fusion = F.relu(self.fc_fusion(fusion))\n",
    "        fusion = self.dropout_fusion(fusion)\n",
    "        \n",
    "        # 雙輸出頭\n",
    "        class_out = self.fc_class(fusion)  # 分類輸出 (batch, num_classes)\n",
    "        reg_out = self.fc_reg(fusion)       # 回歸輸出 (batch, 2)\n",
    "        \n",
    "        return class_out, reg_out\n",
    "\n",
    "    # def extract_embedding(self, csi_input, rssi_input):\n",
    "    #     if csi_input.dim() == 2:\n",
    "    #         csi_input = csi_input.unsqueeze(1)\n",
    "    #     x = F.relu(self.bn1(self.conv1(csi_input)))\n",
    "    #     x = self.pool1(x)\n",
    "    #     x = F.relu(self.bn2(self.conv2(x)))\n",
    "    #     x = self.pool2(x)\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "    #     x = F.relu(self.fc1(x))\n",
    "    #     csi_feat = F.relu(self.fc2(x))\n",
    "    #     rssi_feat = F.relu(self.bn_rssi1(self.fc_rssi1(rssi_input)))\n",
    "    #     rssi_feat = F.relu(self.bn_rssi2(self.fc_rssi2(rssi_feat)))\n",
    "    #     fusion = torch.cat([csi_feat, rssi_feat], dim=1)\n",
    "    #     fusion = F.relu(self.fc_fusion(fusion))\n",
    "    #     return fusion\n",
    "\n",
    "# -----------------------\n",
    "# 模型初始化與摘要\n",
    "# -----------------------\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = CSIRSSI_DualHead_BN(num_classes=49, rssi_dim=4).to(device)\n",
    "\n",
    "print(\"模型摘要:\")\n",
    "summary(model, input_data=(torch.randn(1, 1, 48).to(device), torch.randn(1, 4).to(device)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用裝置: cuda\n",
      "\n",
      "[Alpha = 0.1] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.1\n",
      "Run 1, Epoch 1, val_loss = 0.9187\n",
      "Run 1, Epoch 2, val_loss = 0.4017\n",
      "Run 1, Epoch 3, val_loss = 0.2463\n",
      "Run 1, Epoch 4, val_loss = 0.1962\n",
      "Run 1, Epoch 5, val_loss = 0.1600\n",
      "Run 1, Epoch 6, val_loss = 0.1633\n",
      "Run 1, Epoch 7, val_loss = 0.1389\n",
      "Run 1, Epoch 8, val_loss = 0.1452\n",
      "Run 1, Epoch 9, val_loss = 0.1089\n",
      "Run 1, Epoch 10, val_loss = 0.1037\n",
      "Run 1, Epoch 11, val_loss = 0.0958\n",
      "Run 1, Epoch 12, val_loss = 0.1172\n",
      "Run 1, Epoch 13, val_loss = 0.0816\n",
      "Run 1, Epoch 14, val_loss = 0.0836\n",
      "Run 1, Epoch 15, val_loss = 0.0906\n",
      "Run 1, Epoch 16, val_loss = 0.0707\n",
      "Run 1, Epoch 17, val_loss = 0.0743\n",
      "Run 1, Epoch 18, val_loss = 0.0756\n",
      "Run 1, Epoch 19, val_loss = 0.0904\n",
      "Run 1, Epoch 20, val_loss = 0.0671\n",
      "Run 1, Epoch 21, val_loss = 0.0738\n",
      "Run 1, Epoch 22, val_loss = 0.0729\n",
      "Run 1, Epoch 23, val_loss = 0.1083\n",
      "Run 1, Epoch 24, val_loss = 0.0650\n",
      "Run 1, Epoch 25, val_loss = 0.0595\n",
      "Run 1, Epoch 26, val_loss = 0.0597\n",
      "Run 1, Epoch 27, val_loss = 0.0672\n",
      "Run 1, Epoch 28, val_loss = 0.0554\n",
      "Run 1, Epoch 29, val_loss = 0.0844\n",
      "Run 1, Epoch 30, val_loss = 0.0525\n",
      "Run 1, Epoch 31, val_loss = 0.0549\n",
      "Run 1, Epoch 32, val_loss = 0.0572\n",
      "Run 1, Epoch 33, val_loss = 0.0626\n",
      "Run 1, Epoch 34, val_loss = 0.0658\n",
      "Run 1, Epoch 35, val_loss = 0.0589\n",
      "Run 1, Epoch 36, val_loss = 0.0605\n",
      "Run 1, Epoch 37, val_loss = 0.0768\n",
      "Run 1, Epoch 38, val_loss = 0.0645\n",
      "Run 1, Epoch 39, val_loss = 0.0462\n",
      "Run 1, Epoch 40, val_loss = 0.0520\n",
      "Run 1, Epoch 41, val_loss = 0.0742\n",
      "Run 1, Epoch 42, val_loss = 0.0558\n",
      "Run 1, Epoch 43, val_loss = 0.0422\n",
      "Run 1, Epoch 44, val_loss = 0.0653\n",
      "Run 1, Epoch 45, val_loss = 0.0423\n",
      "Run 1, Epoch 46, val_loss = 0.0421\n",
      "Run 1, Epoch 47, val_loss = 0.0439\n",
      "Run 1, Epoch 48, val_loss = 0.0409\n",
      "Run 1, Epoch 49, val_loss = 0.0757\n",
      "Run 1, Epoch 50, val_loss = 0.0395\n",
      "Run 1, Epoch 51, val_loss = 0.0442\n",
      "Run 1, Epoch 52, val_loss = 0.0424\n",
      "Run 1, Epoch 53, val_loss = 0.0490\n",
      "Run 1, Epoch 54, val_loss = 0.0452\n",
      "Run 1, Epoch 55, val_loss = 0.0387\n",
      "Run 1, Epoch 56, val_loss = 0.0420\n",
      "Run 1, Epoch 57, val_loss = 0.0334\n",
      "Run 1, Epoch 58, val_loss = 0.0443\n",
      "Run 1, Epoch 59, val_loss = 0.0378\n",
      "Run 1, Epoch 60, val_loss = 0.0317\n",
      "Run 1, Epoch 61, val_loss = 0.0327\n",
      "Run 1, Epoch 62, val_loss = 0.0361\n",
      "Run 1, Epoch 63, val_loss = 0.0530\n",
      "Run 1, Epoch 64, val_loss = 0.0418\n",
      "Run 1, Epoch 65, val_loss = 0.0537\n",
      "Run 1, Epoch 66, val_loss = 0.0268\n",
      "Run 1, Epoch 67, val_loss = 0.0319\n",
      "Run 1, Epoch 68, val_loss = 0.0278\n",
      "Run 1, Epoch 69, val_loss = 0.0301\n",
      "Run 1, Epoch 70, val_loss = 0.0412\n",
      "Run 1, Epoch 71, val_loss = 0.0550\n",
      "Run 1, Epoch 72, val_loss = 0.0356\n",
      "Run 1, Epoch 73, val_loss = 0.0312\n",
      "Run 1, Epoch 74, val_loss = 0.0376\n",
      "Run 1, Epoch 75, val_loss = 0.0504\n",
      "Run 1, Epoch 76, val_loss = 0.0379\n",
      "Run 1, Epoch 77, val_loss = 0.0310\n",
      "Run 1, Epoch 78, val_loss = 0.0320\n",
      "Run 1, Epoch 79, val_loss = 0.0269\n",
      "Run 1, Epoch 80, val_loss = 0.0522\n",
      "Run 1, Epoch 81, val_loss = 0.0485\n",
      "Run 1, Epoch 82, val_loss = 0.0417\n",
      "Run 1, Epoch 83, val_loss = 0.0312\n",
      "Run 1, Epoch 84, val_loss = 0.0345\n",
      "Run 1, Epoch 85, val_loss = 0.0364\n",
      "Run 1, Epoch 86, val_loss = 0.0326\n",
      "✅ Run 1: Acc = 99.64%, MDE = 0.0105\n",
      "Run 2/5 - Alpha = 0.1\n",
      "Run 2, Epoch 1, val_loss = 1.1313\n",
      "Run 2, Epoch 2, val_loss = 0.3753\n",
      "Run 2, Epoch 3, val_loss = 0.2634\n",
      "Run 2, Epoch 4, val_loss = 0.2353\n",
      "Run 2, Epoch 5, val_loss = 0.1645\n",
      "Run 2, Epoch 6, val_loss = 0.1542\n",
      "Run 2, Epoch 7, val_loss = 0.1230\n",
      "Run 2, Epoch 8, val_loss = 0.1069\n",
      "Run 2, Epoch 9, val_loss = 0.1201\n",
      "Run 2, Epoch 10, val_loss = 0.0909\n",
      "Run 2, Epoch 11, val_loss = 0.1188\n",
      "Run 2, Epoch 12, val_loss = 0.1443\n",
      "Run 2, Epoch 13, val_loss = 0.0780\n",
      "Run 2, Epoch 14, val_loss = 0.0860\n",
      "Run 2, Epoch 15, val_loss = 0.0757\n",
      "Run 2, Epoch 16, val_loss = 0.0805\n",
      "Run 2, Epoch 17, val_loss = 0.0764\n",
      "Run 2, Epoch 18, val_loss = 0.0704\n",
      "Run 2, Epoch 19, val_loss = 0.1060\n",
      "Run 2, Epoch 20, val_loss = 0.0715\n",
      "Run 2, Epoch 21, val_loss = 0.0675\n",
      "Run 2, Epoch 22, val_loss = 0.0558\n",
      "Run 2, Epoch 23, val_loss = 0.0629\n",
      "Run 2, Epoch 24, val_loss = 0.0814\n",
      "Run 2, Epoch 25, val_loss = 0.0722\n",
      "Run 2, Epoch 26, val_loss = 0.0637\n",
      "Run 2, Epoch 27, val_loss = 0.0564\n",
      "Run 2, Epoch 28, val_loss = 0.0600\n",
      "Run 2, Epoch 29, val_loss = 0.0539\n",
      "Run 2, Epoch 30, val_loss = 0.0549\n",
      "Run 2, Epoch 31, val_loss = 0.0515\n",
      "Run 2, Epoch 32, val_loss = 0.0519\n",
      "Run 2, Epoch 33, val_loss = 0.0631\n",
      "Run 2, Epoch 34, val_loss = 0.0518\n",
      "Run 2, Epoch 35, val_loss = 0.0597\n",
      "Run 2, Epoch 36, val_loss = 0.0545\n",
      "Run 2, Epoch 37, val_loss = 0.0430\n",
      "Run 2, Epoch 38, val_loss = 0.0530\n",
      "Run 2, Epoch 39, val_loss = 0.0616\n",
      "Run 2, Epoch 40, val_loss = 0.0424\n",
      "Run 2, Epoch 41, val_loss = 0.0462\n",
      "Run 2, Epoch 42, val_loss = 0.0441\n",
      "Run 2, Epoch 43, val_loss = 0.0408\n",
      "Run 2, Epoch 44, val_loss = 0.0424\n",
      "Run 2, Epoch 45, val_loss = 0.0478\n",
      "Run 2, Epoch 46, val_loss = 0.0340\n",
      "Run 2, Epoch 47, val_loss = 0.0774\n",
      "Run 2, Epoch 48, val_loss = 0.0338\n",
      "Run 2, Epoch 49, val_loss = 0.0400\n",
      "Run 2, Epoch 50, val_loss = 0.0398\n",
      "Run 2, Epoch 51, val_loss = 0.0479\n",
      "Run 2, Epoch 52, val_loss = 0.0401\n",
      "Run 2, Epoch 53, val_loss = 0.0370\n",
      "Run 2, Epoch 54, val_loss = 0.0398\n",
      "Run 2, Epoch 55, val_loss = 0.0366\n",
      "Run 2, Epoch 56, val_loss = 0.0377\n",
      "Run 2, Epoch 57, val_loss = 0.0539\n",
      "Run 2, Epoch 58, val_loss = 0.0438\n",
      "Run 2, Epoch 59, val_loss = 0.0353\n",
      "Run 2, Epoch 60, val_loss = 0.0426\n",
      "Run 2, Epoch 61, val_loss = 0.0394\n",
      "Run 2, Epoch 62, val_loss = 0.0575\n",
      "Run 2, Epoch 63, val_loss = 0.0470\n",
      "Run 2, Epoch 64, val_loss = 0.0419\n",
      "Run 2, Epoch 65, val_loss = 0.0353\n",
      "Run 2, Epoch 66, val_loss = 0.0341\n",
      "Run 2, Epoch 67, val_loss = 0.0340\n",
      "Run 2, Epoch 68, val_loss = 0.0518\n",
      "✅ Run 2: Acc = 99.80%, MDE = 0.0069\n",
      "Run 3/5 - Alpha = 0.1\n",
      "Run 3, Epoch 1, val_loss = 0.9939\n",
      "Run 3, Epoch 2, val_loss = 0.3796\n",
      "Run 3, Epoch 3, val_loss = 0.2493\n",
      "Run 3, Epoch 4, val_loss = 0.2190\n",
      "Run 3, Epoch 5, val_loss = 0.1827\n",
      "Run 3, Epoch 6, val_loss = 0.1851\n",
      "Run 3, Epoch 7, val_loss = 0.1368\n",
      "Run 3, Epoch 8, val_loss = 0.1095\n",
      "Run 3, Epoch 9, val_loss = 0.1097\n",
      "Run 3, Epoch 10, val_loss = 0.1024\n",
      "Run 3, Epoch 11, val_loss = 0.0951\n",
      "Run 3, Epoch 12, val_loss = 0.0924\n",
      "Run 3, Epoch 13, val_loss = 0.1134\n",
      "Run 3, Epoch 14, val_loss = 0.0927\n",
      "Run 3, Epoch 15, val_loss = 0.0907\n",
      "Run 3, Epoch 16, val_loss = 0.0810\n",
      "Run 3, Epoch 17, val_loss = 0.0730\n",
      "Run 3, Epoch 18, val_loss = 0.0806\n",
      "Run 3, Epoch 19, val_loss = 0.0748\n",
      "Run 3, Epoch 20, val_loss = 0.0834\n",
      "Run 3, Epoch 21, val_loss = 0.0609\n",
      "Run 3, Epoch 22, val_loss = 0.0684\n",
      "Run 3, Epoch 23, val_loss = 0.0884\n",
      "Run 3, Epoch 24, val_loss = 0.0669\n",
      "Run 3, Epoch 25, val_loss = 0.0562\n",
      "Run 3, Epoch 26, val_loss = 0.0596\n",
      "Run 3, Epoch 27, val_loss = 0.0524\n",
      "Run 3, Epoch 28, val_loss = 0.0477\n",
      "Run 3, Epoch 29, val_loss = 0.0517\n",
      "Run 3, Epoch 30, val_loss = 0.0420\n",
      "Run 3, Epoch 31, val_loss = 0.0715\n",
      "Run 3, Epoch 32, val_loss = 0.0453\n",
      "Run 3, Epoch 33, val_loss = 0.0485\n",
      "Run 3, Epoch 34, val_loss = 0.0553\n",
      "Run 3, Epoch 35, val_loss = 0.0450\n",
      "Run 3, Epoch 36, val_loss = 0.0593\n",
      "Run 3, Epoch 37, val_loss = 0.0459\n",
      "Run 3, Epoch 38, val_loss = 0.0419\n",
      "Run 3, Epoch 39, val_loss = 0.0756\n",
      "Run 3, Epoch 40, val_loss = 0.0494\n",
      "Run 3, Epoch 41, val_loss = 0.0446\n",
      "Run 3, Epoch 42, val_loss = 0.0457\n",
      "Run 3, Epoch 43, val_loss = 0.0567\n",
      "Run 3, Epoch 44, val_loss = 0.0451\n",
      "Run 3, Epoch 45, val_loss = 0.0384\n",
      "Run 3, Epoch 46, val_loss = 0.0427\n",
      "Run 3, Epoch 47, val_loss = 0.0485\n",
      "Run 3, Epoch 48, val_loss = 0.0642\n",
      "Run 3, Epoch 49, val_loss = 0.0361\n",
      "Run 3, Epoch 50, val_loss = 0.0405\n",
      "Run 3, Epoch 51, val_loss = 0.0518\n",
      "Run 3, Epoch 52, val_loss = 0.0423\n",
      "Run 3, Epoch 53, val_loss = 0.0405\n",
      "Run 3, Epoch 54, val_loss = 0.0411\n",
      "Run 3, Epoch 55, val_loss = 0.0434\n",
      "Run 3, Epoch 56, val_loss = 0.0539\n",
      "Run 3, Epoch 57, val_loss = 0.0452\n",
      "Run 3, Epoch 58, val_loss = 0.0473\n",
      "Run 3, Epoch 59, val_loss = 0.0454\n",
      "Run 3, Epoch 60, val_loss = 0.0489\n",
      "Run 3, Epoch 61, val_loss = 0.0437\n",
      "Run 3, Epoch 62, val_loss = 0.0347\n",
      "Run 3, Epoch 63, val_loss = 0.0464\n",
      "Run 3, Epoch 64, val_loss = 0.0350\n",
      "Run 3, Epoch 65, val_loss = 0.0373\n",
      "Run 3, Epoch 66, val_loss = 0.0327\n",
      "Run 3, Epoch 67, val_loss = 0.0361\n",
      "Run 3, Epoch 68, val_loss = 0.0339\n",
      "Run 3, Epoch 69, val_loss = 0.0404\n",
      "Run 3, Epoch 70, val_loss = 0.0339\n",
      "Run 3, Epoch 71, val_loss = 0.0670\n",
      "Run 3, Epoch 72, val_loss = 0.0347\n",
      "Run 3, Epoch 73, val_loss = 0.0312\n",
      "Run 3, Epoch 74, val_loss = 0.0400\n",
      "Run 3, Epoch 75, val_loss = 0.0537\n",
      "Run 3, Epoch 76, val_loss = 0.0333\n",
      "Run 3, Epoch 77, val_loss = 0.0335\n",
      "Run 3, Epoch 78, val_loss = 0.0357\n",
      "Run 3, Epoch 79, val_loss = 0.0321\n",
      "Run 3, Epoch 80, val_loss = 0.0397\n",
      "Run 3, Epoch 81, val_loss = 0.0366\n",
      "Run 3, Epoch 82, val_loss = 0.0321\n",
      "Run 3, Epoch 83, val_loss = 0.0323\n",
      "Run 3, Epoch 84, val_loss = 0.0344\n",
      "Run 3, Epoch 85, val_loss = 0.0346\n",
      "Run 3, Epoch 86, val_loss = 0.0325\n",
      "Run 3, Epoch 87, val_loss = 0.0360\n",
      "Run 3, Epoch 88, val_loss = 0.0327\n",
      "Run 3, Epoch 89, val_loss = 0.0412\n",
      "Run 3, Epoch 90, val_loss = 0.0345\n",
      "Run 3, Epoch 91, val_loss = 0.0398\n",
      "Run 3, Epoch 92, val_loss = 0.0342\n",
      "Run 3, Epoch 93, val_loss = 0.0327\n",
      "✅ Run 3: Acc = 99.85%, MDE = 0.0043\n",
      "Run 4/5 - Alpha = 0.1\n",
      "Run 4, Epoch 1, val_loss = 0.9881\n",
      "Run 4, Epoch 2, val_loss = 0.3434\n",
      "Run 4, Epoch 3, val_loss = 0.2744\n",
      "Run 4, Epoch 4, val_loss = 0.1871\n",
      "Run 4, Epoch 5, val_loss = 0.1530\n",
      "Run 4, Epoch 6, val_loss = 0.1438\n",
      "Run 4, Epoch 7, val_loss = 0.1318\n",
      "Run 4, Epoch 8, val_loss = 0.1280\n",
      "Run 4, Epoch 9, val_loss = 0.1059\n",
      "Run 4, Epoch 10, val_loss = 0.0984\n",
      "Run 4, Epoch 11, val_loss = 0.0921\n",
      "Run 4, Epoch 12, val_loss = 0.0843\n",
      "Run 4, Epoch 13, val_loss = 0.0877\n",
      "Run 4, Epoch 14, val_loss = 0.0865\n",
      "Run 4, Epoch 15, val_loss = 0.0972\n",
      "Run 4, Epoch 16, val_loss = 0.0860\n",
      "Run 4, Epoch 17, val_loss = 0.0875\n",
      "Run 4, Epoch 18, val_loss = 0.0700\n",
      "Run 4, Epoch 19, val_loss = 0.0685\n",
      "Run 4, Epoch 20, val_loss = 0.0913\n",
      "Run 4, Epoch 21, val_loss = 0.0813\n",
      "Run 4, Epoch 22, val_loss = 0.0662\n",
      "Run 4, Epoch 23, val_loss = 0.0942\n",
      "Run 4, Epoch 24, val_loss = 0.0737\n",
      "Run 4, Epoch 25, val_loss = 0.0739\n",
      "Run 4, Epoch 26, val_loss = 0.0627\n",
      "Run 4, Epoch 27, val_loss = 0.0602\n",
      "Run 4, Epoch 28, val_loss = 0.0548\n",
      "Run 4, Epoch 29, val_loss = 0.0582\n",
      "Run 4, Epoch 30, val_loss = 0.0458\n",
      "Run 4, Epoch 31, val_loss = 0.0474\n",
      "Run 4, Epoch 32, val_loss = 0.0623\n",
      "Run 4, Epoch 33, val_loss = 0.0534\n",
      "Run 4, Epoch 34, val_loss = 0.0467\n",
      "Run 4, Epoch 35, val_loss = 0.0573\n",
      "Run 4, Epoch 36, val_loss = 0.0553\n",
      "Run 4, Epoch 37, val_loss = 0.0586\n",
      "Run 4, Epoch 38, val_loss = 0.0816\n",
      "Run 4, Epoch 39, val_loss = 0.0473\n",
      "Run 4, Epoch 40, val_loss = 0.0543\n",
      "Run 4, Epoch 41, val_loss = 0.0807\n",
      "Run 4, Epoch 42, val_loss = 0.0594\n",
      "Run 4, Epoch 43, val_loss = 0.0409\n",
      "Run 4, Epoch 44, val_loss = 0.0521\n",
      "Run 4, Epoch 45, val_loss = 0.0476\n",
      "Run 4, Epoch 46, val_loss = 0.0548\n",
      "Run 4, Epoch 47, val_loss = 0.0541\n",
      "Run 4, Epoch 48, val_loss = 0.0476\n",
      "Run 4, Epoch 49, val_loss = 0.0429\n",
      "Run 4, Epoch 50, val_loss = 0.0492\n",
      "Run 4, Epoch 51, val_loss = 0.0563\n",
      "Run 4, Epoch 52, val_loss = 0.0389\n",
      "Run 4, Epoch 53, val_loss = 0.0397\n",
      "Run 4, Epoch 54, val_loss = 0.0424\n",
      "Run 4, Epoch 55, val_loss = 0.0461\n",
      "Run 4, Epoch 56, val_loss = 0.0408\n",
      "Run 4, Epoch 57, val_loss = 0.0431\n",
      "Run 4, Epoch 58, val_loss = 0.0413\n",
      "Run 4, Epoch 59, val_loss = 0.0388\n",
      "Run 4, Epoch 60, val_loss = 0.0393\n",
      "Run 4, Epoch 61, val_loss = 0.0391\n",
      "Run 4, Epoch 62, val_loss = 0.0417\n",
      "Run 4, Epoch 63, val_loss = 0.0325\n",
      "Run 4, Epoch 64, val_loss = 0.0437\n",
      "Run 4, Epoch 65, val_loss = 0.0354\n",
      "Run 4, Epoch 66, val_loss = 0.0609\n",
      "Run 4, Epoch 67, val_loss = 0.0374\n",
      "Run 4, Epoch 68, val_loss = 0.0352\n",
      "Run 4, Epoch 69, val_loss = 0.0366\n",
      "Run 4, Epoch 70, val_loss = 0.0469\n",
      "Run 4, Epoch 71, val_loss = 0.0343\n",
      "Run 4, Epoch 72, val_loss = 0.0378\n",
      "Run 4, Epoch 73, val_loss = 0.0355\n",
      "Run 4, Epoch 74, val_loss = 0.0306\n",
      "Run 4, Epoch 75, val_loss = 0.0348\n",
      "Run 4, Epoch 76, val_loss = 0.0339\n",
      "Run 4, Epoch 77, val_loss = 0.0337\n",
      "Run 4, Epoch 78, val_loss = 0.0394\n",
      "Run 4, Epoch 79, val_loss = 0.0299\n",
      "Run 4, Epoch 80, val_loss = 0.0389\n",
      "Run 4, Epoch 81, val_loss = 0.0622\n",
      "Run 4, Epoch 82, val_loss = 0.0473\n",
      "Run 4, Epoch 83, val_loss = 0.0465\n",
      "Run 4, Epoch 84, val_loss = 0.0307\n",
      "Run 4, Epoch 85, val_loss = 0.0340\n",
      "Run 4, Epoch 86, val_loss = 0.0341\n",
      "Run 4, Epoch 87, val_loss = 0.0355\n",
      "Run 4, Epoch 88, val_loss = 0.0417\n",
      "Run 4, Epoch 89, val_loss = 0.0488\n",
      "Run 4, Epoch 90, val_loss = 0.0506\n",
      "Run 4, Epoch 91, val_loss = 0.0294\n",
      "Run 4, Epoch 92, val_loss = 0.0350\n",
      "Run 4, Epoch 93, val_loss = 0.0402\n",
      "Run 4, Epoch 94, val_loss = 0.0373\n",
      "Run 4, Epoch 95, val_loss = 0.0312\n",
      "Run 4, Epoch 96, val_loss = 0.0324\n",
      "Run 4, Epoch 97, val_loss = 0.0487\n",
      "Run 4, Epoch 98, val_loss = 0.0473\n",
      "Run 4, Epoch 99, val_loss = 0.0400\n",
      "Run 4, Epoch 100, val_loss = 0.0422\n",
      "Run 4, Epoch 101, val_loss = 0.0366\n",
      "Run 4, Epoch 102, val_loss = 0.0287\n",
      "Run 4, Epoch 103, val_loss = 0.0243\n",
      "Run 4, Epoch 104, val_loss = 0.0334\n",
      "Run 4, Epoch 105, val_loss = 0.0292\n",
      "Run 4, Epoch 106, val_loss = 0.0272\n",
      "Run 4, Epoch 107, val_loss = 0.0318\n",
      "Run 4, Epoch 108, val_loss = 0.0318\n",
      "Run 4, Epoch 109, val_loss = 0.0342\n",
      "Run 4, Epoch 110, val_loss = 0.0344\n",
      "Run 4, Epoch 111, val_loss = 0.0305\n",
      "Run 4, Epoch 112, val_loss = 0.0370\n",
      "Run 4, Epoch 113, val_loss = 0.0330\n",
      "Run 4, Epoch 114, val_loss = 0.0355\n",
      "Run 4, Epoch 115, val_loss = 0.0419\n",
      "Run 4, Epoch 116, val_loss = 0.0487\n",
      "Run 4, Epoch 117, val_loss = 0.0378\n",
      "Run 4, Epoch 118, val_loss = 0.0465\n",
      "Run 4, Epoch 119, val_loss = 0.0407\n",
      "Run 4, Epoch 120, val_loss = 0.0358\n",
      "Run 4, Epoch 121, val_loss = 0.0320\n",
      "Run 4, Epoch 122, val_loss = 0.0341\n",
      "Run 4, Epoch 123, val_loss = 0.0260\n",
      "✅ Run 4: Acc = 99.59%, MDE = 0.0113\n",
      "Run 5/5 - Alpha = 0.1\n",
      "Run 5, Epoch 1, val_loss = 1.0737\n",
      "Run 5, Epoch 2, val_loss = 0.3775\n",
      "Run 5, Epoch 3, val_loss = 0.3300\n",
      "Run 5, Epoch 4, val_loss = 0.2688\n",
      "Run 5, Epoch 5, val_loss = 0.1839\n",
      "Run 5, Epoch 6, val_loss = 0.1712\n",
      "Run 5, Epoch 7, val_loss = 0.1273\n",
      "Run 5, Epoch 8, val_loss = 0.1391\n",
      "Run 5, Epoch 9, val_loss = 0.1070\n",
      "Run 5, Epoch 10, val_loss = 0.0864\n",
      "Run 5, Epoch 11, val_loss = 0.1068\n",
      "Run 5, Epoch 12, val_loss = 0.1105\n",
      "Run 5, Epoch 13, val_loss = 0.0925\n",
      "Run 5, Epoch 14, val_loss = 0.0806\n",
      "Run 5, Epoch 15, val_loss = 0.0921\n",
      "Run 5, Epoch 16, val_loss = 0.0671\n",
      "Run 5, Epoch 17, val_loss = 0.0676\n",
      "Run 5, Epoch 18, val_loss = 0.0792\n",
      "Run 5, Epoch 19, val_loss = 0.1011\n",
      "Run 5, Epoch 20, val_loss = 0.0931\n",
      "Run 5, Epoch 21, val_loss = 0.0613\n",
      "Run 5, Epoch 22, val_loss = 0.0700\n",
      "Run 5, Epoch 23, val_loss = 0.0881\n",
      "Run 5, Epoch 24, val_loss = 0.0577\n",
      "Run 5, Epoch 25, val_loss = 0.0604\n",
      "Run 5, Epoch 26, val_loss = 0.0743\n",
      "Run 5, Epoch 27, val_loss = 0.0596\n",
      "Run 5, Epoch 28, val_loss = 0.0471\n",
      "Run 5, Epoch 29, val_loss = 0.0730\n",
      "Run 5, Epoch 30, val_loss = 0.0566\n",
      "Run 5, Epoch 31, val_loss = 0.0604\n",
      "Run 5, Epoch 32, val_loss = 0.0537\n",
      "Run 5, Epoch 33, val_loss = 0.0520\n",
      "Run 5, Epoch 34, val_loss = 0.0721\n",
      "Run 5, Epoch 35, val_loss = 0.0541\n",
      "Run 5, Epoch 36, val_loss = 0.0577\n",
      "Run 5, Epoch 37, val_loss = 0.0547\n",
      "Run 5, Epoch 38, val_loss = 0.0454\n",
      "Run 5, Epoch 39, val_loss = 0.0454\n",
      "Run 5, Epoch 40, val_loss = 0.0549\n",
      "Run 5, Epoch 41, val_loss = 0.0631\n",
      "Run 5, Epoch 42, val_loss = 0.0468\n",
      "Run 5, Epoch 43, val_loss = 0.0574\n",
      "Run 5, Epoch 44, val_loss = 0.0416\n",
      "Run 5, Epoch 45, val_loss = 0.0498\n",
      "Run 5, Epoch 46, val_loss = 0.0620\n",
      "Run 5, Epoch 47, val_loss = 0.0491\n",
      "Run 5, Epoch 48, val_loss = 0.0483\n",
      "Run 5, Epoch 49, val_loss = 0.0482\n",
      "Run 5, Epoch 50, val_loss = 0.0497\n",
      "Run 5, Epoch 51, val_loss = 0.0520\n",
      "Run 5, Epoch 52, val_loss = 0.0409\n",
      "Run 5, Epoch 53, val_loss = 0.0491\n",
      "Run 5, Epoch 54, val_loss = 0.0383\n",
      "Run 5, Epoch 55, val_loss = 0.0453\n",
      "Run 5, Epoch 56, val_loss = 0.0431\n",
      "Run 5, Epoch 57, val_loss = 0.0375\n",
      "Run 5, Epoch 58, val_loss = 0.0451\n",
      "Run 5, Epoch 59, val_loss = 0.0466\n",
      "Run 5, Epoch 60, val_loss = 0.0363\n",
      "Run 5, Epoch 61, val_loss = 0.0452\n",
      "Run 5, Epoch 62, val_loss = 0.0435\n",
      "Run 5, Epoch 63, val_loss = 0.0401\n",
      "Run 5, Epoch 64, val_loss = 0.0294\n",
      "Run 5, Epoch 65, val_loss = 0.0425\n",
      "Run 5, Epoch 66, val_loss = 0.0366\n",
      "Run 5, Epoch 67, val_loss = 0.0485\n",
      "Run 5, Epoch 68, val_loss = 0.0313\n",
      "Run 5, Epoch 69, val_loss = 0.0294\n",
      "Run 5, Epoch 70, val_loss = 0.0436\n",
      "Run 5, Epoch 71, val_loss = 0.0251\n",
      "Run 5, Epoch 72, val_loss = 0.0328\n",
      "Run 5, Epoch 73, val_loss = 0.0349\n",
      "Run 5, Epoch 74, val_loss = 0.0399\n",
      "Run 5, Epoch 75, val_loss = 0.0298\n",
      "Run 5, Epoch 76, val_loss = 0.0345\n",
      "Run 5, Epoch 77, val_loss = 0.0322\n",
      "Run 5, Epoch 78, val_loss = 0.0307\n",
      "Run 5, Epoch 79, val_loss = 0.0436\n",
      "Run 5, Epoch 80, val_loss = 0.0275\n",
      "Run 5, Epoch 81, val_loss = 0.0339\n",
      "Run 5, Epoch 82, val_loss = 0.0288\n",
      "Run 5, Epoch 83, val_loss = 0.0327\n",
      "Run 5, Epoch 84, val_loss = 0.0299\n",
      "Run 5, Epoch 85, val_loss = 0.0532\n",
      "Run 5, Epoch 86, val_loss = 0.0274\n",
      "Run 5, Epoch 87, val_loss = 0.0374\n",
      "Run 5, Epoch 88, val_loss = 0.0251\n",
      "Run 5, Epoch 89, val_loss = 0.0208\n",
      "Run 5, Epoch 90, val_loss = 0.0203\n",
      "Run 5, Epoch 91, val_loss = 0.0301\n",
      "Run 5, Epoch 92, val_loss = 0.0350\n",
      "Run 5, Epoch 93, val_loss = 0.0209\n",
      "Run 5, Epoch 94, val_loss = 0.0231\n",
      "Run 5, Epoch 95, val_loss = 0.0351\n",
      "Run 5, Epoch 96, val_loss = 0.0236\n",
      "Run 5, Epoch 97, val_loss = 0.0263\n",
      "Run 5, Epoch 98, val_loss = 0.0211\n",
      "Run 5, Epoch 99, val_loss = 0.0263\n",
      "Run 5, Epoch 100, val_loss = 0.0238\n",
      "Run 5, Epoch 101, val_loss = 0.0257\n",
      "Run 5, Epoch 102, val_loss = 0.0240\n",
      "Run 5, Epoch 103, val_loss = 0.0234\n",
      "Run 5, Epoch 104, val_loss = 0.0236\n",
      "Run 5, Epoch 105, val_loss = 0.0196\n",
      "Run 5, Epoch 106, val_loss = 0.0202\n",
      "Run 5, Epoch 107, val_loss = 0.0213\n",
      "Run 5, Epoch 108, val_loss = 0.0191\n",
      "Run 5, Epoch 109, val_loss = 0.0237\n",
      "Run 5, Epoch 110, val_loss = 0.0198\n",
      "Run 5, Epoch 111, val_loss = 0.0184\n",
      "Run 5, Epoch 112, val_loss = 0.0234\n",
      "Run 5, Epoch 113, val_loss = 0.0228\n",
      "Run 5, Epoch 114, val_loss = 0.0208\n",
      "Run 5, Epoch 115, val_loss = 0.0199\n",
      "Run 5, Epoch 116, val_loss = 0.0347\n",
      "Run 5, Epoch 117, val_loss = 0.0258\n",
      "Run 5, Epoch 118, val_loss = 0.0230\n",
      "Run 5, Epoch 119, val_loss = 0.0206\n",
      "Run 5, Epoch 120, val_loss = 0.0235\n",
      "Run 5, Epoch 121, val_loss = 0.0233\n",
      "Run 5, Epoch 122, val_loss = 0.0259\n",
      "Run 5, Epoch 123, val_loss = 0.0195\n",
      "Run 5, Epoch 124, val_loss = 0.0241\n",
      "Run 5, Epoch 125, val_loss = 0.0215\n",
      "Run 5, Epoch 126, val_loss = 0.0162\n",
      "Run 5, Epoch 127, val_loss = 0.0187\n",
      "Run 5, Epoch 128, val_loss = 0.0173\n",
      "Run 5, Epoch 129, val_loss = 0.0174\n",
      "Run 5, Epoch 130, val_loss = 0.0188\n",
      "Run 5, Epoch 131, val_loss = 0.0172\n",
      "Run 5, Epoch 132, val_loss = 0.0250\n",
      "Run 5, Epoch 133, val_loss = 0.0232\n",
      "Run 5, Epoch 134, val_loss = 0.0183\n",
      "Run 5, Epoch 135, val_loss = 0.0163\n",
      "Run 5, Epoch 136, val_loss = 0.0165\n",
      "Run 5, Epoch 137, val_loss = 0.0145\n",
      "Run 5, Epoch 138, val_loss = 0.0200\n",
      "Run 5, Epoch 139, val_loss = 0.0171\n",
      "Run 5, Epoch 140, val_loss = 0.0194\n",
      "Run 5, Epoch 141, val_loss = 0.0183\n",
      "Run 5, Epoch 142, val_loss = 0.0272\n",
      "Run 5, Epoch 143, val_loss = 0.0231\n",
      "Run 5, Epoch 144, val_loss = 0.0249\n",
      "Run 5, Epoch 145, val_loss = 0.0307\n",
      "Run 5, Epoch 146, val_loss = 0.0293\n",
      "Run 5, Epoch 147, val_loss = 0.0228\n",
      "Run 5, Epoch 148, val_loss = 0.0228\n",
      "Run 5, Epoch 149, val_loss = 0.0190\n",
      "Run 5, Epoch 150, val_loss = 0.0213\n",
      "Run 5, Epoch 151, val_loss = 0.0250\n",
      "Run 5, Epoch 152, val_loss = 0.0194\n",
      "Run 5, Epoch 153, val_loss = 0.0204\n",
      "Run 5, Epoch 154, val_loss = 0.0177\n",
      "Run 5, Epoch 155, val_loss = 0.0172\n",
      "Run 5, Epoch 156, val_loss = 0.0157\n",
      "Run 5, Epoch 157, val_loss = 0.0222\n",
      "✅ Run 5: Acc = 99.75%, MDE = 0.0076\n",
      "📁 Results & all errors saved to repeat_copy/01\n",
      "\n",
      "[Alpha = 0.2] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.2\n",
      "Run 1, Epoch 1, val_loss = 1.3935\n",
      "Run 1, Epoch 2, val_loss = 0.6607\n",
      "Run 1, Epoch 3, val_loss = 0.4527\n",
      "Run 1, Epoch 4, val_loss = 0.3293\n",
      "Run 1, Epoch 5, val_loss = 0.3129\n",
      "Run 1, Epoch 6, val_loss = 0.2323\n",
      "Run 1, Epoch 7, val_loss = 0.2070\n",
      "Run 1, Epoch 8, val_loss = 0.1801\n",
      "Run 1, Epoch 9, val_loss = 0.1794\n",
      "Run 1, Epoch 10, val_loss = 0.1605\n",
      "Run 1, Epoch 11, val_loss = 0.1489\n",
      "Run 1, Epoch 12, val_loss = 0.1601\n",
      "Run 1, Epoch 13, val_loss = 0.1353\n",
      "Run 1, Epoch 14, val_loss = 0.1448\n",
      "Run 1, Epoch 15, val_loss = 0.1358\n",
      "Run 1, Epoch 16, val_loss = 0.1290\n",
      "Run 1, Epoch 17, val_loss = 0.1069\n",
      "Run 1, Epoch 18, val_loss = 0.1140\n",
      "Run 1, Epoch 19, val_loss = 0.1065\n",
      "Run 1, Epoch 20, val_loss = 0.1441\n",
      "Run 1, Epoch 21, val_loss = 0.1044\n",
      "Run 1, Epoch 22, val_loss = 0.1215\n",
      "Run 1, Epoch 23, val_loss = 0.1121\n",
      "Run 1, Epoch 24, val_loss = 0.0988\n",
      "Run 1, Epoch 25, val_loss = 0.0799\n",
      "Run 1, Epoch 26, val_loss = 0.0837\n",
      "Run 1, Epoch 27, val_loss = 0.0997\n",
      "Run 1, Epoch 28, val_loss = 0.1012\n",
      "Run 1, Epoch 29, val_loss = 0.0852\n",
      "Run 1, Epoch 30, val_loss = 0.1132\n",
      "Run 1, Epoch 31, val_loss = 0.1006\n",
      "Run 1, Epoch 32, val_loss = 0.1095\n",
      "Run 1, Epoch 33, val_loss = 0.0797\n",
      "Run 1, Epoch 34, val_loss = 0.1130\n",
      "Run 1, Epoch 35, val_loss = 0.0770\n",
      "Run 1, Epoch 36, val_loss = 0.0865\n",
      "Run 1, Epoch 37, val_loss = 0.0721\n",
      "Run 1, Epoch 38, val_loss = 0.0653\n",
      "Run 1, Epoch 39, val_loss = 0.0558\n",
      "Run 1, Epoch 40, val_loss = 0.0588\n",
      "Run 1, Epoch 41, val_loss = 0.0757\n",
      "Run 1, Epoch 42, val_loss = 0.0767\n",
      "Run 1, Epoch 43, val_loss = 0.0674\n",
      "Run 1, Epoch 44, val_loss = 0.0611\n",
      "Run 1, Epoch 45, val_loss = 0.0795\n",
      "Run 1, Epoch 46, val_loss = 0.0752\n",
      "Run 1, Epoch 47, val_loss = 0.0770\n",
      "Run 1, Epoch 48, val_loss = 0.0697\n",
      "Run 1, Epoch 49, val_loss = 0.0614\n",
      "Run 1, Epoch 50, val_loss = 0.0616\n",
      "Run 1, Epoch 51, val_loss = 0.0671\n",
      "Run 1, Epoch 52, val_loss = 0.0756\n",
      "Run 1, Epoch 53, val_loss = 0.0502\n",
      "Run 1, Epoch 54, val_loss = 0.0555\n",
      "Run 1, Epoch 55, val_loss = 0.0579\n",
      "Run 1, Epoch 56, val_loss = 0.0488\n",
      "Run 1, Epoch 57, val_loss = 0.0650\n",
      "Run 1, Epoch 58, val_loss = 0.0567\n",
      "Run 1, Epoch 59, val_loss = 0.0634\n",
      "Run 1, Epoch 60, val_loss = 0.0508\n",
      "Run 1, Epoch 61, val_loss = 0.0472\n",
      "Run 1, Epoch 62, val_loss = 0.0610\n",
      "Run 1, Epoch 63, val_loss = 0.0480\n",
      "Run 1, Epoch 64, val_loss = 0.0413\n",
      "Run 1, Epoch 65, val_loss = 0.0598\n",
      "Run 1, Epoch 66, val_loss = 0.0620\n",
      "Run 1, Epoch 67, val_loss = 0.0648\n",
      "Run 1, Epoch 68, val_loss = 0.0459\n",
      "Run 1, Epoch 69, val_loss = 0.0491\n",
      "Run 1, Epoch 70, val_loss = 0.0551\n",
      "Run 1, Epoch 71, val_loss = 0.0486\n",
      "Run 1, Epoch 72, val_loss = 0.0600\n",
      "Run 1, Epoch 73, val_loss = 0.0573\n",
      "Run 1, Epoch 74, val_loss = 0.0513\n",
      "Run 1, Epoch 75, val_loss = 0.0419\n",
      "Run 1, Epoch 76, val_loss = 0.0443\n",
      "Run 1, Epoch 77, val_loss = 0.0502\n",
      "Run 1, Epoch 78, val_loss = 0.0435\n",
      "Run 1, Epoch 79, val_loss = 0.0427\n",
      "Run 1, Epoch 80, val_loss = 0.0394\n",
      "Run 1, Epoch 81, val_loss = 0.0656\n",
      "Run 1, Epoch 82, val_loss = 0.0527\n",
      "Run 1, Epoch 83, val_loss = 0.0462\n",
      "Run 1, Epoch 84, val_loss = 0.0450\n",
      "Run 1, Epoch 85, val_loss = 0.0466\n",
      "Run 1, Epoch 86, val_loss = 0.0531\n",
      "Run 1, Epoch 87, val_loss = 0.0484\n",
      "Run 1, Epoch 88, val_loss = 0.0601\n",
      "Run 1, Epoch 89, val_loss = 0.0491\n",
      "Run 1, Epoch 90, val_loss = 0.0538\n",
      "Run 1, Epoch 91, val_loss = 0.0450\n",
      "Run 1, Epoch 92, val_loss = 0.0530\n",
      "Run 1, Epoch 93, val_loss = 0.0393\n",
      "Run 1, Epoch 94, val_loss = 0.0508\n",
      "Run 1, Epoch 95, val_loss = 0.0454\n",
      "Run 1, Epoch 96, val_loss = 0.0458\n",
      "Run 1, Epoch 97, val_loss = 0.0421\n",
      "Run 1, Epoch 98, val_loss = 0.0471\n",
      "Run 1, Epoch 99, val_loss = 0.0347\n",
      "Run 1, Epoch 100, val_loss = 0.0401\n",
      "Run 1, Epoch 101, val_loss = 0.0422\n",
      "Run 1, Epoch 102, val_loss = 0.0564\n",
      "Run 1, Epoch 103, val_loss = 0.0511\n",
      "Run 1, Epoch 104, val_loss = 0.0360\n",
      "Run 1, Epoch 105, val_loss = 0.0445\n",
      "Run 1, Epoch 106, val_loss = 0.0398\n",
      "Run 1, Epoch 107, val_loss = 0.0394\n",
      "Run 1, Epoch 108, val_loss = 0.0356\n",
      "Run 1, Epoch 109, val_loss = 0.0395\n",
      "Run 1, Epoch 110, val_loss = 0.0385\n",
      "Run 1, Epoch 111, val_loss = 0.0438\n",
      "Run 1, Epoch 112, val_loss = 0.0337\n",
      "Run 1, Epoch 113, val_loss = 0.0381\n",
      "Run 1, Epoch 114, val_loss = 0.0372\n",
      "Run 1, Epoch 115, val_loss = 0.0356\n",
      "Run 1, Epoch 116, val_loss = 0.0345\n",
      "Run 1, Epoch 117, val_loss = 0.0480\n",
      "Run 1, Epoch 118, val_loss = 0.0389\n",
      "Run 1, Epoch 119, val_loss = 0.0540\n",
      "Run 1, Epoch 120, val_loss = 0.0458\n",
      "Run 1, Epoch 121, val_loss = 0.0421\n",
      "Run 1, Epoch 122, val_loss = 0.0344\n",
      "Run 1, Epoch 123, val_loss = 0.0347\n",
      "Run 1, Epoch 124, val_loss = 0.0377\n",
      "Run 1, Epoch 125, val_loss = 0.0451\n",
      "Run 1, Epoch 126, val_loss = 0.0357\n",
      "Run 1, Epoch 127, val_loss = 0.0427\n",
      "Run 1, Epoch 128, val_loss = 0.0542\n",
      "Run 1, Epoch 129, val_loss = 0.0435\n",
      "Run 1, Epoch 130, val_loss = 0.0301\n",
      "Run 1, Epoch 131, val_loss = 0.0300\n",
      "Run 1, Epoch 132, val_loss = 0.0281\n",
      "Run 1, Epoch 133, val_loss = 0.0285\n",
      "Run 1, Epoch 134, val_loss = 0.0326\n",
      "Run 1, Epoch 135, val_loss = 0.0256\n",
      "Run 1, Epoch 136, val_loss = 0.0273\n",
      "Run 1, Epoch 137, val_loss = 0.0266\n",
      "Run 1, Epoch 138, val_loss = 0.0275\n",
      "Run 1, Epoch 139, val_loss = 0.0332\n",
      "Run 1, Epoch 140, val_loss = 0.0288\n",
      "Run 1, Epoch 141, val_loss = 0.0249\n",
      "Run 1, Epoch 142, val_loss = 0.0302\n",
      "Run 1, Epoch 143, val_loss = 0.0281\n",
      "Run 1, Epoch 144, val_loss = 0.0323\n",
      "Run 1, Epoch 145, val_loss = 0.0285\n",
      "Run 1, Epoch 146, val_loss = 0.0298\n",
      "Run 1, Epoch 147, val_loss = 0.0386\n",
      "Run 1, Epoch 148, val_loss = 0.0281\n",
      "Run 1, Epoch 149, val_loss = 0.0421\n",
      "Run 1, Epoch 150, val_loss = 0.0283\n",
      "Run 1, Epoch 151, val_loss = 0.0292\n",
      "Run 1, Epoch 152, val_loss = 0.0312\n",
      "Run 1, Epoch 153, val_loss = 0.0362\n",
      "Run 1, Epoch 154, val_loss = 0.0548\n",
      "Run 1, Epoch 155, val_loss = 0.0364\n",
      "Run 1, Epoch 156, val_loss = 0.0311\n",
      "Run 1, Epoch 157, val_loss = 0.0327\n",
      "Run 1, Epoch 158, val_loss = 0.0306\n",
      "Run 1, Epoch 159, val_loss = 0.0302\n",
      "Run 1, Epoch 160, val_loss = 0.0384\n",
      "Run 1, Epoch 161, val_loss = 0.0257\n",
      "✅ Run 1: Acc = 99.69%, MDE = 0.0083\n",
      "Run 2/5 - Alpha = 0.2\n",
      "Run 2, Epoch 1, val_loss = 1.3891\n",
      "Run 2, Epoch 2, val_loss = 0.5828\n",
      "Run 2, Epoch 3, val_loss = 0.3528\n",
      "Run 2, Epoch 4, val_loss = 0.2862\n",
      "Run 2, Epoch 5, val_loss = 0.2491\n",
      "Run 2, Epoch 6, val_loss = 0.2621\n",
      "Run 2, Epoch 7, val_loss = 0.1995\n",
      "Run 2, Epoch 8, val_loss = 0.1688\n",
      "Run 2, Epoch 9, val_loss = 0.1996\n",
      "Run 2, Epoch 10, val_loss = 0.1583\n",
      "Run 2, Epoch 11, val_loss = 0.1296\n",
      "Run 2, Epoch 12, val_loss = 0.1100\n",
      "Run 2, Epoch 13, val_loss = 0.1443\n",
      "Run 2, Epoch 14, val_loss = 0.0980\n",
      "Run 2, Epoch 15, val_loss = 0.1029\n",
      "Run 2, Epoch 16, val_loss = 0.1168\n",
      "Run 2, Epoch 17, val_loss = 0.0948\n",
      "Run 2, Epoch 18, val_loss = 0.1062\n",
      "Run 2, Epoch 19, val_loss = 0.0977\n",
      "Run 2, Epoch 20, val_loss = 0.0809\n",
      "Run 2, Epoch 21, val_loss = 0.0892\n",
      "Run 2, Epoch 22, val_loss = 0.1172\n",
      "Run 2, Epoch 23, val_loss = 0.0866\n",
      "Run 2, Epoch 24, val_loss = 0.1016\n",
      "Run 2, Epoch 25, val_loss = 0.0738\n",
      "Run 2, Epoch 26, val_loss = 0.0650\n",
      "Run 2, Epoch 27, val_loss = 0.0767\n",
      "Run 2, Epoch 28, val_loss = 0.0951\n",
      "Run 2, Epoch 29, val_loss = 0.0666\n",
      "Run 2, Epoch 30, val_loss = 0.0759\n",
      "Run 2, Epoch 31, val_loss = 0.0788\n",
      "Run 2, Epoch 32, val_loss = 0.0800\n",
      "Run 2, Epoch 33, val_loss = 0.0759\n",
      "Run 2, Epoch 34, val_loss = 0.0687\n",
      "Run 2, Epoch 35, val_loss = 0.0592\n",
      "Run 2, Epoch 36, val_loss = 0.0946\n",
      "Run 2, Epoch 37, val_loss = 0.0649\n",
      "Run 2, Epoch 38, val_loss = 0.0812\n",
      "Run 2, Epoch 39, val_loss = 0.0558\n",
      "Run 2, Epoch 40, val_loss = 0.0501\n",
      "Run 2, Epoch 41, val_loss = 0.0572\n",
      "Run 2, Epoch 42, val_loss = 0.0766\n",
      "Run 2, Epoch 43, val_loss = 0.0930\n",
      "Run 2, Epoch 44, val_loss = 0.0526\n",
      "Run 2, Epoch 45, val_loss = 0.0705\n",
      "Run 2, Epoch 46, val_loss = 0.0622\n",
      "Run 2, Epoch 47, val_loss = 0.0672\n",
      "Run 2, Epoch 48, val_loss = 0.0724\n",
      "Run 2, Epoch 49, val_loss = 0.0634\n",
      "Run 2, Epoch 50, val_loss = 0.0608\n",
      "Run 2, Epoch 51, val_loss = 0.0623\n",
      "Run 2, Epoch 52, val_loss = 0.0615\n",
      "Run 2, Epoch 53, val_loss = 0.0725\n",
      "Run 2, Epoch 54, val_loss = 0.0810\n",
      "Run 2, Epoch 55, val_loss = 0.0605\n",
      "Run 2, Epoch 56, val_loss = 0.0803\n",
      "Run 2, Epoch 57, val_loss = 0.0495\n",
      "Run 2, Epoch 58, val_loss = 0.0448\n",
      "Run 2, Epoch 59, val_loss = 0.0468\n",
      "Run 2, Epoch 60, val_loss = 0.0460\n",
      "Run 2, Epoch 61, val_loss = 0.0459\n",
      "Run 2, Epoch 62, val_loss = 0.0516\n",
      "Run 2, Epoch 63, val_loss = 0.0434\n",
      "Run 2, Epoch 64, val_loss = 0.0404\n",
      "Run 2, Epoch 65, val_loss = 0.0531\n",
      "Run 2, Epoch 66, val_loss = 0.0429\n",
      "Run 2, Epoch 67, val_loss = 0.0475\n",
      "Run 2, Epoch 68, val_loss = 0.0500\n",
      "Run 2, Epoch 69, val_loss = 0.0458\n",
      "Run 2, Epoch 70, val_loss = 0.0484\n",
      "Run 2, Epoch 71, val_loss = 0.0464\n",
      "Run 2, Epoch 72, val_loss = 0.0406\n",
      "Run 2, Epoch 73, val_loss = 0.0480\n",
      "Run 2, Epoch 74, val_loss = 0.0384\n",
      "Run 2, Epoch 75, val_loss = 0.0383\n",
      "Run 2, Epoch 76, val_loss = 0.0358\n",
      "Run 2, Epoch 77, val_loss = 0.0407\n",
      "Run 2, Epoch 78, val_loss = 0.0466\n",
      "Run 2, Epoch 79, val_loss = 0.0430\n",
      "Run 2, Epoch 80, val_loss = 0.0440\n",
      "Run 2, Epoch 81, val_loss = 0.0553\n",
      "Run 2, Epoch 82, val_loss = 0.0468\n",
      "Run 2, Epoch 83, val_loss = 0.0405\n",
      "Run 2, Epoch 84, val_loss = 0.0433\n",
      "Run 2, Epoch 85, val_loss = 0.0529\n",
      "Run 2, Epoch 86, val_loss = 0.0400\n",
      "Run 2, Epoch 87, val_loss = 0.0447\n",
      "Run 2, Epoch 88, val_loss = 0.0490\n",
      "Run 2, Epoch 89, val_loss = 0.0395\n",
      "Run 2, Epoch 90, val_loss = 0.0510\n",
      "Run 2, Epoch 91, val_loss = 0.0383\n",
      "Run 2, Epoch 92, val_loss = 0.0459\n",
      "Run 2, Epoch 93, val_loss = 0.0386\n",
      "Run 2, Epoch 94, val_loss = 0.0344\n",
      "Run 2, Epoch 95, val_loss = 0.0447\n",
      "Run 2, Epoch 96, val_loss = 0.0411\n",
      "Run 2, Epoch 97, val_loss = 0.0436\n",
      "Run 2, Epoch 98, val_loss = 0.0444\n",
      "Run 2, Epoch 99, val_loss = 0.0517\n",
      "Run 2, Epoch 100, val_loss = 0.0380\n",
      "Run 2, Epoch 101, val_loss = 0.0642\n",
      "Run 2, Epoch 102, val_loss = 0.0455\n",
      "Run 2, Epoch 103, val_loss = 0.0356\n",
      "Run 2, Epoch 104, val_loss = 0.0441\n",
      "Run 2, Epoch 105, val_loss = 0.0403\n",
      "Run 2, Epoch 106, val_loss = 0.0379\n",
      "Run 2, Epoch 107, val_loss = 0.0363\n",
      "Run 2, Epoch 108, val_loss = 0.0372\n",
      "Run 2, Epoch 109, val_loss = 0.0323\n",
      "Run 2, Epoch 110, val_loss = 0.0351\n",
      "Run 2, Epoch 111, val_loss = 0.0319\n",
      "Run 2, Epoch 112, val_loss = 0.0360\n",
      "Run 2, Epoch 113, val_loss = 0.0373\n",
      "Run 2, Epoch 114, val_loss = 0.0368\n",
      "Run 2, Epoch 115, val_loss = 0.0326\n",
      "Run 2, Epoch 116, val_loss = 0.0336\n",
      "Run 2, Epoch 117, val_loss = 0.0362\n",
      "Run 2, Epoch 118, val_loss = 0.0314\n",
      "Run 2, Epoch 119, val_loss = 0.0308\n",
      "Run 2, Epoch 120, val_loss = 0.0320\n",
      "Run 2, Epoch 121, val_loss = 0.0330\n",
      "Run 2, Epoch 122, val_loss = 0.0352\n",
      "Run 2, Epoch 123, val_loss = 0.0266\n",
      "Run 2, Epoch 124, val_loss = 0.0352\n",
      "Run 2, Epoch 125, val_loss = 0.0430\n",
      "Run 2, Epoch 126, val_loss = 0.0303\n",
      "Run 2, Epoch 127, val_loss = 0.0309\n",
      "Run 2, Epoch 128, val_loss = 0.0333\n",
      "Run 2, Epoch 129, val_loss = 0.0305\n",
      "Run 2, Epoch 130, val_loss = 0.0394\n",
      "Run 2, Epoch 131, val_loss = 0.0373\n",
      "Run 2, Epoch 132, val_loss = 0.0317\n",
      "Run 2, Epoch 133, val_loss = 0.0319\n",
      "Run 2, Epoch 134, val_loss = 0.0282\n",
      "Run 2, Epoch 135, val_loss = 0.0296\n",
      "Run 2, Epoch 136, val_loss = 0.0317\n",
      "Run 2, Epoch 137, val_loss = 0.0307\n",
      "Run 2, Epoch 138, val_loss = 0.0277\n",
      "Run 2, Epoch 139, val_loss = 0.0321\n",
      "Run 2, Epoch 140, val_loss = 0.0301\n",
      "Run 2, Epoch 141, val_loss = 0.0291\n",
      "Run 2, Epoch 142, val_loss = 0.0323\n",
      "Run 2, Epoch 143, val_loss = 0.0313\n",
      "✅ Run 2: Acc = 99.69%, MDE = 0.0102\n",
      "Run 3/5 - Alpha = 0.2\n",
      "Run 3, Epoch 1, val_loss = 1.4196\n",
      "Run 3, Epoch 2, val_loss = 0.6000\n",
      "Run 3, Epoch 3, val_loss = 0.3535\n",
      "Run 3, Epoch 4, val_loss = 0.3081\n",
      "Run 3, Epoch 5, val_loss = 0.2443\n",
      "Run 3, Epoch 6, val_loss = 0.2362\n",
      "Run 3, Epoch 7, val_loss = 0.2425\n",
      "Run 3, Epoch 8, val_loss = 0.1778\n",
      "Run 3, Epoch 9, val_loss = 0.1681\n",
      "Run 3, Epoch 10, val_loss = 0.1203\n",
      "Run 3, Epoch 11, val_loss = 0.1559\n",
      "Run 3, Epoch 12, val_loss = 0.1270\n",
      "Run 3, Epoch 13, val_loss = 0.1144\n",
      "Run 3, Epoch 14, val_loss = 0.1134\n",
      "Run 3, Epoch 15, val_loss = 0.1247\n",
      "Run 3, Epoch 16, val_loss = 0.1233\n",
      "Run 3, Epoch 17, val_loss = 0.0982\n",
      "Run 3, Epoch 18, val_loss = 0.0878\n",
      "Run 3, Epoch 19, val_loss = 0.0952\n",
      "Run 3, Epoch 20, val_loss = 0.1008\n",
      "Run 3, Epoch 21, val_loss = 0.0961\n",
      "Run 3, Epoch 22, val_loss = 0.0749\n",
      "Run 3, Epoch 23, val_loss = 0.0733\n",
      "Run 3, Epoch 24, val_loss = 0.0973\n",
      "Run 3, Epoch 25, val_loss = 0.0884\n",
      "Run 3, Epoch 26, val_loss = 0.0878\n",
      "Run 3, Epoch 27, val_loss = 0.0920\n",
      "Run 3, Epoch 28, val_loss = 0.0833\n",
      "Run 3, Epoch 29, val_loss = 0.0762\n",
      "Run 3, Epoch 30, val_loss = 0.0582\n",
      "Run 3, Epoch 31, val_loss = 0.0658\n",
      "Run 3, Epoch 32, val_loss = 0.0623\n",
      "Run 3, Epoch 33, val_loss = 0.0667\n",
      "Run 3, Epoch 34, val_loss = 0.0660\n",
      "Run 3, Epoch 35, val_loss = 0.0580\n",
      "Run 3, Epoch 36, val_loss = 0.0570\n",
      "Run 3, Epoch 37, val_loss = 0.0890\n",
      "Run 3, Epoch 38, val_loss = 0.0694\n",
      "Run 3, Epoch 39, val_loss = 0.0600\n",
      "Run 3, Epoch 40, val_loss = 0.0605\n",
      "Run 3, Epoch 41, val_loss = 0.0625\n",
      "Run 3, Epoch 42, val_loss = 0.0535\n",
      "Run 3, Epoch 43, val_loss = 0.0612\n",
      "Run 3, Epoch 44, val_loss = 0.0734\n",
      "Run 3, Epoch 45, val_loss = 0.0769\n",
      "Run 3, Epoch 46, val_loss = 0.0791\n",
      "Run 3, Epoch 47, val_loss = 0.0664\n",
      "Run 3, Epoch 48, val_loss = 0.0539\n",
      "Run 3, Epoch 49, val_loss = 0.0547\n",
      "Run 3, Epoch 50, val_loss = 0.0698\n",
      "Run 3, Epoch 51, val_loss = 0.0515\n",
      "Run 3, Epoch 52, val_loss = 0.0630\n",
      "Run 3, Epoch 53, val_loss = 0.0503\n",
      "Run 3, Epoch 54, val_loss = 0.0660\n",
      "Run 3, Epoch 55, val_loss = 0.0479\n",
      "Run 3, Epoch 56, val_loss = 0.0451\n",
      "Run 3, Epoch 57, val_loss = 0.0648\n",
      "Run 3, Epoch 58, val_loss = 0.0438\n",
      "Run 3, Epoch 59, val_loss = 0.0632\n",
      "Run 3, Epoch 60, val_loss = 0.0554\n",
      "Run 3, Epoch 61, val_loss = 0.0462\n",
      "Run 3, Epoch 62, val_loss = 0.0478\n",
      "Run 3, Epoch 63, val_loss = 0.0549\n",
      "Run 3, Epoch 64, val_loss = 0.0605\n",
      "Run 3, Epoch 65, val_loss = 0.0477\n",
      "Run 3, Epoch 66, val_loss = 0.0468\n",
      "Run 3, Epoch 67, val_loss = 0.0464\n",
      "Run 3, Epoch 68, val_loss = 0.0669\n",
      "Run 3, Epoch 69, val_loss = 0.0368\n",
      "Run 3, Epoch 70, val_loss = 0.0404\n",
      "Run 3, Epoch 71, val_loss = 0.0383\n",
      "Run 3, Epoch 72, val_loss = 0.0552\n",
      "Run 3, Epoch 73, val_loss = 0.0498\n",
      "Run 3, Epoch 74, val_loss = 0.1205\n",
      "Run 3, Epoch 75, val_loss = 0.0430\n",
      "Run 3, Epoch 76, val_loss = 0.0503\n",
      "Run 3, Epoch 77, val_loss = 0.0484\n",
      "Run 3, Epoch 78, val_loss = 0.0651\n",
      "Run 3, Epoch 79, val_loss = 0.0399\n",
      "Run 3, Epoch 80, val_loss = 0.0529\n",
      "Run 3, Epoch 81, val_loss = 0.0506\n",
      "Run 3, Epoch 82, val_loss = 0.0375\n",
      "Run 3, Epoch 83, val_loss = 0.0381\n",
      "Run 3, Epoch 84, val_loss = 0.0422\n",
      "Run 3, Epoch 85, val_loss = 0.0403\n",
      "Run 3, Epoch 86, val_loss = 0.0358\n",
      "Run 3, Epoch 87, val_loss = 0.0366\n",
      "Run 3, Epoch 88, val_loss = 0.0331\n",
      "Run 3, Epoch 89, val_loss = 0.0381\n",
      "Run 3, Epoch 90, val_loss = 0.0369\n",
      "Run 3, Epoch 91, val_loss = 0.0329\n",
      "Run 3, Epoch 92, val_loss = 0.0346\n",
      "Run 3, Epoch 93, val_loss = 0.0461\n",
      "Run 3, Epoch 94, val_loss = 0.0399\n",
      "Run 3, Epoch 95, val_loss = 0.0443\n",
      "Run 3, Epoch 96, val_loss = 0.0399\n",
      "Run 3, Epoch 97, val_loss = 0.0415\n",
      "Run 3, Epoch 98, val_loss = 0.0401\n",
      "Run 3, Epoch 99, val_loss = 0.0537\n",
      "Run 3, Epoch 100, val_loss = 0.0422\n",
      "Run 3, Epoch 101, val_loss = 0.0372\n",
      "Run 3, Epoch 102, val_loss = 0.0352\n",
      "Run 3, Epoch 103, val_loss = 0.0344\n",
      "Run 3, Epoch 104, val_loss = 0.0348\n",
      "Run 3, Epoch 105, val_loss = 0.0317\n",
      "Run 3, Epoch 106, val_loss = 0.0296\n",
      "Run 3, Epoch 107, val_loss = 0.0348\n",
      "Run 3, Epoch 108, val_loss = 0.0395\n",
      "Run 3, Epoch 109, val_loss = 0.0381\n",
      "Run 3, Epoch 110, val_loss = 0.0317\n",
      "Run 3, Epoch 111, val_loss = 0.0325\n",
      "Run 3, Epoch 112, val_loss = 0.0375\n",
      "Run 3, Epoch 113, val_loss = 0.0339\n",
      "Run 3, Epoch 114, val_loss = 0.0317\n",
      "Run 3, Epoch 115, val_loss = 0.0272\n",
      "Run 3, Epoch 116, val_loss = 0.0302\n",
      "Run 3, Epoch 117, val_loss = 0.0343\n",
      "Run 3, Epoch 118, val_loss = 0.0343\n",
      "Run 3, Epoch 119, val_loss = 0.0432\n",
      "Run 3, Epoch 120, val_loss = 0.0295\n",
      "Run 3, Epoch 121, val_loss = 0.0295\n",
      "Run 3, Epoch 122, val_loss = 0.0300\n",
      "Run 3, Epoch 123, val_loss = 0.0318\n",
      "Run 3, Epoch 124, val_loss = 0.0382\n",
      "Run 3, Epoch 125, val_loss = 0.0351\n",
      "Run 3, Epoch 126, val_loss = 0.0330\n",
      "Run 3, Epoch 127, val_loss = 0.0344\n",
      "Run 3, Epoch 128, val_loss = 0.0304\n",
      "Run 3, Epoch 129, val_loss = 0.0286\n",
      "Run 3, Epoch 130, val_loss = 0.0312\n",
      "Run 3, Epoch 131, val_loss = 0.0303\n",
      "Run 3, Epoch 132, val_loss = 0.0275\n",
      "Run 3, Epoch 133, val_loss = 0.0304\n",
      "Run 3, Epoch 134, val_loss = 0.0278\n",
      "Run 3, Epoch 135, val_loss = 0.0312\n",
      "✅ Run 3: Acc = 99.69%, MDE = 0.0082\n",
      "Run 4/5 - Alpha = 0.2\n",
      "Run 4, Epoch 1, val_loss = 1.2891\n",
      "Run 4, Epoch 2, val_loss = 0.5810\n",
      "Run 4, Epoch 3, val_loss = 0.3657\n",
      "Run 4, Epoch 4, val_loss = 0.3477\n",
      "Run 4, Epoch 5, val_loss = 0.2468\n",
      "Run 4, Epoch 6, val_loss = 0.2546\n",
      "Run 4, Epoch 7, val_loss = 0.2442\n",
      "Run 4, Epoch 8, val_loss = 0.2037\n",
      "Run 4, Epoch 9, val_loss = 0.1722\n",
      "Run 4, Epoch 10, val_loss = 0.1914\n",
      "Run 4, Epoch 11, val_loss = 0.1383\n",
      "Run 4, Epoch 12, val_loss = 0.1498\n",
      "Run 4, Epoch 13, val_loss = 0.1966\n",
      "Run 4, Epoch 14, val_loss = 0.1217\n",
      "Run 4, Epoch 15, val_loss = 0.1174\n",
      "Run 4, Epoch 16, val_loss = 0.1191\n",
      "Run 4, Epoch 17, val_loss = 0.1324\n",
      "Run 4, Epoch 18, val_loss = 0.1208\n",
      "Run 4, Epoch 19, val_loss = 0.0895\n",
      "Run 4, Epoch 20, val_loss = 0.0761\n",
      "Run 4, Epoch 21, val_loss = 0.0907\n",
      "Run 4, Epoch 22, val_loss = 0.0829\n",
      "Run 4, Epoch 23, val_loss = 0.0854\n",
      "Run 4, Epoch 24, val_loss = 0.0844\n",
      "Run 4, Epoch 25, val_loss = 0.0933\n",
      "Run 4, Epoch 26, val_loss = 0.0941\n",
      "Run 4, Epoch 27, val_loss = 0.1005\n",
      "Run 4, Epoch 28, val_loss = 0.0742\n",
      "Run 4, Epoch 29, val_loss = 0.0851\n",
      "Run 4, Epoch 30, val_loss = 0.0797\n",
      "Run 4, Epoch 31, val_loss = 0.0767\n",
      "Run 4, Epoch 32, val_loss = 0.0817\n",
      "Run 4, Epoch 33, val_loss = 0.0683\n",
      "Run 4, Epoch 34, val_loss = 0.0712\n",
      "Run 4, Epoch 35, val_loss = 0.0699\n",
      "Run 4, Epoch 36, val_loss = 0.0833\n",
      "Run 4, Epoch 37, val_loss = 0.0842\n",
      "Run 4, Epoch 38, val_loss = 0.0708\n",
      "Run 4, Epoch 39, val_loss = 0.0675\n",
      "Run 4, Epoch 40, val_loss = 0.0799\n",
      "Run 4, Epoch 41, val_loss = 0.0548\n",
      "Run 4, Epoch 42, val_loss = 0.0592\n",
      "Run 4, Epoch 43, val_loss = 0.0638\n",
      "Run 4, Epoch 44, val_loss = 0.0649\n",
      "Run 4, Epoch 45, val_loss = 0.0583\n",
      "Run 4, Epoch 46, val_loss = 0.0499\n",
      "Run 4, Epoch 47, val_loss = 0.0689\n",
      "Run 4, Epoch 48, val_loss = 0.0480\n",
      "Run 4, Epoch 49, val_loss = 0.0824\n",
      "Run 4, Epoch 50, val_loss = 0.0511\n",
      "Run 4, Epoch 51, val_loss = 0.0623\n",
      "Run 4, Epoch 52, val_loss = 0.0575\n",
      "Run 4, Epoch 53, val_loss = 0.0712\n",
      "Run 4, Epoch 54, val_loss = 0.0611\n",
      "Run 4, Epoch 55, val_loss = 0.0633\n",
      "Run 4, Epoch 56, val_loss = 0.0587\n",
      "Run 4, Epoch 57, val_loss = 0.0542\n",
      "Run 4, Epoch 58, val_loss = 0.0565\n",
      "Run 4, Epoch 59, val_loss = 0.0459\n",
      "Run 4, Epoch 60, val_loss = 0.0502\n",
      "Run 4, Epoch 61, val_loss = 0.0656\n",
      "Run 4, Epoch 62, val_loss = 0.0574\n",
      "Run 4, Epoch 63, val_loss = 0.0616\n",
      "Run 4, Epoch 64, val_loss = 0.0615\n",
      "Run 4, Epoch 65, val_loss = 0.0534\n",
      "Run 4, Epoch 66, val_loss = 0.0516\n",
      "Run 4, Epoch 67, val_loss = 0.0510\n",
      "Run 4, Epoch 68, val_loss = 0.0497\n",
      "Run 4, Epoch 69, val_loss = 0.0493\n",
      "Run 4, Epoch 70, val_loss = 0.0644\n",
      "Run 4, Epoch 71, val_loss = 0.0536\n",
      "Run 4, Epoch 72, val_loss = 0.0603\n",
      "Run 4, Epoch 73, val_loss = 0.0510\n",
      "Run 4, Epoch 74, val_loss = 0.0694\n",
      "Run 4, Epoch 75, val_loss = 0.0690\n",
      "Run 4, Epoch 76, val_loss = 0.0526\n",
      "Run 4, Epoch 77, val_loss = 0.0441\n",
      "Run 4, Epoch 78, val_loss = 0.0500\n",
      "Run 4, Epoch 79, val_loss = 0.0468\n",
      "Run 4, Epoch 80, val_loss = 0.0494\n",
      "Run 4, Epoch 81, val_loss = 0.0476\n",
      "Run 4, Epoch 82, val_loss = 0.0435\n",
      "Run 4, Epoch 83, val_loss = 0.0507\n",
      "Run 4, Epoch 84, val_loss = 0.0489\n",
      "Run 4, Epoch 85, val_loss = 0.0486\n",
      "Run 4, Epoch 86, val_loss = 0.0492\n",
      "Run 4, Epoch 87, val_loss = 0.0456\n",
      "Run 4, Epoch 88, val_loss = 0.0480\n",
      "Run 4, Epoch 89, val_loss = 0.0454\n",
      "Run 4, Epoch 90, val_loss = 0.0514\n",
      "Run 4, Epoch 91, val_loss = 0.0467\n",
      "Run 4, Epoch 92, val_loss = 0.0487\n",
      "Run 4, Epoch 93, val_loss = 0.0404\n",
      "Run 4, Epoch 94, val_loss = 0.0547\n",
      "Run 4, Epoch 95, val_loss = 0.0432\n",
      "Run 4, Epoch 96, val_loss = 0.0439\n",
      "Run 4, Epoch 97, val_loss = 0.0369\n",
      "Run 4, Epoch 98, val_loss = 0.0398\n",
      "Run 4, Epoch 99, val_loss = 0.0399\n",
      "Run 4, Epoch 100, val_loss = 0.0464\n",
      "Run 4, Epoch 101, val_loss = 0.0446\n",
      "Run 4, Epoch 102, val_loss = 0.0437\n",
      "Run 4, Epoch 103, val_loss = 0.0410\n",
      "Run 4, Epoch 104, val_loss = 0.0439\n",
      "Run 4, Epoch 105, val_loss = 0.0476\n",
      "Run 4, Epoch 106, val_loss = 0.0384\n",
      "Run 4, Epoch 107, val_loss = 0.0627\n",
      "Run 4, Epoch 108, val_loss = 0.0414\n",
      "Run 4, Epoch 109, val_loss = 0.0462\n",
      "Run 4, Epoch 110, val_loss = 0.0397\n",
      "Run 4, Epoch 111, val_loss = 0.0413\n",
      "Run 4, Epoch 112, val_loss = 0.0485\n",
      "Run 4, Epoch 113, val_loss = 0.0459\n",
      "Run 4, Epoch 114, val_loss = 0.0349\n",
      "Run 4, Epoch 115, val_loss = 0.0387\n",
      "Run 4, Epoch 116, val_loss = 0.0370\n",
      "Run 4, Epoch 117, val_loss = 0.0347\n",
      "Run 4, Epoch 118, val_loss = 0.0323\n",
      "Run 4, Epoch 119, val_loss = 0.0326\n",
      "Run 4, Epoch 120, val_loss = 0.0356\n",
      "Run 4, Epoch 121, val_loss = 0.0363\n",
      "Run 4, Epoch 122, val_loss = 0.0365\n",
      "Run 4, Epoch 123, val_loss = 0.0384\n",
      "Run 4, Epoch 124, val_loss = 0.0333\n",
      "Run 4, Epoch 125, val_loss = 0.0378\n",
      "Run 4, Epoch 126, val_loss = 0.0305\n",
      "Run 4, Epoch 127, val_loss = 0.0277\n",
      "Run 4, Epoch 128, val_loss = 0.0336\n",
      "Run 4, Epoch 129, val_loss = 0.0344\n",
      "Run 4, Epoch 130, val_loss = 0.0397\n",
      "Run 4, Epoch 131, val_loss = 0.0353\n",
      "Run 4, Epoch 132, val_loss = 0.0346\n",
      "Run 4, Epoch 133, val_loss = 0.0339\n",
      "Run 4, Epoch 134, val_loss = 0.0331\n",
      "Run 4, Epoch 135, val_loss = 0.0366\n",
      "Run 4, Epoch 136, val_loss = 0.0355\n",
      "Run 4, Epoch 137, val_loss = 0.0357\n",
      "Run 4, Epoch 138, val_loss = 0.0423\n",
      "Run 4, Epoch 139, val_loss = 0.0377\n",
      "Run 4, Epoch 140, val_loss = 0.0352\n",
      "Run 4, Epoch 141, val_loss = 0.0322\n",
      "Run 4, Epoch 142, val_loss = 0.0306\n",
      "Run 4, Epoch 143, val_loss = 0.0296\n",
      "Run 4, Epoch 144, val_loss = 0.0304\n",
      "Run 4, Epoch 145, val_loss = 0.0320\n",
      "Run 4, Epoch 146, val_loss = 0.0417\n",
      "Run 4, Epoch 147, val_loss = 0.0301\n",
      "✅ Run 4: Acc = 99.75%, MDE = 0.0080\n",
      "Run 5/5 - Alpha = 0.2\n",
      "Run 5, Epoch 1, val_loss = 1.3072\n",
      "Run 5, Epoch 2, val_loss = 0.5628\n",
      "Run 5, Epoch 3, val_loss = 0.3832\n",
      "Run 5, Epoch 4, val_loss = 0.3185\n",
      "Run 5, Epoch 5, val_loss = 0.2721\n",
      "Run 5, Epoch 6, val_loss = 0.2074\n",
      "Run 5, Epoch 7, val_loss = 0.2030\n",
      "Run 5, Epoch 8, val_loss = 0.2034\n",
      "Run 5, Epoch 9, val_loss = 0.1521\n",
      "Run 5, Epoch 10, val_loss = 0.1403\n",
      "Run 5, Epoch 11, val_loss = 0.1363\n",
      "Run 5, Epoch 12, val_loss = 0.1620\n",
      "Run 5, Epoch 13, val_loss = 0.1387\n",
      "Run 5, Epoch 14, val_loss = 0.1245\n",
      "Run 5, Epoch 15, val_loss = 0.0967\n",
      "Run 5, Epoch 16, val_loss = 0.1194\n",
      "Run 5, Epoch 17, val_loss = 0.1052\n",
      "Run 5, Epoch 18, val_loss = 0.1149\n",
      "Run 5, Epoch 19, val_loss = 0.1186\n",
      "Run 5, Epoch 20, val_loss = 0.1041\n",
      "Run 5, Epoch 21, val_loss = 0.1004\n",
      "Run 5, Epoch 22, val_loss = 0.0806\n",
      "Run 5, Epoch 23, val_loss = 0.1028\n",
      "Run 5, Epoch 24, val_loss = 0.0970\n",
      "Run 5, Epoch 25, val_loss = 0.0805\n",
      "Run 5, Epoch 26, val_loss = 0.0767\n",
      "Run 5, Epoch 27, val_loss = 0.0964\n",
      "Run 5, Epoch 28, val_loss = 0.0579\n",
      "Run 5, Epoch 29, val_loss = 0.0665\n",
      "Run 5, Epoch 30, val_loss = 0.0699\n",
      "Run 5, Epoch 31, val_loss = 0.0746\n",
      "Run 5, Epoch 32, val_loss = 0.0775\n",
      "Run 5, Epoch 33, val_loss = 0.0711\n",
      "Run 5, Epoch 34, val_loss = 0.0738\n",
      "Run 5, Epoch 35, val_loss = 0.0781\n",
      "Run 5, Epoch 36, val_loss = 0.0535\n",
      "Run 5, Epoch 37, val_loss = 0.0779\n",
      "Run 5, Epoch 38, val_loss = 0.0687\n",
      "Run 5, Epoch 39, val_loss = 0.0640\n",
      "Run 5, Epoch 40, val_loss = 0.0598\n",
      "Run 5, Epoch 41, val_loss = 0.0576\n",
      "Run 5, Epoch 42, val_loss = 0.0821\n",
      "Run 5, Epoch 43, val_loss = 0.1127\n",
      "Run 5, Epoch 44, val_loss = 0.0685\n",
      "Run 5, Epoch 45, val_loss = 0.0484\n",
      "Run 5, Epoch 46, val_loss = 0.0572\n",
      "Run 5, Epoch 47, val_loss = 0.0604\n",
      "Run 5, Epoch 48, val_loss = 0.0495\n",
      "Run 5, Epoch 49, val_loss = 0.0441\n",
      "Run 5, Epoch 50, val_loss = 0.0510\n",
      "Run 5, Epoch 51, val_loss = 0.0775\n",
      "Run 5, Epoch 52, val_loss = 0.0471\n",
      "Run 5, Epoch 53, val_loss = 0.0600\n",
      "Run 5, Epoch 54, val_loss = 0.0570\n",
      "Run 5, Epoch 55, val_loss = 0.0555\n",
      "Run 5, Epoch 56, val_loss = 0.0530\n",
      "Run 5, Epoch 57, val_loss = 0.0736\n",
      "Run 5, Epoch 58, val_loss = 0.0507\n",
      "Run 5, Epoch 59, val_loss = 0.0475\n",
      "Run 5, Epoch 60, val_loss = 0.0514\n",
      "Run 5, Epoch 61, val_loss = 0.0605\n",
      "Run 5, Epoch 62, val_loss = 0.0469\n",
      "Run 5, Epoch 63, val_loss = 0.0362\n",
      "Run 5, Epoch 64, val_loss = 0.0501\n",
      "Run 5, Epoch 65, val_loss = 0.0550\n",
      "Run 5, Epoch 66, val_loss = 0.0433\n",
      "Run 5, Epoch 67, val_loss = 0.0462\n",
      "Run 5, Epoch 68, val_loss = 0.0424\n",
      "Run 5, Epoch 69, val_loss = 0.0634\n",
      "Run 5, Epoch 70, val_loss = 0.0699\n",
      "Run 5, Epoch 71, val_loss = 0.0468\n",
      "Run 5, Epoch 72, val_loss = 0.0368\n",
      "Run 5, Epoch 73, val_loss = 0.0499\n",
      "Run 5, Epoch 74, val_loss = 0.0459\n",
      "Run 5, Epoch 75, val_loss = 0.0429\n",
      "Run 5, Epoch 76, val_loss = 0.0493\n",
      "Run 5, Epoch 77, val_loss = 0.0418\n",
      "Run 5, Epoch 78, val_loss = 0.0462\n",
      "Run 5, Epoch 79, val_loss = 0.0435\n",
      "Run 5, Epoch 80, val_loss = 0.0360\n",
      "Run 5, Epoch 81, val_loss = 0.0348\n",
      "Run 5, Epoch 82, val_loss = 0.0442\n",
      "Run 5, Epoch 83, val_loss = 0.0400\n",
      "Run 5, Epoch 84, val_loss = 0.0397\n",
      "Run 5, Epoch 85, val_loss = 0.0409\n",
      "Run 5, Epoch 86, val_loss = 0.0377\n",
      "Run 5, Epoch 87, val_loss = 0.0382\n",
      "Run 5, Epoch 88, val_loss = 0.0396\n",
      "Run 5, Epoch 89, val_loss = 0.0388\n",
      "Run 5, Epoch 90, val_loss = 0.0326\n",
      "Run 5, Epoch 91, val_loss = 0.0373\n",
      "Run 5, Epoch 92, val_loss = 0.0401\n",
      "Run 5, Epoch 93, val_loss = 0.0345\n",
      "Run 5, Epoch 94, val_loss = 0.0425\n",
      "Run 5, Epoch 95, val_loss = 0.0332\n",
      "Run 5, Epoch 96, val_loss = 0.0412\n",
      "Run 5, Epoch 97, val_loss = 0.0358\n",
      "Run 5, Epoch 98, val_loss = 0.0368\n",
      "Run 5, Epoch 99, val_loss = 0.0333\n",
      "Run 5, Epoch 100, val_loss = 0.0343\n",
      "Run 5, Epoch 101, val_loss = 0.0352\n",
      "Run 5, Epoch 102, val_loss = 0.0387\n",
      "Run 5, Epoch 103, val_loss = 0.0330\n",
      "Run 5, Epoch 104, val_loss = 0.0435\n",
      "Run 5, Epoch 105, val_loss = 0.0375\n",
      "Run 5, Epoch 106, val_loss = 0.0392\n",
      "Run 5, Epoch 107, val_loss = 0.0322\n",
      "Run 5, Epoch 108, val_loss = 0.0324\n",
      "Run 5, Epoch 109, val_loss = 0.0325\n",
      "Run 5, Epoch 110, val_loss = 0.0291\n",
      "Run 5, Epoch 111, val_loss = 0.0316\n",
      "Run 5, Epoch 112, val_loss = 0.0297\n",
      "Run 5, Epoch 113, val_loss = 0.0305\n",
      "Run 5, Epoch 114, val_loss = 0.0335\n",
      "Run 5, Epoch 115, val_loss = 0.0298\n",
      "Run 5, Epoch 116, val_loss = 0.0299\n",
      "Run 5, Epoch 117, val_loss = 0.0267\n",
      "Run 5, Epoch 118, val_loss = 0.0300\n",
      "Run 5, Epoch 119, val_loss = 0.0514\n",
      "Run 5, Epoch 120, val_loss = 0.0324\n",
      "Run 5, Epoch 121, val_loss = 0.0293\n",
      "Run 5, Epoch 122, val_loss = 0.0335\n",
      "Run 5, Epoch 123, val_loss = 0.0301\n",
      "Run 5, Epoch 124, val_loss = 0.0262\n",
      "Run 5, Epoch 125, val_loss = 0.0258\n",
      "Run 5, Epoch 126, val_loss = 0.0319\n",
      "Run 5, Epoch 127, val_loss = 0.0300\n",
      "Run 5, Epoch 128, val_loss = 0.0264\n",
      "Run 5, Epoch 129, val_loss = 0.0276\n",
      "Run 5, Epoch 130, val_loss = 0.0313\n",
      "Run 5, Epoch 131, val_loss = 0.0297\n",
      "Run 5, Epoch 132, val_loss = 0.0256\n",
      "Run 5, Epoch 133, val_loss = 0.0282\n",
      "Run 5, Epoch 134, val_loss = 0.0257\n",
      "Run 5, Epoch 135, val_loss = 0.0397\n",
      "Run 5, Epoch 136, val_loss = 0.0288\n",
      "Run 5, Epoch 137, val_loss = 0.0292\n",
      "Run 5, Epoch 138, val_loss = 0.0280\n",
      "Run 5, Epoch 139, val_loss = 0.0292\n",
      "Run 5, Epoch 140, val_loss = 0.0292\n",
      "Run 5, Epoch 141, val_loss = 0.0324\n",
      "Run 5, Epoch 142, val_loss = 0.0321\n",
      "Run 5, Epoch 143, val_loss = 0.0333\n",
      "Run 5, Epoch 144, val_loss = 0.0263\n",
      "Run 5, Epoch 145, val_loss = 0.0285\n",
      "Run 5, Epoch 146, val_loss = 0.0236\n",
      "Run 5, Epoch 147, val_loss = 0.0254\n",
      "Run 5, Epoch 148, val_loss = 0.0267\n",
      "Run 5, Epoch 149, val_loss = 0.0255\n",
      "Run 5, Epoch 150, val_loss = 0.0243\n",
      "Run 5, Epoch 151, val_loss = 0.0234\n",
      "Run 5, Epoch 152, val_loss = 0.0300\n",
      "Run 5, Epoch 153, val_loss = 0.0330\n",
      "Run 5, Epoch 154, val_loss = 0.0275\n",
      "Run 5, Epoch 155, val_loss = 0.0274\n",
      "Run 5, Epoch 156, val_loss = 0.0288\n",
      "Run 5, Epoch 157, val_loss = 0.0263\n",
      "Run 5, Epoch 158, val_loss = 0.0253\n",
      "Run 5, Epoch 159, val_loss = 0.0243\n",
      "Run 5, Epoch 160, val_loss = 0.0232\n",
      "Run 5, Epoch 161, val_loss = 0.0228\n",
      "Run 5, Epoch 162, val_loss = 0.0237\n",
      "Run 5, Epoch 163, val_loss = 0.0236\n",
      "Run 5, Epoch 164, val_loss = 0.0260\n",
      "Run 5, Epoch 165, val_loss = 0.0264\n",
      "Run 5, Epoch 166, val_loss = 0.0235\n",
      "Run 5, Epoch 167, val_loss = 0.0236\n",
      "Run 5, Epoch 168, val_loss = 0.0241\n",
      "Run 5, Epoch 169, val_loss = 0.0231\n",
      "Run 5, Epoch 170, val_loss = 0.0238\n",
      "Run 5, Epoch 171, val_loss = 0.0300\n",
      "Run 5, Epoch 172, val_loss = 0.0228\n",
      "Run 5, Epoch 173, val_loss = 0.0255\n",
      "Run 5, Epoch 174, val_loss = 0.0238\n",
      "Run 5, Epoch 175, val_loss = 0.0224\n",
      "Run 5, Epoch 176, val_loss = 0.0245\n",
      "Run 5, Epoch 177, val_loss = 0.0270\n",
      "Run 5, Epoch 178, val_loss = 0.0252\n",
      "Run 5, Epoch 179, val_loss = 0.0246\n",
      "Run 5, Epoch 180, val_loss = 0.0281\n",
      "Run 5, Epoch 181, val_loss = 0.0250\n",
      "Run 5, Epoch 182, val_loss = 0.0250\n",
      "Run 5, Epoch 183, val_loss = 0.0194\n",
      "Run 5, Epoch 184, val_loss = 0.0199\n",
      "Run 5, Epoch 185, val_loss = 0.0281\n",
      "Run 5, Epoch 186, val_loss = 0.0257\n",
      "Run 5, Epoch 187, val_loss = 0.0221\n",
      "Run 5, Epoch 188, val_loss = 0.0198\n",
      "Run 5, Epoch 189, val_loss = 0.0195\n",
      "Run 5, Epoch 190, val_loss = 0.0252\n",
      "Run 5, Epoch 191, val_loss = 0.0257\n",
      "Run 5, Epoch 192, val_loss = 0.0261\n",
      "Run 5, Epoch 193, val_loss = 0.0271\n",
      "Run 5, Epoch 194, val_loss = 0.0254\n",
      "Run 5, Epoch 195, val_loss = 0.0249\n",
      "Run 5, Epoch 196, val_loss = 0.0267\n",
      "Run 5, Epoch 197, val_loss = 0.0250\n",
      "Run 5, Epoch 198, val_loss = 0.0263\n",
      "Run 5, Epoch 199, val_loss = 0.0249\n",
      "Run 5, Epoch 200, val_loss = 0.0232\n",
      "✅ Run 5: Acc = 99.69%, MDE = 0.0093\n",
      "📁 Results & all errors saved to repeat_copy/02\n",
      "\n",
      "[Alpha = 0.3] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.3\n",
      "Run 1, Epoch 1, val_loss = 1.6391\n",
      "Run 1, Epoch 2, val_loss = 0.7701\n",
      "Run 1, Epoch 3, val_loss = 0.4629\n",
      "Run 1, Epoch 4, val_loss = 0.4004\n",
      "Run 1, Epoch 5, val_loss = 0.4149\n",
      "Run 1, Epoch 6, val_loss = 0.2597\n",
      "Run 1, Epoch 7, val_loss = 0.2263\n",
      "Run 1, Epoch 8, val_loss = 0.2252\n",
      "Run 1, Epoch 9, val_loss = 0.1741\n",
      "Run 1, Epoch 10, val_loss = 0.2422\n",
      "Run 1, Epoch 11, val_loss = 0.2599\n",
      "Run 1, Epoch 12, val_loss = 0.2014\n",
      "Run 1, Epoch 13, val_loss = 0.1926\n",
      "Run 1, Epoch 14, val_loss = 0.1432\n",
      "Run 1, Epoch 15, val_loss = 0.1590\n",
      "Run 1, Epoch 16, val_loss = 0.2310\n",
      "Run 1, Epoch 17, val_loss = 0.1219\n",
      "Run 1, Epoch 18, val_loss = 0.1638\n",
      "Run 1, Epoch 19, val_loss = 0.1264\n",
      "Run 1, Epoch 20, val_loss = 0.2402\n",
      "Run 1, Epoch 21, val_loss = 0.0981\n",
      "Run 1, Epoch 22, val_loss = 0.1342\n",
      "Run 1, Epoch 23, val_loss = 0.1317\n",
      "Run 1, Epoch 24, val_loss = 0.0856\n",
      "Run 1, Epoch 25, val_loss = 0.1212\n",
      "Run 1, Epoch 26, val_loss = 0.1181\n",
      "Run 1, Epoch 27, val_loss = 0.0814\n",
      "Run 1, Epoch 28, val_loss = 0.0849\n",
      "Run 1, Epoch 29, val_loss = 0.0929\n",
      "Run 1, Epoch 30, val_loss = 0.1000\n",
      "Run 1, Epoch 31, val_loss = 0.1112\n",
      "Run 1, Epoch 32, val_loss = 0.1230\n",
      "Run 1, Epoch 33, val_loss = 0.0818\n",
      "Run 1, Epoch 34, val_loss = 0.1219\n",
      "Run 1, Epoch 35, val_loss = 0.0795\n",
      "Run 1, Epoch 36, val_loss = 0.0820\n",
      "Run 1, Epoch 37, val_loss = 0.0795\n",
      "Run 1, Epoch 38, val_loss = 0.0699\n",
      "Run 1, Epoch 39, val_loss = 0.0894\n",
      "Run 1, Epoch 40, val_loss = 0.1210\n",
      "Run 1, Epoch 41, val_loss = 0.0764\n",
      "Run 1, Epoch 42, val_loss = 0.0804\n",
      "Run 1, Epoch 43, val_loss = 0.0810\n",
      "Run 1, Epoch 44, val_loss = 0.0733\n",
      "Run 1, Epoch 45, val_loss = 0.0998\n",
      "Run 1, Epoch 46, val_loss = 0.1036\n",
      "Run 1, Epoch 47, val_loss = 0.0921\n",
      "Run 1, Epoch 48, val_loss = 0.0747\n",
      "Run 1, Epoch 49, val_loss = 0.1167\n",
      "Run 1, Epoch 50, val_loss = 0.0649\n",
      "Run 1, Epoch 51, val_loss = 0.0670\n",
      "Run 1, Epoch 52, val_loss = 0.0746\n",
      "Run 1, Epoch 53, val_loss = 0.0607\n",
      "Run 1, Epoch 54, val_loss = 0.0668\n",
      "Run 1, Epoch 55, val_loss = 0.1769\n",
      "Run 1, Epoch 56, val_loss = 0.0678\n",
      "Run 1, Epoch 57, val_loss = 0.0592\n",
      "Run 1, Epoch 58, val_loss = 0.0457\n",
      "Run 1, Epoch 59, val_loss = 0.0578\n",
      "Run 1, Epoch 60, val_loss = 0.0582\n",
      "Run 1, Epoch 61, val_loss = 0.0453\n",
      "Run 1, Epoch 62, val_loss = 0.0693\n",
      "Run 1, Epoch 63, val_loss = 0.0594\n",
      "Run 1, Epoch 64, val_loss = 0.0685\n",
      "Run 1, Epoch 65, val_loss = 0.0412\n",
      "Run 1, Epoch 66, val_loss = 0.0379\n",
      "Run 1, Epoch 67, val_loss = 0.0382\n",
      "Run 1, Epoch 68, val_loss = 0.0431\n",
      "Run 1, Epoch 69, val_loss = 0.0494\n",
      "Run 1, Epoch 70, val_loss = 0.0562\n",
      "Run 1, Epoch 71, val_loss = 0.0947\n",
      "Run 1, Epoch 72, val_loss = 0.0514\n",
      "Run 1, Epoch 73, val_loss = 0.0568\n",
      "Run 1, Epoch 74, val_loss = 0.0641\n",
      "Run 1, Epoch 75, val_loss = 0.0634\n",
      "Run 1, Epoch 76, val_loss = 0.0535\n",
      "Run 1, Epoch 77, val_loss = 0.0441\n",
      "Run 1, Epoch 78, val_loss = 0.0643\n",
      "Run 1, Epoch 79, val_loss = 0.0436\n",
      "Run 1, Epoch 80, val_loss = 0.0520\n",
      "Run 1, Epoch 81, val_loss = 0.0511\n",
      "Run 1, Epoch 82, val_loss = 0.0581\n",
      "Run 1, Epoch 83, val_loss = 0.0388\n",
      "Run 1, Epoch 84, val_loss = 0.0458\n",
      "Run 1, Epoch 85, val_loss = 0.0460\n",
      "Run 1, Epoch 86, val_loss = 0.0512\n",
      "✅ Run 1: Acc = 99.69%, MDE = 0.0104\n",
      "Run 2/5 - Alpha = 0.3\n",
      "Run 2, Epoch 1, val_loss = 1.8340\n",
      "Run 2, Epoch 2, val_loss = 0.8432\n",
      "Run 2, Epoch 3, val_loss = 0.5703\n",
      "Run 2, Epoch 4, val_loss = 0.5306\n",
      "Run 2, Epoch 5, val_loss = 0.4307\n",
      "Run 2, Epoch 6, val_loss = 0.3181\n",
      "Run 2, Epoch 7, val_loss = 0.3059\n",
      "Run 2, Epoch 8, val_loss = 0.2800\n",
      "Run 2, Epoch 9, val_loss = 0.2390\n",
      "Run 2, Epoch 10, val_loss = 0.2512\n",
      "Run 2, Epoch 11, val_loss = 0.1842\n",
      "Run 2, Epoch 12, val_loss = 0.2519\n",
      "Run 2, Epoch 13, val_loss = 0.1692\n",
      "Run 2, Epoch 14, val_loss = 0.1804\n",
      "Run 2, Epoch 15, val_loss = 0.1619\n",
      "Run 2, Epoch 16, val_loss = 0.2068\n",
      "Run 2, Epoch 17, val_loss = 0.1642\n",
      "Run 2, Epoch 18, val_loss = 0.1254\n",
      "Run 2, Epoch 19, val_loss = 0.1595\n",
      "Run 2, Epoch 20, val_loss = 0.1055\n",
      "Run 2, Epoch 21, val_loss = 0.1267\n",
      "Run 2, Epoch 22, val_loss = 0.1197\n",
      "Run 2, Epoch 23, val_loss = 0.1261\n",
      "Run 2, Epoch 24, val_loss = 0.1187\n",
      "Run 2, Epoch 25, val_loss = 0.1248\n",
      "Run 2, Epoch 26, val_loss = 0.1081\n",
      "Run 2, Epoch 27, val_loss = 0.0940\n",
      "Run 2, Epoch 28, val_loss = 0.1072\n",
      "Run 2, Epoch 29, val_loss = 0.1089\n",
      "Run 2, Epoch 30, val_loss = 0.0823\n",
      "Run 2, Epoch 31, val_loss = 0.1499\n",
      "Run 2, Epoch 32, val_loss = 0.1097\n",
      "Run 2, Epoch 33, val_loss = 0.0762\n",
      "Run 2, Epoch 34, val_loss = 0.0859\n",
      "Run 2, Epoch 35, val_loss = 0.0955\n",
      "Run 2, Epoch 36, val_loss = 0.0802\n",
      "Run 2, Epoch 37, val_loss = 0.0744\n",
      "Run 2, Epoch 38, val_loss = 0.0942\n",
      "Run 2, Epoch 39, val_loss = 0.0852\n",
      "Run 2, Epoch 40, val_loss = 0.0659\n",
      "Run 2, Epoch 41, val_loss = 0.0720\n",
      "Run 2, Epoch 42, val_loss = 0.0714\n",
      "Run 2, Epoch 43, val_loss = 0.0556\n",
      "Run 2, Epoch 44, val_loss = 0.0756\n",
      "Run 2, Epoch 45, val_loss = 0.0706\n",
      "Run 2, Epoch 46, val_loss = 0.0999\n",
      "Run 2, Epoch 47, val_loss = 0.0739\n",
      "Run 2, Epoch 48, val_loss = 0.0687\n",
      "Run 2, Epoch 49, val_loss = 0.0535\n",
      "Run 2, Epoch 50, val_loss = 0.0539\n",
      "Run 2, Epoch 51, val_loss = 0.0617\n",
      "Run 2, Epoch 52, val_loss = 0.0800\n",
      "Run 2, Epoch 53, val_loss = 0.0539\n",
      "Run 2, Epoch 54, val_loss = 0.0633\n",
      "Run 2, Epoch 55, val_loss = 0.0562\n",
      "Run 2, Epoch 56, val_loss = 0.0553\n",
      "Run 2, Epoch 57, val_loss = 0.0558\n",
      "Run 2, Epoch 58, val_loss = 0.0488\n",
      "Run 2, Epoch 59, val_loss = 0.0582\n",
      "Run 2, Epoch 60, val_loss = 0.0459\n",
      "Run 2, Epoch 61, val_loss = 0.0510\n",
      "Run 2, Epoch 62, val_loss = 0.0502\n",
      "Run 2, Epoch 63, val_loss = 0.0808\n",
      "Run 2, Epoch 64, val_loss = 0.0647\n",
      "Run 2, Epoch 65, val_loss = 0.0448\n",
      "Run 2, Epoch 66, val_loss = 0.0614\n",
      "Run 2, Epoch 67, val_loss = 0.0468\n",
      "Run 2, Epoch 68, val_loss = 0.0422\n",
      "Run 2, Epoch 69, val_loss = 0.0506\n",
      "Run 2, Epoch 70, val_loss = 0.0527\n",
      "Run 2, Epoch 71, val_loss = 0.0442\n",
      "Run 2, Epoch 72, val_loss = 0.0494\n",
      "Run 2, Epoch 73, val_loss = 0.0487\n",
      "Run 2, Epoch 74, val_loss = 0.0598\n",
      "Run 2, Epoch 75, val_loss = 0.0400\n",
      "Run 2, Epoch 76, val_loss = 0.0479\n",
      "Run 2, Epoch 77, val_loss = 0.0599\n",
      "Run 2, Epoch 78, val_loss = 0.0633\n",
      "Run 2, Epoch 79, val_loss = 0.0538\n",
      "Run 2, Epoch 80, val_loss = 0.0467\n",
      "Run 2, Epoch 81, val_loss = 0.0510\n",
      "Run 2, Epoch 82, val_loss = 0.0487\n",
      "Run 2, Epoch 83, val_loss = 0.0494\n",
      "Run 2, Epoch 84, val_loss = 0.0474\n",
      "Run 2, Epoch 85, val_loss = 0.0477\n",
      "Run 2, Epoch 86, val_loss = 0.0468\n",
      "Run 2, Epoch 87, val_loss = 0.0434\n",
      "Run 2, Epoch 88, val_loss = 0.0448\n",
      "Run 2, Epoch 89, val_loss = 0.0426\n",
      "Run 2, Epoch 90, val_loss = 0.0750\n",
      "Run 2, Epoch 91, val_loss = 0.0540\n",
      "Run 2, Epoch 92, val_loss = 0.0505\n",
      "Run 2, Epoch 93, val_loss = 0.0388\n",
      "Run 2, Epoch 94, val_loss = 0.0346\n",
      "Run 2, Epoch 95, val_loss = 0.0380\n",
      "Run 2, Epoch 96, val_loss = 0.0344\n",
      "Run 2, Epoch 97, val_loss = 0.0358\n",
      "Run 2, Epoch 98, val_loss = 0.0352\n",
      "Run 2, Epoch 99, val_loss = 0.0377\n",
      "Run 2, Epoch 100, val_loss = 0.0391\n",
      "Run 2, Epoch 101, val_loss = 0.0291\n",
      "Run 2, Epoch 102, val_loss = 0.0383\n",
      "Run 2, Epoch 103, val_loss = 0.0405\n",
      "Run 2, Epoch 104, val_loss = 0.0384\n",
      "Run 2, Epoch 105, val_loss = 0.0420\n",
      "Run 2, Epoch 106, val_loss = 0.0362\n",
      "Run 2, Epoch 107, val_loss = 0.0368\n",
      "Run 2, Epoch 108, val_loss = 0.0333\n",
      "Run 2, Epoch 109, val_loss = 0.0312\n",
      "Run 2, Epoch 110, val_loss = 0.0320\n",
      "Run 2, Epoch 111, val_loss = 0.0325\n",
      "Run 2, Epoch 112, val_loss = 0.0407\n",
      "Run 2, Epoch 113, val_loss = 0.0348\n",
      "Run 2, Epoch 114, val_loss = 0.0284\n",
      "Run 2, Epoch 115, val_loss = 0.0302\n",
      "Run 2, Epoch 116, val_loss = 0.0303\n",
      "Run 2, Epoch 117, val_loss = 0.0346\n",
      "Run 2, Epoch 118, val_loss = 0.0325\n",
      "Run 2, Epoch 119, val_loss = 0.0373\n",
      "Run 2, Epoch 120, val_loss = 0.0359\n",
      "Run 2, Epoch 121, val_loss = 0.0308\n",
      "Run 2, Epoch 122, val_loss = 0.0342\n",
      "Run 2, Epoch 123, val_loss = 0.0294\n",
      "Run 2, Epoch 124, val_loss = 0.0300\n",
      "Run 2, Epoch 125, val_loss = 0.0566\n",
      "Run 2, Epoch 126, val_loss = 0.0350\n",
      "Run 2, Epoch 127, val_loss = 0.0334\n",
      "Run 2, Epoch 128, val_loss = 0.0322\n",
      "Run 2, Epoch 129, val_loss = 0.0282\n",
      "Run 2, Epoch 130, val_loss = 0.0259\n",
      "Run 2, Epoch 131, val_loss = 0.0369\n",
      "Run 2, Epoch 132, val_loss = 0.0484\n",
      "Run 2, Epoch 133, val_loss = 0.0283\n",
      "Run 2, Epoch 134, val_loss = 0.0307\n",
      "Run 2, Epoch 135, val_loss = 0.0308\n",
      "Run 2, Epoch 136, val_loss = 0.0303\n",
      "Run 2, Epoch 137, val_loss = 0.0339\n",
      "Run 2, Epoch 138, val_loss = 0.0328\n",
      "Run 2, Epoch 139, val_loss = 0.0481\n",
      "Run 2, Epoch 140, val_loss = 0.0300\n",
      "Run 2, Epoch 141, val_loss = 0.0295\n",
      "Run 2, Epoch 142, val_loss = 0.0532\n",
      "Run 2, Epoch 143, val_loss = 0.0406\n",
      "Run 2, Epoch 144, val_loss = 0.0432\n",
      "Run 2, Epoch 145, val_loss = 0.0411\n",
      "Run 2, Epoch 146, val_loss = 0.0362\n",
      "Run 2, Epoch 147, val_loss = 0.0373\n",
      "Run 2, Epoch 148, val_loss = 0.0354\n",
      "Run 2, Epoch 149, val_loss = 0.0303\n",
      "Run 2, Epoch 150, val_loss = 0.0597\n",
      "✅ Run 2: Acc = 99.69%, MDE = 0.0071\n",
      "Run 3/5 - Alpha = 0.3\n",
      "Run 3, Epoch 1, val_loss = 1.6455\n",
      "Run 3, Epoch 2, val_loss = 0.6742\n",
      "Run 3, Epoch 3, val_loss = 0.4803\n",
      "Run 3, Epoch 4, val_loss = 0.3515\n",
      "Run 3, Epoch 5, val_loss = 0.3091\n",
      "Run 3, Epoch 6, val_loss = 0.2618\n",
      "Run 3, Epoch 7, val_loss = 0.2317\n",
      "Run 3, Epoch 8, val_loss = 0.2282\n",
      "Run 3, Epoch 9, val_loss = 0.2351\n",
      "Run 3, Epoch 10, val_loss = 0.1876\n",
      "Run 3, Epoch 11, val_loss = 0.1931\n",
      "Run 3, Epoch 12, val_loss = 0.1516\n",
      "Run 3, Epoch 13, val_loss = 0.1313\n",
      "Run 3, Epoch 14, val_loss = 0.1455\n",
      "Run 3, Epoch 15, val_loss = 0.1571\n",
      "Run 3, Epoch 16, val_loss = 0.1476\n",
      "Run 3, Epoch 17, val_loss = 0.1327\n",
      "Run 3, Epoch 18, val_loss = 0.1309\n",
      "Run 3, Epoch 19, val_loss = 0.1174\n",
      "Run 3, Epoch 20, val_loss = 0.1253\n",
      "Run 3, Epoch 21, val_loss = 0.1121\n",
      "Run 3, Epoch 22, val_loss = 0.0977\n",
      "Run 3, Epoch 23, val_loss = 0.1097\n",
      "Run 3, Epoch 24, val_loss = 0.0979\n",
      "Run 3, Epoch 25, val_loss = 0.1055\n",
      "Run 3, Epoch 26, val_loss = 0.0912\n",
      "Run 3, Epoch 27, val_loss = 0.0864\n",
      "Run 3, Epoch 28, val_loss = 0.1003\n",
      "Run 3, Epoch 29, val_loss = 0.0772\n",
      "Run 3, Epoch 30, val_loss = 0.0953\n",
      "Run 3, Epoch 31, val_loss = 0.0691\n",
      "Run 3, Epoch 32, val_loss = 0.0789\n",
      "Run 3, Epoch 33, val_loss = 0.0827\n",
      "Run 3, Epoch 34, val_loss = 0.0757\n",
      "Run 3, Epoch 35, val_loss = 0.0721\n",
      "Run 3, Epoch 36, val_loss = 0.0804\n",
      "Run 3, Epoch 37, val_loss = 0.0810\n",
      "Run 3, Epoch 38, val_loss = 0.0747\n",
      "Run 3, Epoch 39, val_loss = 0.0673\n",
      "Run 3, Epoch 40, val_loss = 0.0833\n",
      "Run 3, Epoch 41, val_loss = 0.0536\n",
      "Run 3, Epoch 42, val_loss = 0.0659\n",
      "Run 3, Epoch 43, val_loss = 0.0517\n",
      "Run 3, Epoch 44, val_loss = 0.0493\n",
      "Run 3, Epoch 45, val_loss = 0.0541\n",
      "Run 3, Epoch 46, val_loss = 0.0480\n",
      "Run 3, Epoch 47, val_loss = 0.0539\n",
      "Run 3, Epoch 48, val_loss = 0.0649\n",
      "Run 3, Epoch 49, val_loss = 0.0608\n",
      "Run 3, Epoch 50, val_loss = 0.0522\n",
      "Run 3, Epoch 51, val_loss = 0.0515\n",
      "Run 3, Epoch 52, val_loss = 0.0590\n",
      "Run 3, Epoch 53, val_loss = 0.0443\n",
      "Run 3, Epoch 54, val_loss = 0.0477\n",
      "Run 3, Epoch 55, val_loss = 0.0415\n",
      "Run 3, Epoch 56, val_loss = 0.0456\n",
      "Run 3, Epoch 57, val_loss = 0.0430\n",
      "Run 3, Epoch 58, val_loss = 0.0418\n",
      "Run 3, Epoch 59, val_loss = 0.0405\n",
      "Run 3, Epoch 60, val_loss = 0.0444\n",
      "Run 3, Epoch 61, val_loss = 0.0406\n",
      "Run 3, Epoch 62, val_loss = 0.0631\n",
      "Run 3, Epoch 63, val_loss = 0.0602\n",
      "Run 3, Epoch 64, val_loss = 0.0675\n",
      "Run 3, Epoch 65, val_loss = 0.0360\n",
      "Run 3, Epoch 66, val_loss = 0.0453\n",
      "Run 3, Epoch 67, val_loss = 0.0476\n",
      "Run 3, Epoch 68, val_loss = 0.0419\n",
      "Run 3, Epoch 69, val_loss = 0.0589\n",
      "Run 3, Epoch 70, val_loss = 0.0493\n",
      "Run 3, Epoch 71, val_loss = 0.0457\n",
      "Run 3, Epoch 72, val_loss = 0.0829\n",
      "Run 3, Epoch 73, val_loss = 0.0434\n",
      "Run 3, Epoch 74, val_loss = 0.0450\n",
      "Run 3, Epoch 75, val_loss = 0.0436\n",
      "Run 3, Epoch 76, val_loss = 0.0683\n",
      "Run 3, Epoch 77, val_loss = 0.0447\n",
      "Run 3, Epoch 78, val_loss = 0.0426\n",
      "Run 3, Epoch 79, val_loss = 0.0462\n",
      "Run 3, Epoch 80, val_loss = 0.0404\n",
      "Run 3, Epoch 81, val_loss = 0.0550\n",
      "Run 3, Epoch 82, val_loss = 0.0384\n",
      "Run 3, Epoch 83, val_loss = 0.0383\n",
      "Run 3, Epoch 84, val_loss = 0.0315\n",
      "Run 3, Epoch 85, val_loss = 0.0356\n",
      "Run 3, Epoch 86, val_loss = 0.0358\n",
      "Run 3, Epoch 87, val_loss = 0.0404\n",
      "Run 3, Epoch 88, val_loss = 0.0391\n",
      "Run 3, Epoch 89, val_loss = 0.0344\n",
      "Run 3, Epoch 90, val_loss = 0.0264\n",
      "Run 3, Epoch 91, val_loss = 0.0302\n",
      "Run 3, Epoch 92, val_loss = 0.0369\n",
      "Run 3, Epoch 93, val_loss = 0.0346\n",
      "Run 3, Epoch 94, val_loss = 0.0303\n",
      "Run 3, Epoch 95, val_loss = 0.0263\n",
      "Run 3, Epoch 96, val_loss = 0.0346\n",
      "Run 3, Epoch 97, val_loss = 0.0299\n",
      "Run 3, Epoch 98, val_loss = 0.0333\n",
      "Run 3, Epoch 99, val_loss = 0.0304\n",
      "Run 3, Epoch 100, val_loss = 0.0269\n",
      "Run 3, Epoch 101, val_loss = 0.0296\n",
      "Run 3, Epoch 102, val_loss = 0.0303\n",
      "Run 3, Epoch 103, val_loss = 0.0326\n",
      "Run 3, Epoch 104, val_loss = 0.0334\n",
      "Run 3, Epoch 105, val_loss = 0.0309\n",
      "Run 3, Epoch 106, val_loss = 0.0334\n",
      "Run 3, Epoch 107, val_loss = 0.0471\n",
      "Run 3, Epoch 108, val_loss = 0.0324\n",
      "Run 3, Epoch 109, val_loss = 0.0450\n",
      "Run 3, Epoch 110, val_loss = 0.0327\n",
      "Run 3, Epoch 111, val_loss = 0.0322\n",
      "Run 3, Epoch 112, val_loss = 0.0276\n",
      "Run 3, Epoch 113, val_loss = 0.0265\n",
      "Run 3, Epoch 114, val_loss = 0.0234\n",
      "Run 3, Epoch 115, val_loss = 0.0281\n",
      "Run 3, Epoch 116, val_loss = 0.0248\n",
      "Run 3, Epoch 117, val_loss = 0.0237\n",
      "Run 3, Epoch 118, val_loss = 0.0260\n",
      "Run 3, Epoch 119, val_loss = 0.0330\n",
      "Run 3, Epoch 120, val_loss = 0.0287\n",
      "Run 3, Epoch 121, val_loss = 0.0286\n",
      "Run 3, Epoch 122, val_loss = 0.0260\n",
      "Run 3, Epoch 123, val_loss = 0.0260\n",
      "Run 3, Epoch 124, val_loss = 0.0298\n",
      "Run 3, Epoch 125, val_loss = 0.0237\n",
      "Run 3, Epoch 126, val_loss = 0.0311\n",
      "Run 3, Epoch 127, val_loss = 0.0381\n",
      "Run 3, Epoch 128, val_loss = 0.0284\n",
      "Run 3, Epoch 129, val_loss = 0.0317\n",
      "Run 3, Epoch 130, val_loss = 0.0266\n",
      "Run 3, Epoch 131, val_loss = 0.0271\n",
      "Run 3, Epoch 132, val_loss = 0.0281\n",
      "Run 3, Epoch 133, val_loss = 0.0277\n",
      "Run 3, Epoch 134, val_loss = 0.0282\n",
      "✅ Run 3: Acc = 99.64%, MDE = 0.0108\n",
      "Run 4/5 - Alpha = 0.3\n",
      "Run 4, Epoch 1, val_loss = 1.6807\n",
      "Run 4, Epoch 2, val_loss = 0.7499\n",
      "Run 4, Epoch 3, val_loss = 0.4849\n",
      "Run 4, Epoch 4, val_loss = 0.4428\n",
      "Run 4, Epoch 5, val_loss = 0.3098\n",
      "Run 4, Epoch 6, val_loss = 0.2986\n",
      "Run 4, Epoch 7, val_loss = 0.2835\n",
      "Run 4, Epoch 8, val_loss = 0.2136\n",
      "Run 4, Epoch 9, val_loss = 0.2323\n",
      "Run 4, Epoch 10, val_loss = 0.2209\n",
      "Run 4, Epoch 11, val_loss = 0.1717\n",
      "Run 4, Epoch 12, val_loss = 0.1652\n",
      "Run 4, Epoch 13, val_loss = 0.1857\n",
      "Run 4, Epoch 14, val_loss = 0.1416\n",
      "Run 4, Epoch 15, val_loss = 0.1382\n",
      "Run 4, Epoch 16, val_loss = 0.1519\n",
      "Run 4, Epoch 17, val_loss = 0.1346\n",
      "Run 4, Epoch 18, val_loss = 0.1271\n",
      "Run 4, Epoch 19, val_loss = 0.1392\n",
      "Run 4, Epoch 20, val_loss = 0.1496\n",
      "Run 4, Epoch 21, val_loss = 0.1326\n",
      "Run 4, Epoch 22, val_loss = 0.1365\n",
      "Run 4, Epoch 23, val_loss = 0.0932\n",
      "Run 4, Epoch 24, val_loss = 0.1348\n",
      "Run 4, Epoch 25, val_loss = 0.1027\n",
      "Run 4, Epoch 26, val_loss = 0.0929\n",
      "Run 4, Epoch 27, val_loss = 0.0971\n",
      "Run 4, Epoch 28, val_loss = 0.0946\n",
      "Run 4, Epoch 29, val_loss = 0.0994\n",
      "Run 4, Epoch 30, val_loss = 0.1222\n",
      "Run 4, Epoch 31, val_loss = 0.0714\n",
      "Run 4, Epoch 32, val_loss = 0.0745\n",
      "Run 4, Epoch 33, val_loss = 0.0702\n",
      "Run 4, Epoch 34, val_loss = 0.0880\n",
      "Run 4, Epoch 35, val_loss = 0.0908\n",
      "Run 4, Epoch 36, val_loss = 0.0673\n",
      "Run 4, Epoch 37, val_loss = 0.0631\n",
      "Run 4, Epoch 38, val_loss = 0.0804\n",
      "Run 4, Epoch 39, val_loss = 0.0789\n",
      "Run 4, Epoch 40, val_loss = 0.0624\n",
      "Run 4, Epoch 41, val_loss = 0.0808\n",
      "Run 4, Epoch 42, val_loss = 0.0849\n",
      "Run 4, Epoch 43, val_loss = 0.0607\n",
      "Run 4, Epoch 44, val_loss = 0.0592\n",
      "Run 4, Epoch 45, val_loss = 0.0572\n",
      "Run 4, Epoch 46, val_loss = 0.0579\n",
      "Run 4, Epoch 47, val_loss = 0.0561\n",
      "Run 4, Epoch 48, val_loss = 0.0721\n",
      "Run 4, Epoch 49, val_loss = 0.0687\n",
      "Run 4, Epoch 50, val_loss = 0.0586\n",
      "Run 4, Epoch 51, val_loss = 0.0623\n",
      "Run 4, Epoch 52, val_loss = 0.0620\n",
      "Run 4, Epoch 53, val_loss = 0.0559\n",
      "Run 4, Epoch 54, val_loss = 0.0497\n",
      "Run 4, Epoch 55, val_loss = 0.0863\n",
      "Run 4, Epoch 56, val_loss = 0.0586\n",
      "Run 4, Epoch 57, val_loss = 0.0672\n",
      "Run 4, Epoch 58, val_loss = 0.0429\n",
      "Run 4, Epoch 59, val_loss = 0.0480\n",
      "Run 4, Epoch 60, val_loss = 0.0488\n",
      "Run 4, Epoch 61, val_loss = 0.0430\n",
      "Run 4, Epoch 62, val_loss = 0.0557\n",
      "Run 4, Epoch 63, val_loss = 0.0472\n",
      "Run 4, Epoch 64, val_loss = 0.0483\n",
      "Run 4, Epoch 65, val_loss = 0.0365\n",
      "Run 4, Epoch 66, val_loss = 0.0456\n",
      "Run 4, Epoch 67, val_loss = 0.0605\n",
      "Run 4, Epoch 68, val_loss = 0.0374\n",
      "Run 4, Epoch 69, val_loss = 0.0516\n",
      "Run 4, Epoch 70, val_loss = 0.0405\n",
      "Run 4, Epoch 71, val_loss = 0.0498\n",
      "Run 4, Epoch 72, val_loss = 0.0465\n",
      "Run 4, Epoch 73, val_loss = 0.0533\n",
      "Run 4, Epoch 74, val_loss = 0.0368\n",
      "Run 4, Epoch 75, val_loss = 0.0488\n",
      "Run 4, Epoch 76, val_loss = 0.0437\n",
      "Run 4, Epoch 77, val_loss = 0.0437\n",
      "Run 4, Epoch 78, val_loss = 0.0489\n",
      "Run 4, Epoch 79, val_loss = 0.0555\n",
      "Run 4, Epoch 80, val_loss = 0.0464\n",
      "Run 4, Epoch 81, val_loss = 0.0477\n",
      "Run 4, Epoch 82, val_loss = 0.0385\n",
      "Run 4, Epoch 83, val_loss = 0.0329\n",
      "Run 4, Epoch 84, val_loss = 0.0357\n",
      "Run 4, Epoch 85, val_loss = 0.0351\n",
      "Run 4, Epoch 86, val_loss = 0.0404\n",
      "Run 4, Epoch 87, val_loss = 0.0393\n",
      "Run 4, Epoch 88, val_loss = 0.0332\n",
      "Run 4, Epoch 89, val_loss = 0.0386\n",
      "Run 4, Epoch 90, val_loss = 0.0465\n",
      "Run 4, Epoch 91, val_loss = 0.0461\n",
      "Run 4, Epoch 92, val_loss = 0.0361\n",
      "Run 4, Epoch 93, val_loss = 0.0371\n",
      "Run 4, Epoch 94, val_loss = 0.0332\n",
      "Run 4, Epoch 95, val_loss = 0.0315\n",
      "Run 4, Epoch 96, val_loss = 0.0314\n",
      "Run 4, Epoch 97, val_loss = 0.0354\n",
      "Run 4, Epoch 98, val_loss = 0.0394\n",
      "Run 4, Epoch 99, val_loss = 0.0377\n",
      "Run 4, Epoch 100, val_loss = 0.0368\n",
      "Run 4, Epoch 101, val_loss = 0.0776\n",
      "Run 4, Epoch 102, val_loss = 0.0292\n",
      "Run 4, Epoch 103, val_loss = 0.0315\n",
      "Run 4, Epoch 104, val_loss = 0.0332\n",
      "Run 4, Epoch 105, val_loss = 0.0433\n",
      "Run 4, Epoch 106, val_loss = 0.0400\n",
      "Run 4, Epoch 107, val_loss = 0.0369\n",
      "Run 4, Epoch 108, val_loss = 0.0367\n",
      "Run 4, Epoch 109, val_loss = 0.0557\n",
      "Run 4, Epoch 110, val_loss = 0.0334\n",
      "Run 4, Epoch 111, val_loss = 0.0353\n",
      "Run 4, Epoch 112, val_loss = 0.0316\n",
      "Run 4, Epoch 113, val_loss = 0.0255\n",
      "Run 4, Epoch 114, val_loss = 0.0306\n",
      "Run 4, Epoch 115, val_loss = 0.0607\n",
      "Run 4, Epoch 116, val_loss = 0.0277\n",
      "Run 4, Epoch 117, val_loss = 0.0425\n",
      "Run 4, Epoch 118, val_loss = 0.0598\n",
      "Run 4, Epoch 119, val_loss = 0.0317\n",
      "Run 4, Epoch 120, val_loss = 0.0498\n",
      "Run 4, Epoch 121, val_loss = 0.0352\n",
      "Run 4, Epoch 122, val_loss = 0.0331\n",
      "Run 4, Epoch 123, val_loss = 0.0330\n",
      "Run 4, Epoch 124, val_loss = 0.0326\n",
      "Run 4, Epoch 125, val_loss = 0.0288\n",
      "Run 4, Epoch 126, val_loss = 0.0293\n",
      "Run 4, Epoch 127, val_loss = 0.0335\n",
      "Run 4, Epoch 128, val_loss = 0.0275\n",
      "Run 4, Epoch 129, val_loss = 0.0379\n",
      "Run 4, Epoch 130, val_loss = 0.0357\n",
      "Run 4, Epoch 131, val_loss = 0.0330\n",
      "Run 4, Epoch 132, val_loss = 0.0348\n",
      "Run 4, Epoch 133, val_loss = 0.0354\n",
      "✅ Run 4: Acc = 99.64%, MDE = 0.0130\n",
      "Run 5/5 - Alpha = 0.3\n",
      "Run 5, Epoch 1, val_loss = 1.5683\n",
      "Run 5, Epoch 2, val_loss = 0.6406\n",
      "Run 5, Epoch 3, val_loss = 0.4555\n",
      "Run 5, Epoch 4, val_loss = 0.3776\n",
      "Run 5, Epoch 5, val_loss = 0.3017\n",
      "Run 5, Epoch 6, val_loss = 0.3061\n",
      "Run 5, Epoch 7, val_loss = 0.2514\n",
      "Run 5, Epoch 8, val_loss = 0.2131\n",
      "Run 5, Epoch 9, val_loss = 0.2503\n",
      "Run 5, Epoch 10, val_loss = 0.2266\n",
      "Run 5, Epoch 11, val_loss = 0.1812\n",
      "Run 5, Epoch 12, val_loss = 0.1441\n",
      "Run 5, Epoch 13, val_loss = 0.1610\n",
      "Run 5, Epoch 14, val_loss = 0.1400\n",
      "Run 5, Epoch 15, val_loss = 0.2130\n",
      "Run 5, Epoch 16, val_loss = 0.1088\n",
      "Run 5, Epoch 17, val_loss = 0.1150\n",
      "Run 5, Epoch 18, val_loss = 0.1458\n",
      "Run 5, Epoch 19, val_loss = 0.1167\n",
      "Run 5, Epoch 20, val_loss = 0.0924\n",
      "Run 5, Epoch 21, val_loss = 0.1350\n",
      "Run 5, Epoch 22, val_loss = 0.1000\n",
      "Run 5, Epoch 23, val_loss = 0.0974\n",
      "Run 5, Epoch 24, val_loss = 0.1391\n",
      "Run 5, Epoch 25, val_loss = 0.0989\n",
      "Run 5, Epoch 26, val_loss = 0.0883\n",
      "Run 5, Epoch 27, val_loss = 0.0969\n",
      "Run 5, Epoch 28, val_loss = 0.0790\n",
      "Run 5, Epoch 29, val_loss = 0.0764\n",
      "Run 5, Epoch 30, val_loss = 0.1137\n",
      "Run 5, Epoch 31, val_loss = 0.0849\n",
      "Run 5, Epoch 32, val_loss = 0.0668\n",
      "Run 5, Epoch 33, val_loss = 0.0742\n",
      "Run 5, Epoch 34, val_loss = 0.0724\n",
      "Run 5, Epoch 35, val_loss = 0.0540\n",
      "Run 5, Epoch 36, val_loss = 0.0608\n",
      "Run 5, Epoch 37, val_loss = 0.0620\n",
      "Run 5, Epoch 38, val_loss = 0.0951\n",
      "Run 5, Epoch 39, val_loss = 0.0642\n",
      "Run 5, Epoch 40, val_loss = 0.0849\n",
      "Run 5, Epoch 41, val_loss = 0.0677\n",
      "Run 5, Epoch 42, val_loss = 0.0781\n",
      "Run 5, Epoch 43, val_loss = 0.0698\n",
      "Run 5, Epoch 44, val_loss = 0.0484\n",
      "Run 5, Epoch 45, val_loss = 0.0447\n",
      "Run 5, Epoch 46, val_loss = 0.0581\n",
      "Run 5, Epoch 47, val_loss = 0.0660\n",
      "Run 5, Epoch 48, val_loss = 0.0506\n",
      "Run 5, Epoch 49, val_loss = 0.0518\n",
      "Run 5, Epoch 50, val_loss = 0.0507\n",
      "Run 5, Epoch 51, val_loss = 0.0730\n",
      "Run 5, Epoch 52, val_loss = 0.0540\n",
      "Run 5, Epoch 53, val_loss = 0.0622\n",
      "Run 5, Epoch 54, val_loss = 0.0466\n",
      "Run 5, Epoch 55, val_loss = 0.0467\n",
      "Run 5, Epoch 56, val_loss = 0.0569\n",
      "Run 5, Epoch 57, val_loss = 0.0685\n",
      "Run 5, Epoch 58, val_loss = 0.0511\n",
      "Run 5, Epoch 59, val_loss = 0.0498\n",
      "Run 5, Epoch 60, val_loss = 0.0480\n",
      "Run 5, Epoch 61, val_loss = 0.0508\n",
      "Run 5, Epoch 62, val_loss = 0.0326\n",
      "Run 5, Epoch 63, val_loss = 0.0559\n",
      "Run 5, Epoch 64, val_loss = 0.0348\n",
      "Run 5, Epoch 65, val_loss = 0.0364\n",
      "Run 5, Epoch 66, val_loss = 0.0372\n",
      "Run 5, Epoch 67, val_loss = 0.0384\n",
      "Run 5, Epoch 68, val_loss = 0.0382\n",
      "Run 5, Epoch 69, val_loss = 0.0406\n",
      "Run 5, Epoch 70, val_loss = 0.0478\n",
      "Run 5, Epoch 71, val_loss = 0.0318\n",
      "Run 5, Epoch 72, val_loss = 0.0234\n",
      "Run 5, Epoch 73, val_loss = 0.0342\n",
      "Run 5, Epoch 74, val_loss = 0.0353\n",
      "Run 5, Epoch 75, val_loss = 0.0598\n",
      "Run 5, Epoch 76, val_loss = 0.0324\n",
      "Run 5, Epoch 77, val_loss = 0.0446\n",
      "Run 5, Epoch 78, val_loss = 0.0312\n",
      "Run 5, Epoch 79, val_loss = 0.0450\n",
      "Run 5, Epoch 80, val_loss = 0.0312\n",
      "Run 5, Epoch 81, val_loss = 0.0266\n",
      "Run 5, Epoch 82, val_loss = 0.0363\n",
      "Run 5, Epoch 83, val_loss = 0.0280\n",
      "Run 5, Epoch 84, val_loss = 0.0401\n",
      "Run 5, Epoch 85, val_loss = 0.0462\n",
      "Run 5, Epoch 86, val_loss = 0.0339\n",
      "Run 5, Epoch 87, val_loss = 0.0303\n",
      "Run 5, Epoch 88, val_loss = 0.0338\n",
      "Run 5, Epoch 89, val_loss = 0.0395\n",
      "Run 5, Epoch 90, val_loss = 0.0222\n",
      "Run 5, Epoch 91, val_loss = 0.0424\n",
      "Run 5, Epoch 92, val_loss = 0.0280\n",
      "Run 5, Epoch 93, val_loss = 0.0217\n",
      "Run 5, Epoch 94, val_loss = 0.0268\n",
      "Run 5, Epoch 95, val_loss = 0.0242\n",
      "Run 5, Epoch 96, val_loss = 0.0485\n",
      "Run 5, Epoch 97, val_loss = 0.0234\n",
      "Run 5, Epoch 98, val_loss = 0.0283\n",
      "Run 5, Epoch 99, val_loss = 0.0283\n",
      "Run 5, Epoch 100, val_loss = 0.0259\n",
      "Run 5, Epoch 101, val_loss = 0.0239\n",
      "Run 5, Epoch 102, val_loss = 0.0281\n",
      "Run 5, Epoch 103, val_loss = 0.0259\n",
      "Run 5, Epoch 104, val_loss = 0.0258\n",
      "Run 5, Epoch 105, val_loss = 0.0260\n",
      "Run 5, Epoch 106, val_loss = 0.0210\n",
      "Run 5, Epoch 107, val_loss = 0.0220\n",
      "Run 5, Epoch 108, val_loss = 0.0309\n",
      "Run 5, Epoch 109, val_loss = 0.0212\n",
      "Run 5, Epoch 110, val_loss = 0.0304\n",
      "Run 5, Epoch 111, val_loss = 0.0284\n",
      "Run 5, Epoch 112, val_loss = 0.0252\n",
      "Run 5, Epoch 113, val_loss = 0.0264\n",
      "Run 5, Epoch 114, val_loss = 0.0410\n",
      "Run 5, Epoch 115, val_loss = 0.0276\n",
      "Run 5, Epoch 116, val_loss = 0.0259\n",
      "Run 5, Epoch 117, val_loss = 0.0205\n",
      "Run 5, Epoch 118, val_loss = 0.0230\n",
      "Run 5, Epoch 119, val_loss = 0.0256\n",
      "Run 5, Epoch 120, val_loss = 0.0261\n",
      "Run 5, Epoch 121, val_loss = 0.0395\n",
      "Run 5, Epoch 122, val_loss = 0.0251\n",
      "Run 5, Epoch 123, val_loss = 0.0258\n",
      "Run 5, Epoch 124, val_loss = 0.0246\n",
      "Run 5, Epoch 125, val_loss = 0.0228\n",
      "Run 5, Epoch 126, val_loss = 0.0286\n",
      "Run 5, Epoch 127, val_loss = 0.0403\n",
      "Run 5, Epoch 128, val_loss = 0.0313\n",
      "Run 5, Epoch 129, val_loss = 0.0222\n",
      "Run 5, Epoch 130, val_loss = 0.0222\n",
      "Run 5, Epoch 131, val_loss = 0.0255\n",
      "Run 5, Epoch 132, val_loss = 0.0222\n",
      "Run 5, Epoch 133, val_loss = 0.0288\n",
      "Run 5, Epoch 134, val_loss = 0.0625\n",
      "Run 5, Epoch 135, val_loss = 0.0234\n",
      "Run 5, Epoch 136, val_loss = 0.0233\n",
      "Run 5, Epoch 137, val_loss = 0.0255\n",
      "✅ Run 5: Acc = 99.80%, MDE = 0.0075\n",
      "📁 Results & all errors saved to repeat_copy/03\n",
      "\n",
      "[Alpha = 0.4] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.4\n",
      "Run 1, Epoch 1, val_loss = 1.9183\n",
      "Run 1, Epoch 2, val_loss = 0.8895\n",
      "Run 1, Epoch 3, val_loss = 0.5775\n",
      "Run 1, Epoch 4, val_loss = 0.5847\n",
      "Run 1, Epoch 5, val_loss = 0.5053\n",
      "Run 1, Epoch 6, val_loss = 0.4061\n",
      "Run 1, Epoch 7, val_loss = 0.3219\n",
      "Run 1, Epoch 8, val_loss = 0.2978\n",
      "Run 1, Epoch 9, val_loss = 0.3273\n",
      "Run 1, Epoch 10, val_loss = 0.3455\n",
      "Run 1, Epoch 11, val_loss = 0.2344\n",
      "Run 1, Epoch 12, val_loss = 0.1786\n",
      "Run 1, Epoch 13, val_loss = 0.2553\n",
      "Run 1, Epoch 14, val_loss = 0.2105\n",
      "Run 1, Epoch 15, val_loss = 0.1728\n",
      "Run 1, Epoch 16, val_loss = 0.1889\n",
      "Run 1, Epoch 17, val_loss = 0.1500\n",
      "Run 1, Epoch 18, val_loss = 0.1642\n",
      "Run 1, Epoch 19, val_loss = 0.1656\n",
      "Run 1, Epoch 20, val_loss = 0.1314\n",
      "Run 1, Epoch 21, val_loss = 0.1420\n",
      "Run 1, Epoch 22, val_loss = 0.1118\n",
      "Run 1, Epoch 23, val_loss = 0.0816\n",
      "Run 1, Epoch 24, val_loss = 0.1161\n",
      "Run 1, Epoch 25, val_loss = 0.1086\n",
      "Run 1, Epoch 26, val_loss = 0.0882\n",
      "Run 1, Epoch 27, val_loss = 0.1546\n",
      "Run 1, Epoch 28, val_loss = 0.0800\n",
      "Run 1, Epoch 29, val_loss = 0.0705\n",
      "Run 1, Epoch 30, val_loss = 0.0712\n",
      "Run 1, Epoch 31, val_loss = 0.1009\n",
      "Run 1, Epoch 32, val_loss = 0.1112\n",
      "Run 1, Epoch 33, val_loss = 0.1180\n",
      "Run 1, Epoch 34, val_loss = 0.0795\n",
      "Run 1, Epoch 35, val_loss = 0.0969\n",
      "Run 1, Epoch 36, val_loss = 0.0891\n",
      "Run 1, Epoch 37, val_loss = 0.0742\n",
      "Run 1, Epoch 38, val_loss = 0.0767\n",
      "Run 1, Epoch 39, val_loss = 0.0737\n",
      "Run 1, Epoch 40, val_loss = 0.0704\n",
      "Run 1, Epoch 41, val_loss = 0.0811\n",
      "Run 1, Epoch 42, val_loss = 0.0681\n",
      "Run 1, Epoch 43, val_loss = 0.0824\n",
      "Run 1, Epoch 44, val_loss = 0.0887\n",
      "Run 1, Epoch 45, val_loss = 0.0550\n",
      "Run 1, Epoch 46, val_loss = 0.0700\n",
      "Run 1, Epoch 47, val_loss = 0.0889\n",
      "Run 1, Epoch 48, val_loss = 0.0642\n",
      "Run 1, Epoch 49, val_loss = 0.0536\n",
      "Run 1, Epoch 50, val_loss = 0.0732\n",
      "Run 1, Epoch 51, val_loss = 0.0842\n",
      "Run 1, Epoch 52, val_loss = 0.1015\n",
      "Run 1, Epoch 53, val_loss = 0.0784\n",
      "Run 1, Epoch 54, val_loss = 0.0684\n",
      "Run 1, Epoch 55, val_loss = 0.0568\n",
      "Run 1, Epoch 56, val_loss = 0.0708\n",
      "Run 1, Epoch 57, val_loss = 0.0451\n",
      "Run 1, Epoch 58, val_loss = 0.0550\n",
      "Run 1, Epoch 59, val_loss = 0.0635\n",
      "Run 1, Epoch 60, val_loss = 0.0617\n",
      "Run 1, Epoch 61, val_loss = 0.0575\n",
      "Run 1, Epoch 62, val_loss = 0.0647\n",
      "Run 1, Epoch 63, val_loss = 0.0595\n",
      "Run 1, Epoch 64, val_loss = 0.0814\n",
      "Run 1, Epoch 65, val_loss = 0.0453\n",
      "Run 1, Epoch 66, val_loss = 0.0644\n",
      "Run 1, Epoch 67, val_loss = 0.0610\n",
      "Run 1, Epoch 68, val_loss = 0.0533\n",
      "Run 1, Epoch 69, val_loss = 0.0964\n",
      "Run 1, Epoch 70, val_loss = 0.0470\n",
      "Run 1, Epoch 71, val_loss = 0.0554\n",
      "Run 1, Epoch 72, val_loss = 0.0573\n",
      "Run 1, Epoch 73, val_loss = 0.0748\n",
      "Run 1, Epoch 74, val_loss = 0.0518\n",
      "Run 1, Epoch 75, val_loss = 0.0482\n",
      "Run 1, Epoch 76, val_loss = 0.0449\n",
      "Run 1, Epoch 77, val_loss = 0.0497\n",
      "Run 1, Epoch 78, val_loss = 0.0420\n",
      "Run 1, Epoch 79, val_loss = 0.0402\n",
      "Run 1, Epoch 80, val_loss = 0.0488\n",
      "Run 1, Epoch 81, val_loss = 0.0518\n",
      "Run 1, Epoch 82, val_loss = 0.0535\n",
      "Run 1, Epoch 83, val_loss = 0.0510\n",
      "Run 1, Epoch 84, val_loss = 0.0431\n",
      "Run 1, Epoch 85, val_loss = 0.0479\n",
      "Run 1, Epoch 86, val_loss = 0.0409\n",
      "Run 1, Epoch 87, val_loss = 0.0428\n",
      "Run 1, Epoch 88, val_loss = 0.0413\n",
      "Run 1, Epoch 89, val_loss = 0.0435\n",
      "Run 1, Epoch 90, val_loss = 0.0471\n",
      "Run 1, Epoch 91, val_loss = 0.0485\n",
      "Run 1, Epoch 92, val_loss = 0.0486\n",
      "Run 1, Epoch 93, val_loss = 0.0439\n",
      "Run 1, Epoch 94, val_loss = 0.0383\n",
      "Run 1, Epoch 95, val_loss = 0.0428\n",
      "Run 1, Epoch 96, val_loss = 0.0431\n",
      "Run 1, Epoch 97, val_loss = 0.0403\n",
      "Run 1, Epoch 98, val_loss = 0.0394\n",
      "Run 1, Epoch 99, val_loss = 0.0411\n",
      "Run 1, Epoch 100, val_loss = 0.0354\n",
      "Run 1, Epoch 101, val_loss = 0.0385\n",
      "Run 1, Epoch 102, val_loss = 0.0652\n",
      "Run 1, Epoch 103, val_loss = 0.0430\n",
      "Run 1, Epoch 104, val_loss = 0.0391\n",
      "Run 1, Epoch 105, val_loss = 0.0350\n",
      "Run 1, Epoch 106, val_loss = 0.0316\n",
      "Run 1, Epoch 107, val_loss = 0.0401\n",
      "Run 1, Epoch 108, val_loss = 0.0366\n",
      "Run 1, Epoch 109, val_loss = 0.0692\n",
      "Run 1, Epoch 110, val_loss = 0.0344\n",
      "Run 1, Epoch 111, val_loss = 0.0415\n",
      "Run 1, Epoch 112, val_loss = 0.0380\n",
      "Run 1, Epoch 113, val_loss = 0.0375\n",
      "Run 1, Epoch 114, val_loss = 0.0304\n",
      "Run 1, Epoch 115, val_loss = 0.0347\n",
      "Run 1, Epoch 116, val_loss = 0.0334\n",
      "Run 1, Epoch 117, val_loss = 0.0536\n",
      "Run 1, Epoch 118, val_loss = 0.0385\n",
      "Run 1, Epoch 119, val_loss = 0.0338\n",
      "Run 1, Epoch 120, val_loss = 0.0344\n",
      "Run 1, Epoch 121, val_loss = 0.0385\n",
      "Run 1, Epoch 122, val_loss = 0.0336\n",
      "Run 1, Epoch 123, val_loss = 0.0370\n",
      "Run 1, Epoch 124, val_loss = 0.0430\n",
      "Run 1, Epoch 125, val_loss = 0.0365\n",
      "Run 1, Epoch 126, val_loss = 0.0428\n",
      "Run 1, Epoch 127, val_loss = 0.0339\n",
      "Run 1, Epoch 128, val_loss = 0.0364\n",
      "Run 1, Epoch 129, val_loss = 0.0386\n",
      "Run 1, Epoch 130, val_loss = 0.0384\n",
      "Run 1, Epoch 131, val_loss = 0.0455\n",
      "Run 1, Epoch 132, val_loss = 0.0360\n",
      "Run 1, Epoch 133, val_loss = 0.0297\n",
      "Run 1, Epoch 134, val_loss = 0.0349\n",
      "Run 1, Epoch 135, val_loss = 0.0334\n",
      "Run 1, Epoch 136, val_loss = 0.0348\n",
      "Run 1, Epoch 137, val_loss = 0.0322\n",
      "Run 1, Epoch 138, val_loss = 0.0300\n",
      "Run 1, Epoch 139, val_loss = 0.0288\n",
      "Run 1, Epoch 140, val_loss = 0.0292\n",
      "Run 1, Epoch 141, val_loss = 0.0295\n",
      "Run 1, Epoch 142, val_loss = 0.0299\n",
      "Run 1, Epoch 143, val_loss = 0.0397\n",
      "Run 1, Epoch 144, val_loss = 0.0376\n",
      "Run 1, Epoch 145, val_loss = 0.0422\n",
      "Run 1, Epoch 146, val_loss = 0.0355\n",
      "Run 1, Epoch 147, val_loss = 0.0310\n",
      "Run 1, Epoch 148, val_loss = 0.0288\n",
      "Run 1, Epoch 149, val_loss = 0.0285\n",
      "Run 1, Epoch 150, val_loss = 0.0269\n",
      "Run 1, Epoch 151, val_loss = 0.0324\n",
      "Run 1, Epoch 152, val_loss = 0.0329\n",
      "Run 1, Epoch 153, val_loss = 0.0314\n",
      "Run 1, Epoch 154, val_loss = 0.0316\n",
      "Run 1, Epoch 155, val_loss = 0.0316\n",
      "Run 1, Epoch 156, val_loss = 0.0299\n",
      "Run 1, Epoch 157, val_loss = 0.0351\n",
      "Run 1, Epoch 158, val_loss = 0.0345\n",
      "Run 1, Epoch 159, val_loss = 0.0316\n",
      "Run 1, Epoch 160, val_loss = 0.0315\n",
      "Run 1, Epoch 161, val_loss = 0.0324\n",
      "Run 1, Epoch 162, val_loss = 0.0331\n",
      "Run 1, Epoch 163, val_loss = 0.0306\n",
      "Run 1, Epoch 164, val_loss = 0.0299\n",
      "Run 1, Epoch 165, val_loss = 0.0334\n",
      "Run 1, Epoch 166, val_loss = 0.0296\n",
      "Run 1, Epoch 167, val_loss = 0.0369\n",
      "Run 1, Epoch 168, val_loss = 0.0323\n",
      "Run 1, Epoch 169, val_loss = 0.0348\n",
      "Run 1, Epoch 170, val_loss = 0.0348\n",
      "✅ Run 1: Acc = 99.75%, MDE = 0.0123\n",
      "Run 2/5 - Alpha = 0.4\n",
      "Run 2, Epoch 1, val_loss = 1.6471\n",
      "Run 2, Epoch 2, val_loss = 0.7247\n",
      "Run 2, Epoch 3, val_loss = 0.5997\n",
      "Run 2, Epoch 4, val_loss = 0.3934\n",
      "Run 2, Epoch 5, val_loss = 0.3746\n",
      "Run 2, Epoch 6, val_loss = 0.3168\n",
      "Run 2, Epoch 7, val_loss = 0.2452\n",
      "Run 2, Epoch 8, val_loss = 0.2497\n",
      "Run 2, Epoch 9, val_loss = 0.2704\n",
      "Run 2, Epoch 10, val_loss = 0.1903\n",
      "Run 2, Epoch 11, val_loss = 0.2709\n",
      "Run 2, Epoch 12, val_loss = 0.1898\n",
      "Run 2, Epoch 13, val_loss = 0.2391\n",
      "Run 2, Epoch 14, val_loss = 0.1683\n",
      "Run 2, Epoch 15, val_loss = 0.1979\n",
      "Run 2, Epoch 16, val_loss = 0.1578\n",
      "Run 2, Epoch 17, val_loss = 0.1333\n",
      "Run 2, Epoch 18, val_loss = 0.1139\n",
      "Run 2, Epoch 19, val_loss = 0.1275\n",
      "Run 2, Epoch 20, val_loss = 0.1038\n",
      "Run 2, Epoch 21, val_loss = 0.1147\n",
      "Run 2, Epoch 22, val_loss = 0.1062\n",
      "Run 2, Epoch 23, val_loss = 0.0914\n",
      "Run 2, Epoch 24, val_loss = 0.1361\n",
      "Run 2, Epoch 25, val_loss = 0.0921\n",
      "Run 2, Epoch 26, val_loss = 0.1085\n",
      "Run 2, Epoch 27, val_loss = 0.0913\n",
      "Run 2, Epoch 28, val_loss = 0.0924\n",
      "Run 2, Epoch 29, val_loss = 0.0884\n",
      "Run 2, Epoch 30, val_loss = 0.0798\n",
      "Run 2, Epoch 31, val_loss = 0.0868\n",
      "Run 2, Epoch 32, val_loss = 0.0908\n",
      "Run 2, Epoch 33, val_loss = 0.0813\n",
      "Run 2, Epoch 34, val_loss = 0.0949\n",
      "Run 2, Epoch 35, val_loss = 0.0764\n",
      "Run 2, Epoch 36, val_loss = 0.0880\n",
      "Run 2, Epoch 37, val_loss = 0.0898\n",
      "Run 2, Epoch 38, val_loss = 0.0934\n",
      "Run 2, Epoch 39, val_loss = 0.0736\n",
      "Run 2, Epoch 40, val_loss = 0.0770\n",
      "Run 2, Epoch 41, val_loss = 0.0755\n",
      "Run 2, Epoch 42, val_loss = 0.0718\n",
      "Run 2, Epoch 43, val_loss = 0.0596\n",
      "Run 2, Epoch 44, val_loss = 0.0682\n",
      "Run 2, Epoch 45, val_loss = 0.0583\n",
      "Run 2, Epoch 46, val_loss = 0.0516\n",
      "Run 2, Epoch 47, val_loss = 0.0740\n",
      "Run 2, Epoch 48, val_loss = 0.0559\n",
      "Run 2, Epoch 49, val_loss = 0.0551\n",
      "Run 2, Epoch 50, val_loss = 0.0534\n",
      "Run 2, Epoch 51, val_loss = 0.0544\n",
      "Run 2, Epoch 52, val_loss = 0.0532\n",
      "Run 2, Epoch 53, val_loss = 0.0595\n",
      "Run 2, Epoch 54, val_loss = 0.0695\n",
      "Run 2, Epoch 55, val_loss = 0.0438\n",
      "Run 2, Epoch 56, val_loss = 0.0559\n",
      "Run 2, Epoch 57, val_loss = 0.0573\n",
      "Run 2, Epoch 58, val_loss = 0.0524\n",
      "Run 2, Epoch 59, val_loss = 0.0503\n",
      "Run 2, Epoch 60, val_loss = 0.0414\n",
      "Run 2, Epoch 61, val_loss = 0.0458\n",
      "Run 2, Epoch 62, val_loss = 0.0906\n",
      "Run 2, Epoch 63, val_loss = 0.0480\n",
      "Run 2, Epoch 64, val_loss = 0.0643\n",
      "Run 2, Epoch 65, val_loss = 0.0589\n",
      "Run 2, Epoch 66, val_loss = 0.0602\n",
      "Run 2, Epoch 67, val_loss = 0.0563\n",
      "Run 2, Epoch 68, val_loss = 0.0479\n",
      "Run 2, Epoch 69, val_loss = 0.0593\n",
      "Run 2, Epoch 70, val_loss = 0.0612\n",
      "Run 2, Epoch 71, val_loss = 0.0698\n",
      "Run 2, Epoch 72, val_loss = 0.0461\n",
      "Run 2, Epoch 73, val_loss = 0.0579\n",
      "Run 2, Epoch 74, val_loss = 0.0542\n",
      "Run 2, Epoch 75, val_loss = 0.0573\n",
      "Run 2, Epoch 76, val_loss = 0.0496\n",
      "Run 2, Epoch 77, val_loss = 0.0350\n",
      "Run 2, Epoch 78, val_loss = 0.0409\n",
      "Run 2, Epoch 79, val_loss = 0.0454\n",
      "Run 2, Epoch 80, val_loss = 0.0360\n",
      "Run 2, Epoch 81, val_loss = 0.0402\n",
      "Run 2, Epoch 82, val_loss = 0.0378\n",
      "Run 2, Epoch 83, val_loss = 0.0456\n",
      "Run 2, Epoch 84, val_loss = 0.0379\n",
      "Run 2, Epoch 85, val_loss = 0.0446\n",
      "Run 2, Epoch 86, val_loss = 0.0419\n",
      "Run 2, Epoch 87, val_loss = 0.0367\n",
      "Run 2, Epoch 88, val_loss = 0.0344\n",
      "Run 2, Epoch 89, val_loss = 0.0352\n",
      "Run 2, Epoch 90, val_loss = 0.0415\n",
      "Run 2, Epoch 91, val_loss = 0.0386\n",
      "Run 2, Epoch 92, val_loss = 0.0361\n",
      "Run 2, Epoch 93, val_loss = 0.0301\n",
      "Run 2, Epoch 94, val_loss = 0.0408\n",
      "Run 2, Epoch 95, val_loss = 0.0280\n",
      "Run 2, Epoch 96, val_loss = 0.0334\n",
      "Run 2, Epoch 97, val_loss = 0.0547\n",
      "Run 2, Epoch 98, val_loss = 0.0312\n",
      "Run 2, Epoch 99, val_loss = 0.0317\n",
      "Run 2, Epoch 100, val_loss = 0.0319\n",
      "Run 2, Epoch 101, val_loss = 0.0345\n",
      "Run 2, Epoch 102, val_loss = 0.0338\n",
      "Run 2, Epoch 103, val_loss = 0.0358\n",
      "Run 2, Epoch 104, val_loss = 0.0331\n",
      "Run 2, Epoch 105, val_loss = 0.0364\n",
      "Run 2, Epoch 106, val_loss = 0.0345\n",
      "Run 2, Epoch 107, val_loss = 0.0405\n",
      "Run 2, Epoch 108, val_loss = 0.0342\n",
      "Run 2, Epoch 109, val_loss = 0.0357\n",
      "Run 2, Epoch 110, val_loss = 0.0314\n",
      "Run 2, Epoch 111, val_loss = 0.0279\n",
      "Run 2, Epoch 112, val_loss = 0.0542\n",
      "Run 2, Epoch 113, val_loss = 0.0336\n",
      "Run 2, Epoch 114, val_loss = 0.0397\n",
      "Run 2, Epoch 115, val_loss = 0.0338\n",
      "Run 2, Epoch 116, val_loss = 0.0405\n",
      "Run 2, Epoch 117, val_loss = 0.0399\n",
      "Run 2, Epoch 118, val_loss = 0.0258\n",
      "Run 2, Epoch 119, val_loss = 0.0307\n",
      "Run 2, Epoch 120, val_loss = 0.0336\n",
      "Run 2, Epoch 121, val_loss = 0.0325\n",
      "Run 2, Epoch 122, val_loss = 0.0329\n",
      "Run 2, Epoch 123, val_loss = 0.0394\n",
      "Run 2, Epoch 124, val_loss = 0.0297\n",
      "Run 2, Epoch 125, val_loss = 0.0379\n",
      "Run 2, Epoch 126, val_loss = 0.0290\n",
      "Run 2, Epoch 127, val_loss = 0.0356\n",
      "Run 2, Epoch 128, val_loss = 0.0552\n",
      "Run 2, Epoch 129, val_loss = 0.0358\n",
      "Run 2, Epoch 130, val_loss = 0.0375\n",
      "Run 2, Epoch 131, val_loss = 0.0323\n",
      "Run 2, Epoch 132, val_loss = 0.0285\n",
      "Run 2, Epoch 133, val_loss = 0.0359\n",
      "Run 2, Epoch 134, val_loss = 0.0396\n",
      "Run 2, Epoch 135, val_loss = 0.0331\n",
      "Run 2, Epoch 136, val_loss = 0.0338\n",
      "Run 2, Epoch 137, val_loss = 0.0313\n",
      "Run 2, Epoch 138, val_loss = 0.0308\n",
      "✅ Run 2: Acc = 99.59%, MDE = 0.0119\n",
      "Run 3/5 - Alpha = 0.4\n",
      "Run 3, Epoch 1, val_loss = 2.1493\n",
      "Run 3, Epoch 2, val_loss = 1.0483\n",
      "Run 3, Epoch 3, val_loss = 0.7183\n",
      "Run 3, Epoch 4, val_loss = 0.5783\n",
      "Run 3, Epoch 5, val_loss = 0.4361\n",
      "Run 3, Epoch 6, val_loss = 0.4030\n",
      "Run 3, Epoch 7, val_loss = 0.3605\n",
      "Run 3, Epoch 8, val_loss = 0.3000\n",
      "Run 3, Epoch 9, val_loss = 0.3162\n",
      "Run 3, Epoch 10, val_loss = 0.2168\n",
      "Run 3, Epoch 11, val_loss = 0.2769\n",
      "Run 3, Epoch 12, val_loss = 0.1832\n",
      "Run 3, Epoch 13, val_loss = 0.1929\n",
      "Run 3, Epoch 14, val_loss = 0.1903\n",
      "Run 3, Epoch 15, val_loss = 0.1846\n",
      "Run 3, Epoch 16, val_loss = 0.1666\n",
      "Run 3, Epoch 17, val_loss = 0.1473\n",
      "Run 3, Epoch 18, val_loss = 0.1961\n",
      "Run 3, Epoch 19, val_loss = 0.1690\n",
      "Run 3, Epoch 20, val_loss = 0.1328\n",
      "Run 3, Epoch 21, val_loss = 0.1242\n",
      "Run 3, Epoch 22, val_loss = 0.1219\n",
      "Run 3, Epoch 23, val_loss = 0.1229\n",
      "Run 3, Epoch 24, val_loss = 0.0966\n",
      "Run 3, Epoch 25, val_loss = 0.1424\n",
      "Run 3, Epoch 26, val_loss = 0.0946\n",
      "Run 3, Epoch 27, val_loss = 0.0962\n",
      "Run 3, Epoch 28, val_loss = 0.1384\n",
      "Run 3, Epoch 29, val_loss = 0.0881\n",
      "Run 3, Epoch 30, val_loss = 0.0917\n",
      "Run 3, Epoch 31, val_loss = 0.1209\n",
      "Run 3, Epoch 32, val_loss = 0.1434\n",
      "Run 3, Epoch 33, val_loss = 0.1072\n",
      "Run 3, Epoch 34, val_loss = 0.1143\n",
      "Run 3, Epoch 35, val_loss = 0.0891\n",
      "Run 3, Epoch 36, val_loss = 0.0947\n",
      "Run 3, Epoch 37, val_loss = 0.1126\n",
      "Run 3, Epoch 38, val_loss = 0.1066\n",
      "Run 3, Epoch 39, val_loss = 0.0788\n",
      "Run 3, Epoch 40, val_loss = 0.0728\n",
      "Run 3, Epoch 41, val_loss = 0.0813\n",
      "Run 3, Epoch 42, val_loss = 0.1283\n",
      "Run 3, Epoch 43, val_loss = 0.0724\n",
      "Run 3, Epoch 44, val_loss = 0.0939\n",
      "Run 3, Epoch 45, val_loss = 0.0678\n",
      "Run 3, Epoch 46, val_loss = 0.0959\n",
      "Run 3, Epoch 47, val_loss = 0.0789\n",
      "Run 3, Epoch 48, val_loss = 0.0760\n",
      "Run 3, Epoch 49, val_loss = 0.0726\n",
      "Run 3, Epoch 50, val_loss = 0.0639\n",
      "Run 3, Epoch 51, val_loss = 0.0594\n",
      "Run 3, Epoch 52, val_loss = 0.0856\n",
      "Run 3, Epoch 53, val_loss = 0.0789\n",
      "Run 3, Epoch 54, val_loss = 0.0963\n",
      "Run 3, Epoch 55, val_loss = 0.0796\n",
      "Run 3, Epoch 56, val_loss = 0.0701\n",
      "Run 3, Epoch 57, val_loss = 0.0642\n",
      "Run 3, Epoch 58, val_loss = 0.1090\n",
      "Run 3, Epoch 59, val_loss = 0.0637\n",
      "Run 3, Epoch 60, val_loss = 0.0619\n",
      "Run 3, Epoch 61, val_loss = 0.0629\n",
      "Run 3, Epoch 62, val_loss = 0.0542\n",
      "Run 3, Epoch 63, val_loss = 0.0712\n",
      "Run 3, Epoch 64, val_loss = 0.0505\n",
      "Run 3, Epoch 65, val_loss = 0.0562\n",
      "Run 3, Epoch 66, val_loss = 0.0426\n",
      "Run 3, Epoch 67, val_loss = 0.0536\n",
      "Run 3, Epoch 68, val_loss = 0.0838\n",
      "Run 3, Epoch 69, val_loss = 0.0461\n",
      "Run 3, Epoch 70, val_loss = 0.0691\n",
      "Run 3, Epoch 71, val_loss = 0.0779\n",
      "Run 3, Epoch 72, val_loss = 0.0571\n",
      "Run 3, Epoch 73, val_loss = 0.0853\n",
      "Run 3, Epoch 74, val_loss = 0.0783\n",
      "Run 3, Epoch 75, val_loss = 0.0592\n",
      "Run 3, Epoch 76, val_loss = 0.0628\n",
      "Run 3, Epoch 77, val_loss = 0.0646\n",
      "Run 3, Epoch 78, val_loss = 0.0801\n",
      "Run 3, Epoch 79, val_loss = 0.0629\n",
      "Run 3, Epoch 80, val_loss = 0.0547\n",
      "Run 3, Epoch 81, val_loss = 0.0447\n",
      "Run 3, Epoch 82, val_loss = 0.0615\n",
      "Run 3, Epoch 83, val_loss = 0.0423\n",
      "Run 3, Epoch 84, val_loss = 0.0627\n",
      "Run 3, Epoch 85, val_loss = 0.0537\n",
      "Run 3, Epoch 86, val_loss = 0.0567\n",
      "Run 3, Epoch 87, val_loss = 0.0402\n",
      "Run 3, Epoch 88, val_loss = 0.0537\n",
      "Run 3, Epoch 89, val_loss = 0.0469\n",
      "Run 3, Epoch 90, val_loss = 0.0480\n",
      "Run 3, Epoch 91, val_loss = 0.0627\n",
      "Run 3, Epoch 92, val_loss = 0.0430\n",
      "Run 3, Epoch 93, val_loss = 0.0494\n",
      "Run 3, Epoch 94, val_loss = 0.0438\n",
      "Run 3, Epoch 95, val_loss = 0.0516\n",
      "Run 3, Epoch 96, val_loss = 0.0553\n",
      "Run 3, Epoch 97, val_loss = 0.0471\n",
      "Run 3, Epoch 98, val_loss = 0.0459\n",
      "Run 3, Epoch 99, val_loss = 0.0720\n",
      "Run 3, Epoch 100, val_loss = 0.0377\n",
      "Run 3, Epoch 101, val_loss = 0.0372\n",
      "Run 3, Epoch 102, val_loss = 0.0481\n",
      "Run 3, Epoch 103, val_loss = 0.0355\n",
      "Run 3, Epoch 104, val_loss = 0.0435\n",
      "Run 3, Epoch 105, val_loss = 0.0502\n",
      "Run 3, Epoch 106, val_loss = 0.0529\n",
      "Run 3, Epoch 107, val_loss = 0.0477\n",
      "Run 3, Epoch 108, val_loss = 0.0395\n",
      "Run 3, Epoch 109, val_loss = 0.0440\n",
      "Run 3, Epoch 110, val_loss = 0.0517\n",
      "Run 3, Epoch 111, val_loss = 0.0545\n",
      "Run 3, Epoch 112, val_loss = 0.0404\n",
      "Run 3, Epoch 113, val_loss = 0.0410\n",
      "Run 3, Epoch 114, val_loss = 0.0393\n",
      "Run 3, Epoch 115, val_loss = 0.0474\n",
      "Run 3, Epoch 116, val_loss = 0.0324\n",
      "Run 3, Epoch 117, val_loss = 0.0640\n",
      "Run 3, Epoch 118, val_loss = 0.0380\n",
      "Run 3, Epoch 119, val_loss = 0.0479\n",
      "Run 3, Epoch 120, val_loss = 0.0437\n",
      "Run 3, Epoch 121, val_loss = 0.0407\n",
      "Run 3, Epoch 122, val_loss = 0.0474\n",
      "Run 3, Epoch 123, val_loss = 0.0412\n",
      "Run 3, Epoch 124, val_loss = 0.0620\n",
      "Run 3, Epoch 125, val_loss = 0.0553\n",
      "Run 3, Epoch 126, val_loss = 0.0457\n",
      "Run 3, Epoch 127, val_loss = 0.0497\n",
      "Run 3, Epoch 128, val_loss = 0.0491\n",
      "Run 3, Epoch 129, val_loss = 0.0480\n",
      "Run 3, Epoch 130, val_loss = 0.0434\n",
      "Run 3, Epoch 131, val_loss = 0.0401\n",
      "Run 3, Epoch 132, val_loss = 0.0328\n",
      "Run 3, Epoch 133, val_loss = 0.0419\n",
      "Run 3, Epoch 134, val_loss = 0.0367\n",
      "Run 3, Epoch 135, val_loss = 0.0362\n",
      "Run 3, Epoch 136, val_loss = 0.0396\n",
      "✅ Run 3: Acc = 99.64%, MDE = 0.0107\n",
      "Run 4/5 - Alpha = 0.4\n",
      "Run 4, Epoch 1, val_loss = 2.1563\n",
      "Run 4, Epoch 2, val_loss = 1.0251\n",
      "Run 4, Epoch 3, val_loss = 0.6077\n",
      "Run 4, Epoch 4, val_loss = 0.5206\n",
      "Run 4, Epoch 5, val_loss = 0.4111\n",
      "Run 4, Epoch 6, val_loss = 0.3355\n",
      "Run 4, Epoch 7, val_loss = 0.3200\n",
      "Run 4, Epoch 8, val_loss = 0.3029\n",
      "Run 4, Epoch 9, val_loss = 0.2715\n",
      "Run 4, Epoch 10, val_loss = 0.2579\n",
      "Run 4, Epoch 11, val_loss = 0.2350\n",
      "Run 4, Epoch 12, val_loss = 0.2238\n",
      "Run 4, Epoch 13, val_loss = 0.2038\n",
      "Run 4, Epoch 14, val_loss = 0.2066\n",
      "Run 4, Epoch 15, val_loss = 0.1689\n",
      "Run 4, Epoch 16, val_loss = 0.1741\n",
      "Run 4, Epoch 17, val_loss = 0.2148\n",
      "Run 4, Epoch 18, val_loss = 0.2258\n",
      "Run 4, Epoch 19, val_loss = 0.1682\n",
      "Run 4, Epoch 20, val_loss = 0.1497\n",
      "Run 4, Epoch 21, val_loss = 0.1246\n",
      "Run 4, Epoch 22, val_loss = 0.1223\n",
      "Run 4, Epoch 23, val_loss = 0.1110\n",
      "Run 4, Epoch 24, val_loss = 0.1287\n",
      "Run 4, Epoch 25, val_loss = 0.1119\n",
      "Run 4, Epoch 26, val_loss = 0.1135\n",
      "Run 4, Epoch 27, val_loss = 0.0780\n",
      "Run 4, Epoch 28, val_loss = 0.0903\n",
      "Run 4, Epoch 29, val_loss = 0.1147\n",
      "Run 4, Epoch 30, val_loss = 0.0834\n",
      "Run 4, Epoch 31, val_loss = 0.1018\n",
      "Run 4, Epoch 32, val_loss = 0.1255\n",
      "Run 4, Epoch 33, val_loss = 0.0899\n",
      "Run 4, Epoch 34, val_loss = 0.0885\n",
      "Run 4, Epoch 35, val_loss = 0.0805\n",
      "Run 4, Epoch 36, val_loss = 0.0842\n",
      "Run 4, Epoch 37, val_loss = 0.0853\n",
      "Run 4, Epoch 38, val_loss = 0.0722\n",
      "Run 4, Epoch 39, val_loss = 0.0813\n",
      "Run 4, Epoch 40, val_loss = 0.0627\n",
      "Run 4, Epoch 41, val_loss = 0.0710\n",
      "Run 4, Epoch 42, val_loss = 0.0908\n",
      "Run 4, Epoch 43, val_loss = 0.0669\n",
      "Run 4, Epoch 44, val_loss = 0.0918\n",
      "Run 4, Epoch 45, val_loss = 0.0674\n",
      "Run 4, Epoch 46, val_loss = 0.0506\n",
      "Run 4, Epoch 47, val_loss = 0.0567\n",
      "Run 4, Epoch 48, val_loss = 0.0545\n",
      "Run 4, Epoch 49, val_loss = 0.0875\n",
      "Run 4, Epoch 50, val_loss = 0.0606\n",
      "Run 4, Epoch 51, val_loss = 0.0574\n",
      "Run 4, Epoch 52, val_loss = 0.0650\n",
      "Run 4, Epoch 53, val_loss = 0.0746\n",
      "Run 4, Epoch 54, val_loss = 0.0741\n",
      "Run 4, Epoch 55, val_loss = 0.0837\n",
      "Run 4, Epoch 56, val_loss = 0.0584\n",
      "Run 4, Epoch 57, val_loss = 0.0774\n",
      "Run 4, Epoch 58, val_loss = 0.0602\n",
      "Run 4, Epoch 59, val_loss = 0.0818\n",
      "Run 4, Epoch 60, val_loss = 0.0788\n",
      "Run 4, Epoch 61, val_loss = 0.0624\n",
      "Run 4, Epoch 62, val_loss = 0.0538\n",
      "Run 4, Epoch 63, val_loss = 0.0493\n",
      "Run 4, Epoch 64, val_loss = 0.0551\n",
      "Run 4, Epoch 65, val_loss = 0.0491\n",
      "Run 4, Epoch 66, val_loss = 0.0387\n",
      "Run 4, Epoch 67, val_loss = 0.0373\n",
      "Run 4, Epoch 68, val_loss = 0.0450\n",
      "Run 4, Epoch 69, val_loss = 0.0412\n",
      "Run 4, Epoch 70, val_loss = 0.0370\n",
      "Run 4, Epoch 71, val_loss = 0.0363\n",
      "Run 4, Epoch 72, val_loss = 0.0309\n",
      "Run 4, Epoch 73, val_loss = 0.0376\n",
      "Run 4, Epoch 74, val_loss = 0.0394\n",
      "Run 4, Epoch 75, val_loss = 0.0443\n",
      "Run 4, Epoch 76, val_loss = 0.0436\n",
      "Run 4, Epoch 77, val_loss = 0.0392\n",
      "Run 4, Epoch 78, val_loss = 0.0387\n",
      "Run 4, Epoch 79, val_loss = 0.0416\n",
      "Run 4, Epoch 80, val_loss = 0.0363\n",
      "Run 4, Epoch 81, val_loss = 0.0432\n",
      "Run 4, Epoch 82, val_loss = 0.0431\n",
      "Run 4, Epoch 83, val_loss = 0.0378\n",
      "Run 4, Epoch 84, val_loss = 0.0384\n",
      "Run 4, Epoch 85, val_loss = 0.0436\n",
      "Run 4, Epoch 86, val_loss = 0.0381\n",
      "Run 4, Epoch 87, val_loss = 0.0560\n",
      "Run 4, Epoch 88, val_loss = 0.0391\n",
      "Run 4, Epoch 89, val_loss = 0.0625\n",
      "Run 4, Epoch 90, val_loss = 0.0574\n",
      "Run 4, Epoch 91, val_loss = 0.0510\n",
      "Run 4, Epoch 92, val_loss = 0.0400\n",
      "✅ Run 4: Acc = 99.64%, MDE = 0.0109\n",
      "Run 5/5 - Alpha = 0.4\n",
      "Run 5, Epoch 1, val_loss = 2.1103\n",
      "Run 5, Epoch 2, val_loss = 0.8920\n",
      "Run 5, Epoch 3, val_loss = 0.6576\n",
      "Run 5, Epoch 4, val_loss = 0.4949\n",
      "Run 5, Epoch 5, val_loss = 0.4407\n",
      "Run 5, Epoch 6, val_loss = 0.3552\n",
      "Run 5, Epoch 7, val_loss = 0.3686\n",
      "Run 5, Epoch 8, val_loss = 0.2848\n",
      "Run 5, Epoch 9, val_loss = 0.2727\n",
      "Run 5, Epoch 10, val_loss = 0.2618\n",
      "Run 5, Epoch 11, val_loss = 0.2367\n",
      "Run 5, Epoch 12, val_loss = 0.1763\n",
      "Run 5, Epoch 13, val_loss = 0.1733\n",
      "Run 5, Epoch 14, val_loss = 0.1852\n",
      "Run 5, Epoch 15, val_loss = 0.1772\n",
      "Run 5, Epoch 16, val_loss = 0.1606\n",
      "Run 5, Epoch 17, val_loss = 0.1557\n",
      "Run 5, Epoch 18, val_loss = 0.1861\n",
      "Run 5, Epoch 19, val_loss = 0.1389\n",
      "Run 5, Epoch 20, val_loss = 0.1323\n",
      "Run 5, Epoch 21, val_loss = 0.1253\n",
      "Run 5, Epoch 22, val_loss = 0.1297\n",
      "Run 5, Epoch 23, val_loss = 0.1268\n",
      "Run 5, Epoch 24, val_loss = 0.1225\n",
      "Run 5, Epoch 25, val_loss = 0.1295\n",
      "Run 5, Epoch 26, val_loss = 0.0911\n",
      "Run 5, Epoch 27, val_loss = 0.1362\n",
      "Run 5, Epoch 28, val_loss = 0.0992\n",
      "Run 5, Epoch 29, val_loss = 0.1341\n",
      "Run 5, Epoch 30, val_loss = 0.0947\n",
      "Run 5, Epoch 31, val_loss = 0.1351\n",
      "Run 5, Epoch 32, val_loss = 0.1325\n",
      "Run 5, Epoch 33, val_loss = 0.1199\n",
      "Run 5, Epoch 34, val_loss = 0.0995\n",
      "Run 5, Epoch 35, val_loss = 0.0772\n",
      "Run 5, Epoch 36, val_loss = 0.0786\n",
      "Run 5, Epoch 37, val_loss = 0.0799\n",
      "Run 5, Epoch 38, val_loss = 0.0798\n",
      "Run 5, Epoch 39, val_loss = 0.0857\n",
      "Run 5, Epoch 40, val_loss = 0.0743\n",
      "Run 5, Epoch 41, val_loss = 0.0898\n",
      "Run 5, Epoch 42, val_loss = 0.1186\n",
      "Run 5, Epoch 43, val_loss = 0.0785\n",
      "Run 5, Epoch 44, val_loss = 0.0816\n",
      "Run 5, Epoch 45, val_loss = 0.0961\n",
      "Run 5, Epoch 46, val_loss = 0.1117\n",
      "Run 5, Epoch 47, val_loss = 0.0748\n",
      "Run 5, Epoch 48, val_loss = 0.0945\n",
      "Run 5, Epoch 49, val_loss = 0.0882\n",
      "Run 5, Epoch 50, val_loss = 0.0753\n",
      "Run 5, Epoch 51, val_loss = 0.0694\n",
      "Run 5, Epoch 52, val_loss = 0.0701\n",
      "Run 5, Epoch 53, val_loss = 0.0665\n",
      "Run 5, Epoch 54, val_loss = 0.0694\n",
      "Run 5, Epoch 55, val_loss = 0.0672\n",
      "Run 5, Epoch 56, val_loss = 0.0674\n",
      "Run 5, Epoch 57, val_loss = 0.0685\n",
      "Run 5, Epoch 58, val_loss = 0.0774\n",
      "Run 5, Epoch 59, val_loss = 0.0976\n",
      "Run 5, Epoch 60, val_loss = 0.0589\n",
      "Run 5, Epoch 61, val_loss = 0.0729\n",
      "Run 5, Epoch 62, val_loss = 0.0614\n",
      "Run 5, Epoch 63, val_loss = 0.0643\n",
      "Run 5, Epoch 64, val_loss = 0.0540\n",
      "Run 5, Epoch 65, val_loss = 0.0560\n",
      "Run 5, Epoch 66, val_loss = 0.0531\n",
      "Run 5, Epoch 67, val_loss = 0.0612\n",
      "Run 5, Epoch 68, val_loss = 0.0774\n",
      "Run 5, Epoch 69, val_loss = 0.0500\n",
      "Run 5, Epoch 70, val_loss = 0.0477\n",
      "Run 5, Epoch 71, val_loss = 0.0870\n",
      "Run 5, Epoch 72, val_loss = 0.0666\n",
      "Run 5, Epoch 73, val_loss = 0.0768\n",
      "Run 5, Epoch 74, val_loss = 0.0568\n",
      "Run 5, Epoch 75, val_loss = 0.1156\n",
      "Run 5, Epoch 76, val_loss = 0.0550\n",
      "Run 5, Epoch 77, val_loss = 0.0650\n",
      "Run 5, Epoch 78, val_loss = 0.0551\n",
      "Run 5, Epoch 79, val_loss = 0.0619\n",
      "Run 5, Epoch 80, val_loss = 0.0418\n",
      "Run 5, Epoch 81, val_loss = 0.0699\n",
      "Run 5, Epoch 82, val_loss = 0.0514\n",
      "Run 5, Epoch 83, val_loss = 0.0525\n",
      "Run 5, Epoch 84, val_loss = 0.0640\n",
      "Run 5, Epoch 85, val_loss = 0.0508\n",
      "Run 5, Epoch 86, val_loss = 0.0621\n",
      "Run 5, Epoch 87, val_loss = 0.0478\n",
      "Run 5, Epoch 88, val_loss = 0.0535\n",
      "Run 5, Epoch 89, val_loss = 0.0509\n",
      "Run 5, Epoch 90, val_loss = 0.0631\n",
      "Run 5, Epoch 91, val_loss = 0.0808\n",
      "Run 5, Epoch 92, val_loss = 0.0379\n",
      "Run 5, Epoch 93, val_loss = 0.0651\n",
      "Run 5, Epoch 94, val_loss = 0.0532\n",
      "Run 5, Epoch 95, val_loss = 0.0489\n",
      "Run 5, Epoch 96, val_loss = 0.0446\n",
      "Run 5, Epoch 97, val_loss = 0.0619\n",
      "Run 5, Epoch 98, val_loss = 0.0514\n",
      "Run 5, Epoch 99, val_loss = 0.0422\n",
      "Run 5, Epoch 100, val_loss = 0.0459\n",
      "Run 5, Epoch 101, val_loss = 0.0818\n",
      "Run 5, Epoch 102, val_loss = 0.0388\n",
      "Run 5, Epoch 103, val_loss = 0.0492\n",
      "Run 5, Epoch 104, val_loss = 0.0411\n",
      "Run 5, Epoch 105, val_loss = 0.0483\n",
      "Run 5, Epoch 106, val_loss = 0.0515\n",
      "Run 5, Epoch 107, val_loss = 0.0370\n",
      "Run 5, Epoch 108, val_loss = 0.0489\n",
      "Run 5, Epoch 109, val_loss = 0.0399\n",
      "Run 5, Epoch 110, val_loss = 0.0373\n",
      "Run 5, Epoch 111, val_loss = 0.0446\n",
      "Run 5, Epoch 112, val_loss = 0.0409\n",
      "Run 5, Epoch 113, val_loss = 0.0411\n",
      "Run 5, Epoch 114, val_loss = 0.0406\n",
      "Run 5, Epoch 115, val_loss = 0.0503\n",
      "Run 5, Epoch 116, val_loss = 0.0647\n",
      "Run 5, Epoch 117, val_loss = 0.0504\n",
      "Run 5, Epoch 118, val_loss = 0.0411\n",
      "Run 5, Epoch 119, val_loss = 0.0371\n",
      "Run 5, Epoch 120, val_loss = 0.0489\n",
      "Run 5, Epoch 121, val_loss = 0.0471\n",
      "Run 5, Epoch 122, val_loss = 0.0321\n",
      "Run 5, Epoch 123, val_loss = 0.0380\n",
      "Run 5, Epoch 124, val_loss = 0.0424\n",
      "Run 5, Epoch 125, val_loss = 0.0392\n",
      "Run 5, Epoch 126, val_loss = 0.0443\n",
      "Run 5, Epoch 127, val_loss = 0.0391\n",
      "Run 5, Epoch 128, val_loss = 0.0378\n",
      "Run 5, Epoch 129, val_loss = 0.0466\n",
      "Run 5, Epoch 130, val_loss = 0.0471\n",
      "Run 5, Epoch 131, val_loss = 0.0457\n",
      "Run 5, Epoch 132, val_loss = 0.0419\n",
      "Run 5, Epoch 133, val_loss = 0.0517\n",
      "Run 5, Epoch 134, val_loss = 0.0377\n",
      "Run 5, Epoch 135, val_loss = 0.0482\n",
      "Run 5, Epoch 136, val_loss = 0.0469\n",
      "Run 5, Epoch 137, val_loss = 0.0570\n",
      "Run 5, Epoch 138, val_loss = 0.0439\n",
      "Run 5, Epoch 139, val_loss = 0.0324\n",
      "Run 5, Epoch 140, val_loss = 0.0281\n",
      "Run 5, Epoch 141, val_loss = 0.0404\n",
      "Run 5, Epoch 142, val_loss = 0.0310\n",
      "Run 5, Epoch 143, val_loss = 0.0488\n",
      "Run 5, Epoch 144, val_loss = 0.0322\n",
      "Run 5, Epoch 145, val_loss = 0.0333\n",
      "Run 5, Epoch 146, val_loss = 0.0346\n",
      "Run 5, Epoch 147, val_loss = 0.0310\n",
      "Run 5, Epoch 148, val_loss = 0.0304\n",
      "Run 5, Epoch 149, val_loss = 0.0357\n",
      "Run 5, Epoch 150, val_loss = 0.0349\n",
      "Run 5, Epoch 151, val_loss = 0.0265\n",
      "Run 5, Epoch 152, val_loss = 0.0318\n",
      "Run 5, Epoch 153, val_loss = 0.0319\n",
      "Run 5, Epoch 154, val_loss = 0.0304\n",
      "Run 5, Epoch 155, val_loss = 0.0359\n",
      "Run 5, Epoch 156, val_loss = 0.0317\n",
      "Run 5, Epoch 157, val_loss = 0.0297\n",
      "Run 5, Epoch 158, val_loss = 0.0322\n",
      "Run 5, Epoch 159, val_loss = 0.0340\n",
      "Run 5, Epoch 160, val_loss = 0.0351\n",
      "Run 5, Epoch 161, val_loss = 0.0409\n",
      "Run 5, Epoch 162, val_loss = 0.0297\n",
      "Run 5, Epoch 163, val_loss = 0.0288\n",
      "Run 5, Epoch 164, val_loss = 0.0342\n",
      "Run 5, Epoch 165, val_loss = 0.0298\n",
      "Run 5, Epoch 166, val_loss = 0.0300\n",
      "Run 5, Epoch 167, val_loss = 0.0512\n",
      "Run 5, Epoch 168, val_loss = 0.0311\n",
      "Run 5, Epoch 169, val_loss = 0.0485\n",
      "Run 5, Epoch 170, val_loss = 0.0332\n",
      "Run 5, Epoch 171, val_loss = 0.0346\n",
      "✅ Run 5: Acc = 99.69%, MDE = 0.0096\n",
      "📁 Results & all errors saved to repeat_copy/04\n",
      "\n",
      "[Alpha = 0.5] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.5\n",
      "Run 1, Epoch 1, val_loss = 2.8401\n",
      "Run 1, Epoch 2, val_loss = 1.3569\n",
      "Run 1, Epoch 3, val_loss = 0.8293\n",
      "Run 1, Epoch 4, val_loss = 0.8037\n",
      "Run 1, Epoch 5, val_loss = 0.6171\n",
      "Run 1, Epoch 6, val_loss = 0.4821\n",
      "Run 1, Epoch 7, val_loss = 0.4466\n",
      "Run 1, Epoch 8, val_loss = 0.3867\n",
      "Run 1, Epoch 9, val_loss = 0.3552\n",
      "Run 1, Epoch 10, val_loss = 0.2820\n",
      "Run 1, Epoch 11, val_loss = 0.3084\n",
      "Run 1, Epoch 12, val_loss = 0.4110\n",
      "Run 1, Epoch 13, val_loss = 0.2387\n",
      "Run 1, Epoch 14, val_loss = 0.2238\n",
      "Run 1, Epoch 15, val_loss = 0.2447\n",
      "Run 1, Epoch 16, val_loss = 0.1674\n",
      "Run 1, Epoch 17, val_loss = 0.1955\n",
      "Run 1, Epoch 18, val_loss = 0.1792\n",
      "Run 1, Epoch 19, val_loss = 0.1393\n",
      "Run 1, Epoch 20, val_loss = 0.1616\n",
      "Run 1, Epoch 21, val_loss = 0.1601\n",
      "Run 1, Epoch 22, val_loss = 0.1311\n",
      "Run 1, Epoch 23, val_loss = 0.1607\n",
      "Run 1, Epoch 24, val_loss = 0.2642\n",
      "Run 1, Epoch 25, val_loss = 0.1241\n",
      "Run 1, Epoch 26, val_loss = 0.1228\n",
      "Run 1, Epoch 27, val_loss = 0.1249\n",
      "Run 1, Epoch 28, val_loss = 0.1594\n",
      "Run 1, Epoch 29, val_loss = 0.1552\n",
      "Run 1, Epoch 30, val_loss = 0.1377\n",
      "Run 1, Epoch 31, val_loss = 0.1128\n",
      "Run 1, Epoch 32, val_loss = 0.1022\n",
      "Run 1, Epoch 33, val_loss = 0.0944\n",
      "Run 1, Epoch 34, val_loss = 0.1098\n",
      "Run 1, Epoch 35, val_loss = 0.0893\n",
      "Run 1, Epoch 36, val_loss = 0.0993\n",
      "Run 1, Epoch 37, val_loss = 0.0986\n",
      "Run 1, Epoch 38, val_loss = 0.1291\n",
      "Run 1, Epoch 39, val_loss = 0.0811\n",
      "Run 1, Epoch 40, val_loss = 0.1616\n",
      "Run 1, Epoch 41, val_loss = 0.0740\n",
      "Run 1, Epoch 42, val_loss = 0.1046\n",
      "Run 1, Epoch 43, val_loss = 0.0969\n",
      "Run 1, Epoch 44, val_loss = 0.0757\n",
      "Run 1, Epoch 45, val_loss = 0.0845\n",
      "Run 1, Epoch 46, val_loss = 0.0745\n",
      "Run 1, Epoch 47, val_loss = 0.0762\n",
      "Run 1, Epoch 48, val_loss = 0.0815\n",
      "Run 1, Epoch 49, val_loss = 0.0820\n",
      "Run 1, Epoch 50, val_loss = 0.0633\n",
      "Run 1, Epoch 51, val_loss = 0.0767\n",
      "Run 1, Epoch 52, val_loss = 0.0664\n",
      "Run 1, Epoch 53, val_loss = 0.0908\n",
      "Run 1, Epoch 54, val_loss = 0.0693\n",
      "Run 1, Epoch 55, val_loss = 0.0969\n",
      "Run 1, Epoch 56, val_loss = 0.0801\n",
      "Run 1, Epoch 57, val_loss = 0.0733\n",
      "Run 1, Epoch 58, val_loss = 0.0829\n",
      "Run 1, Epoch 59, val_loss = 0.0859\n",
      "Run 1, Epoch 60, val_loss = 0.0856\n",
      "Run 1, Epoch 61, val_loss = 0.0884\n",
      "Run 1, Epoch 62, val_loss = 0.0762\n",
      "Run 1, Epoch 63, val_loss = 0.0871\n",
      "Run 1, Epoch 64, val_loss = 0.1025\n",
      "Run 1, Epoch 65, val_loss = 0.0855\n",
      "Run 1, Epoch 66, val_loss = 0.0932\n",
      "Run 1, Epoch 67, val_loss = 0.0652\n",
      "Run 1, Epoch 68, val_loss = 0.0783\n",
      "Run 1, Epoch 69, val_loss = 0.0699\n",
      "Run 1, Epoch 70, val_loss = 0.0664\n",
      "✅ Run 1: Acc = 99.69%, MDE = 0.0113\n",
      "Run 2/5 - Alpha = 0.5\n",
      "Run 2, Epoch 1, val_loss = 2.3981\n",
      "Run 2, Epoch 2, val_loss = 0.9519\n",
      "Run 2, Epoch 3, val_loss = 0.7201\n",
      "Run 2, Epoch 4, val_loss = 0.6582\n",
      "Run 2, Epoch 5, val_loss = 0.4758\n",
      "Run 2, Epoch 6, val_loss = 0.3974\n",
      "Run 2, Epoch 7, val_loss = 0.3423\n",
      "Run 2, Epoch 8, val_loss = 0.3325\n",
      "Run 2, Epoch 9, val_loss = 0.3617\n",
      "Run 2, Epoch 10, val_loss = 0.4268\n",
      "Run 2, Epoch 11, val_loss = 0.3421\n",
      "Run 2, Epoch 12, val_loss = 0.2144\n",
      "Run 2, Epoch 13, val_loss = 0.1836\n",
      "Run 2, Epoch 14, val_loss = 0.2052\n",
      "Run 2, Epoch 15, val_loss = 0.1909\n",
      "Run 2, Epoch 16, val_loss = 0.1731\n",
      "Run 2, Epoch 17, val_loss = 0.2260\n",
      "Run 2, Epoch 18, val_loss = 0.1594\n",
      "Run 2, Epoch 19, val_loss = 0.1300\n",
      "Run 2, Epoch 20, val_loss = 0.1882\n",
      "Run 2, Epoch 21, val_loss = 0.1559\n",
      "Run 2, Epoch 22, val_loss = 0.1257\n",
      "Run 2, Epoch 23, val_loss = 0.1163\n",
      "Run 2, Epoch 24, val_loss = 0.1373\n",
      "Run 2, Epoch 25, val_loss = 0.1161\n",
      "Run 2, Epoch 26, val_loss = 0.1375\n",
      "Run 2, Epoch 27, val_loss = 0.1335\n",
      "Run 2, Epoch 28, val_loss = 0.1499\n",
      "Run 2, Epoch 29, val_loss = 0.0895\n",
      "Run 2, Epoch 30, val_loss = 0.0886\n",
      "Run 2, Epoch 31, val_loss = 0.1159\n",
      "Run 2, Epoch 32, val_loss = 0.1058\n",
      "Run 2, Epoch 33, val_loss = 0.1355\n",
      "Run 2, Epoch 34, val_loss = 0.0961\n",
      "Run 2, Epoch 35, val_loss = 0.0941\n",
      "Run 2, Epoch 36, val_loss = 0.0745\n",
      "Run 2, Epoch 37, val_loss = 0.0980\n",
      "Run 2, Epoch 38, val_loss = 0.1469\n",
      "Run 2, Epoch 39, val_loss = 0.0603\n",
      "Run 2, Epoch 40, val_loss = 0.0840\n",
      "Run 2, Epoch 41, val_loss = 0.0681\n",
      "Run 2, Epoch 42, val_loss = 0.1010\n",
      "Run 2, Epoch 43, val_loss = 0.0794\n",
      "Run 2, Epoch 44, val_loss = 0.0773\n",
      "Run 2, Epoch 45, val_loss = 0.0734\n",
      "Run 2, Epoch 46, val_loss = 0.0657\n",
      "Run 2, Epoch 47, val_loss = 0.0786\n",
      "Run 2, Epoch 48, val_loss = 0.0652\n",
      "Run 2, Epoch 49, val_loss = 0.0756\n",
      "Run 2, Epoch 50, val_loss = 0.0801\n",
      "Run 2, Epoch 51, val_loss = 0.0897\n",
      "Run 2, Epoch 52, val_loss = 0.1503\n",
      "Run 2, Epoch 53, val_loss = 0.0765\n",
      "Run 2, Epoch 54, val_loss = 0.0772\n",
      "Run 2, Epoch 55, val_loss = 0.0676\n",
      "Run 2, Epoch 56, val_loss = 0.0538\n",
      "Run 2, Epoch 57, val_loss = 0.0598\n",
      "Run 2, Epoch 58, val_loss = 0.0512\n",
      "Run 2, Epoch 59, val_loss = 0.1078\n",
      "Run 2, Epoch 60, val_loss = 0.0618\n",
      "Run 2, Epoch 61, val_loss = 0.0492\n",
      "Run 2, Epoch 62, val_loss = 0.0517\n",
      "Run 2, Epoch 63, val_loss = 0.0588\n",
      "Run 2, Epoch 64, val_loss = 0.0544\n",
      "Run 2, Epoch 65, val_loss = 0.0511\n",
      "Run 2, Epoch 66, val_loss = 0.0485\n",
      "Run 2, Epoch 67, val_loss = 0.0529\n",
      "Run 2, Epoch 68, val_loss = 0.0532\n",
      "Run 2, Epoch 69, val_loss = 0.0583\n",
      "Run 2, Epoch 70, val_loss = 0.0502\n",
      "Run 2, Epoch 71, val_loss = 0.0676\n",
      "Run 2, Epoch 72, val_loss = 0.0712\n",
      "Run 2, Epoch 73, val_loss = 0.0587\n",
      "Run 2, Epoch 74, val_loss = 0.0560\n",
      "Run 2, Epoch 75, val_loss = 0.0515\n",
      "Run 2, Epoch 76, val_loss = 0.0544\n",
      "Run 2, Epoch 77, val_loss = 0.0592\n",
      "Run 2, Epoch 78, val_loss = 0.0473\n",
      "Run 2, Epoch 79, val_loss = 0.0479\n",
      "Run 2, Epoch 80, val_loss = 0.0603\n",
      "Run 2, Epoch 81, val_loss = 0.0522\n",
      "Run 2, Epoch 82, val_loss = 0.0501\n",
      "Run 2, Epoch 83, val_loss = 0.0480\n",
      "Run 2, Epoch 84, val_loss = 0.0536\n",
      "Run 2, Epoch 85, val_loss = 0.0624\n",
      "Run 2, Epoch 86, val_loss = 0.0743\n",
      "Run 2, Epoch 87, val_loss = 0.0626\n",
      "Run 2, Epoch 88, val_loss = 0.0531\n",
      "Run 2, Epoch 89, val_loss = 0.0476\n",
      "Run 2, Epoch 90, val_loss = 0.0471\n",
      "Run 2, Epoch 91, val_loss = 0.0540\n",
      "Run 2, Epoch 92, val_loss = 0.0484\n",
      "Run 2, Epoch 93, val_loss = 0.0458\n",
      "Run 2, Epoch 94, val_loss = 0.0567\n",
      "Run 2, Epoch 95, val_loss = 0.0438\n",
      "Run 2, Epoch 96, val_loss = 0.0466\n",
      "Run 2, Epoch 97, val_loss = 0.0577\n",
      "Run 2, Epoch 98, val_loss = 0.0493\n",
      "Run 2, Epoch 99, val_loss = 0.0553\n",
      "Run 2, Epoch 100, val_loss = 0.0541\n",
      "Run 2, Epoch 101, val_loss = 0.0571\n",
      "Run 2, Epoch 102, val_loss = 0.0393\n",
      "Run 2, Epoch 103, val_loss = 0.0555\n",
      "Run 2, Epoch 104, val_loss = 0.0539\n",
      "Run 2, Epoch 105, val_loss = 0.0427\n",
      "Run 2, Epoch 106, val_loss = 0.0388\n",
      "Run 2, Epoch 107, val_loss = 0.0442\n",
      "Run 2, Epoch 108, val_loss = 0.0485\n",
      "Run 2, Epoch 109, val_loss = 0.0509\n",
      "Run 2, Epoch 110, val_loss = 0.0534\n",
      "Run 2, Epoch 111, val_loss = 0.0541\n",
      "Run 2, Epoch 112, val_loss = 0.0543\n",
      "Run 2, Epoch 113, val_loss = 0.0621\n",
      "Run 2, Epoch 114, val_loss = 0.0533\n",
      "Run 2, Epoch 115, val_loss = 0.0809\n",
      "Run 2, Epoch 116, val_loss = 0.0517\n",
      "Run 2, Epoch 117, val_loss = 0.0561\n",
      "Run 2, Epoch 118, val_loss = 0.0429\n",
      "Run 2, Epoch 119, val_loss = 0.0489\n",
      "Run 2, Epoch 120, val_loss = 0.0514\n",
      "Run 2, Epoch 121, val_loss = 0.0546\n",
      "Run 2, Epoch 122, val_loss = 0.0421\n",
      "Run 2, Epoch 123, val_loss = 0.0405\n",
      "Run 2, Epoch 124, val_loss = 0.0421\n",
      "Run 2, Epoch 125, val_loss = 0.0417\n",
      "Run 2, Epoch 126, val_loss = 0.0339\n",
      "Run 2, Epoch 127, val_loss = 0.0456\n",
      "Run 2, Epoch 128, val_loss = 0.0410\n",
      "Run 2, Epoch 129, val_loss = 0.0408\n",
      "Run 2, Epoch 130, val_loss = 0.0428\n",
      "Run 2, Epoch 131, val_loss = 0.0429\n",
      "Run 2, Epoch 132, val_loss = 0.0437\n",
      "Run 2, Epoch 133, val_loss = 0.0478\n",
      "Run 2, Epoch 134, val_loss = 0.0546\n",
      "Run 2, Epoch 135, val_loss = 0.0454\n",
      "Run 2, Epoch 136, val_loss = 0.0429\n",
      "Run 2, Epoch 137, val_loss = 0.0404\n",
      "Run 2, Epoch 138, val_loss = 0.0435\n",
      "Run 2, Epoch 139, val_loss = 0.0437\n",
      "Run 2, Epoch 140, val_loss = 0.0442\n",
      "Run 2, Epoch 141, val_loss = 0.0443\n",
      "Run 2, Epoch 142, val_loss = 0.0405\n",
      "Run 2, Epoch 143, val_loss = 0.0384\n",
      "Run 2, Epoch 144, val_loss = 0.0366\n",
      "Run 2, Epoch 145, val_loss = 0.0480\n",
      "Run 2, Epoch 146, val_loss = 0.0443\n",
      "✅ Run 2: Acc = 99.64%, MDE = 0.0111\n",
      "Run 3/5 - Alpha = 0.5\n",
      "Run 3, Epoch 1, val_loss = 2.4214\n",
      "Run 3, Epoch 2, val_loss = 1.1936\n",
      "Run 3, Epoch 3, val_loss = 0.7157\n",
      "Run 3, Epoch 4, val_loss = 0.6754\n",
      "Run 3, Epoch 5, val_loss = 0.6023\n",
      "Run 3, Epoch 6, val_loss = 0.4475\n",
      "Run 3, Epoch 7, val_loss = 0.4494\n",
      "Run 3, Epoch 8, val_loss = 0.4002\n",
      "Run 3, Epoch 9, val_loss = 0.2923\n",
      "Run 3, Epoch 10, val_loss = 0.2811\n",
      "Run 3, Epoch 11, val_loss = 0.2795\n",
      "Run 3, Epoch 12, val_loss = 0.1991\n",
      "Run 3, Epoch 13, val_loss = 0.2904\n",
      "Run 3, Epoch 14, val_loss = 0.1868\n",
      "Run 3, Epoch 15, val_loss = 0.2418\n",
      "Run 3, Epoch 16, val_loss = 0.1523\n",
      "Run 3, Epoch 17, val_loss = 0.2359\n",
      "Run 3, Epoch 18, val_loss = 0.1717\n",
      "Run 3, Epoch 19, val_loss = 0.1775\n",
      "Run 3, Epoch 20, val_loss = 0.1567\n",
      "Run 3, Epoch 21, val_loss = 0.1661\n",
      "Run 3, Epoch 22, val_loss = 0.1553\n",
      "Run 3, Epoch 23, val_loss = 0.1891\n",
      "Run 3, Epoch 24, val_loss = 0.1134\n",
      "Run 3, Epoch 25, val_loss = 0.1578\n",
      "Run 3, Epoch 26, val_loss = 0.1410\n",
      "Run 3, Epoch 27, val_loss = 0.1091\n",
      "Run 3, Epoch 28, val_loss = 0.1236\n",
      "Run 3, Epoch 29, val_loss = 0.1019\n",
      "Run 3, Epoch 30, val_loss = 0.1618\n",
      "Run 3, Epoch 31, val_loss = 0.0778\n",
      "Run 3, Epoch 32, val_loss = 0.1312\n",
      "Run 3, Epoch 33, val_loss = 0.1007\n",
      "Run 3, Epoch 34, val_loss = 0.1002\n",
      "Run 3, Epoch 35, val_loss = 0.1106\n",
      "Run 3, Epoch 36, val_loss = 0.0976\n",
      "Run 3, Epoch 37, val_loss = 0.0856\n",
      "Run 3, Epoch 38, val_loss = 0.1270\n",
      "Run 3, Epoch 39, val_loss = 0.1294\n",
      "Run 3, Epoch 40, val_loss = 0.0942\n",
      "Run 3, Epoch 41, val_loss = 0.0783\n",
      "Run 3, Epoch 42, val_loss = 0.0666\n",
      "Run 3, Epoch 43, val_loss = 0.0584\n",
      "Run 3, Epoch 44, val_loss = 0.1077\n",
      "Run 3, Epoch 45, val_loss = 0.0737\n",
      "Run 3, Epoch 46, val_loss = 0.0669\n",
      "Run 3, Epoch 47, val_loss = 0.0658\n",
      "Run 3, Epoch 48, val_loss = 0.0711\n",
      "Run 3, Epoch 49, val_loss = 0.0568\n",
      "Run 3, Epoch 50, val_loss = 0.0584\n",
      "Run 3, Epoch 51, val_loss = 0.0627\n",
      "Run 3, Epoch 52, val_loss = 0.0832\n",
      "Run 3, Epoch 53, val_loss = 0.0500\n",
      "Run 3, Epoch 54, val_loss = 0.0638\n",
      "Run 3, Epoch 55, val_loss = 0.0605\n",
      "Run 3, Epoch 56, val_loss = 0.0615\n",
      "Run 3, Epoch 57, val_loss = 0.0688\n",
      "Run 3, Epoch 58, val_loss = 0.0438\n",
      "Run 3, Epoch 59, val_loss = 0.0546\n",
      "Run 3, Epoch 60, val_loss = 0.0580\n",
      "Run 3, Epoch 61, val_loss = 0.0505\n",
      "Run 3, Epoch 62, val_loss = 0.0599\n",
      "Run 3, Epoch 63, val_loss = 0.0506\n",
      "Run 3, Epoch 64, val_loss = 0.0474\n",
      "Run 3, Epoch 65, val_loss = 0.0511\n",
      "Run 3, Epoch 66, val_loss = 0.0437\n",
      "Run 3, Epoch 67, val_loss = 0.0508\n",
      "Run 3, Epoch 68, val_loss = 0.0442\n",
      "Run 3, Epoch 69, val_loss = 0.0765\n",
      "Run 3, Epoch 70, val_loss = 0.0957\n",
      "Run 3, Epoch 71, val_loss = 0.0548\n",
      "Run 3, Epoch 72, val_loss = 0.0507\n",
      "Run 3, Epoch 73, val_loss = 0.0451\n",
      "Run 3, Epoch 74, val_loss = 0.0567\n",
      "Run 3, Epoch 75, val_loss = 0.0531\n",
      "Run 3, Epoch 76, val_loss = 0.0459\n",
      "Run 3, Epoch 77, val_loss = 0.0542\n",
      "Run 3, Epoch 78, val_loss = 0.0620\n",
      "Run 3, Epoch 79, val_loss = 0.0503\n",
      "Run 3, Epoch 80, val_loss = 0.0505\n",
      "Run 3, Epoch 81, val_loss = 0.0744\n",
      "Run 3, Epoch 82, val_loss = 0.0865\n",
      "Run 3, Epoch 83, val_loss = 0.0448\n",
      "Run 3, Epoch 84, val_loss = 0.0438\n",
      "Run 3, Epoch 85, val_loss = 0.0454\n",
      "Run 3, Epoch 86, val_loss = 0.0646\n",
      "✅ Run 3: Acc = 99.75%, MDE = 0.0065\n",
      "Run 4/5 - Alpha = 0.5\n",
      "Run 4, Epoch 1, val_loss = 2.6833\n",
      "Run 4, Epoch 2, val_loss = 1.3653\n",
      "Run 4, Epoch 3, val_loss = 0.9530\n",
      "Run 4, Epoch 4, val_loss = 0.7258\n",
      "Run 4, Epoch 5, val_loss = 0.6320\n",
      "Run 4, Epoch 6, val_loss = 0.4993\n",
      "Run 4, Epoch 7, val_loss = 0.3833\n",
      "Run 4, Epoch 8, val_loss = 0.3753\n",
      "Run 4, Epoch 9, val_loss = 0.4153\n",
      "Run 4, Epoch 10, val_loss = 0.3343\n",
      "Run 4, Epoch 11, val_loss = 0.2429\n",
      "Run 4, Epoch 12, val_loss = 0.1952\n",
      "Run 4, Epoch 13, val_loss = 0.3693\n",
      "Run 4, Epoch 14, val_loss = 0.2686\n",
      "Run 4, Epoch 15, val_loss = 0.2024\n",
      "Run 4, Epoch 16, val_loss = 0.2710\n",
      "Run 4, Epoch 17, val_loss = 0.2083\n",
      "Run 4, Epoch 18, val_loss = 0.1878\n",
      "Run 4, Epoch 19, val_loss = 0.1388\n",
      "Run 4, Epoch 20, val_loss = 0.1399\n",
      "Run 4, Epoch 21, val_loss = 0.2342\n",
      "Run 4, Epoch 22, val_loss = 0.2398\n",
      "Run 4, Epoch 23, val_loss = 0.1392\n",
      "Run 4, Epoch 24, val_loss = 0.2050\n",
      "Run 4, Epoch 25, val_loss = 0.2061\n",
      "Run 4, Epoch 26, val_loss = 0.3127\n",
      "Run 4, Epoch 27, val_loss = 0.1178\n",
      "Run 4, Epoch 28, val_loss = 0.1372\n",
      "Run 4, Epoch 29, val_loss = 0.1755\n",
      "Run 4, Epoch 30, val_loss = 0.1498\n",
      "Run 4, Epoch 31, val_loss = 0.1235\n",
      "Run 4, Epoch 32, val_loss = 0.0971\n",
      "Run 4, Epoch 33, val_loss = 0.1443\n",
      "Run 4, Epoch 34, val_loss = 0.0938\n",
      "Run 4, Epoch 35, val_loss = 0.0973\n",
      "Run 4, Epoch 36, val_loss = 0.1358\n",
      "Run 4, Epoch 37, val_loss = 0.1603\n",
      "Run 4, Epoch 38, val_loss = 0.1079\n",
      "Run 4, Epoch 39, val_loss = 0.0903\n",
      "Run 4, Epoch 40, val_loss = 0.1044\n",
      "Run 4, Epoch 41, val_loss = 0.0910\n",
      "Run 4, Epoch 42, val_loss = 0.1009\n",
      "Run 4, Epoch 43, val_loss = 0.1114\n",
      "Run 4, Epoch 44, val_loss = 0.0900\n",
      "Run 4, Epoch 45, val_loss = 0.0833\n",
      "Run 4, Epoch 46, val_loss = 0.0935\n",
      "Run 4, Epoch 47, val_loss = 0.0859\n",
      "Run 4, Epoch 48, val_loss = 0.0860\n",
      "Run 4, Epoch 49, val_loss = 0.1046\n",
      "Run 4, Epoch 50, val_loss = 0.0952\n",
      "Run 4, Epoch 51, val_loss = 0.1122\n",
      "Run 4, Epoch 52, val_loss = 0.1017\n",
      "Run 4, Epoch 53, val_loss = 0.1137\n",
      "Run 4, Epoch 54, val_loss = 0.0766\n",
      "Run 4, Epoch 55, val_loss = 0.0781\n",
      "Run 4, Epoch 56, val_loss = 0.0983\n",
      "Run 4, Epoch 57, val_loss = 0.0761\n",
      "Run 4, Epoch 58, val_loss = 0.0738\n",
      "Run 4, Epoch 59, val_loss = 0.0743\n",
      "Run 4, Epoch 60, val_loss = 0.1218\n",
      "Run 4, Epoch 61, val_loss = 0.0835\n",
      "Run 4, Epoch 62, val_loss = 0.0834\n",
      "Run 4, Epoch 63, val_loss = 0.1076\n",
      "Run 4, Epoch 64, val_loss = 0.0823\n",
      "Run 4, Epoch 65, val_loss = 0.0716\n",
      "Run 4, Epoch 66, val_loss = 0.0882\n",
      "Run 4, Epoch 67, val_loss = 0.0674\n",
      "Run 4, Epoch 68, val_loss = 0.0656\n",
      "Run 4, Epoch 69, val_loss = 0.0627\n",
      "Run 4, Epoch 70, val_loss = 0.0736\n",
      "Run 4, Epoch 71, val_loss = 0.0583\n",
      "Run 4, Epoch 72, val_loss = 0.0602\n",
      "Run 4, Epoch 73, val_loss = 0.0683\n",
      "Run 4, Epoch 74, val_loss = 0.1007\n",
      "Run 4, Epoch 75, val_loss = 0.0598\n",
      "Run 4, Epoch 76, val_loss = 0.0580\n",
      "Run 4, Epoch 77, val_loss = 0.0563\n",
      "Run 4, Epoch 78, val_loss = 0.0784\n",
      "Run 4, Epoch 79, val_loss = 0.0692\n",
      "Run 4, Epoch 80, val_loss = 0.0614\n",
      "Run 4, Epoch 81, val_loss = 0.0505\n",
      "Run 4, Epoch 82, val_loss = 0.0631\n",
      "Run 4, Epoch 83, val_loss = 0.0486\n",
      "Run 4, Epoch 84, val_loss = 0.0506\n",
      "Run 4, Epoch 85, val_loss = 0.0928\n",
      "Run 4, Epoch 86, val_loss = 0.0580\n",
      "Run 4, Epoch 87, val_loss = 0.0591\n",
      "Run 4, Epoch 88, val_loss = 0.0573\n",
      "Run 4, Epoch 89, val_loss = 0.0669\n",
      "Run 4, Epoch 90, val_loss = 0.0802\n",
      "Run 4, Epoch 91, val_loss = 0.0881\n",
      "Run 4, Epoch 92, val_loss = 0.0470\n",
      "Run 4, Epoch 93, val_loss = 0.0559\n",
      "Run 4, Epoch 94, val_loss = 0.0567\n",
      "Run 4, Epoch 95, val_loss = 0.0441\n",
      "Run 4, Epoch 96, val_loss = 0.0721\n",
      "Run 4, Epoch 97, val_loss = 0.0823\n",
      "Run 4, Epoch 98, val_loss = 0.0720\n",
      "Run 4, Epoch 99, val_loss = 0.0596\n",
      "Run 4, Epoch 100, val_loss = 0.0613\n",
      "Run 4, Epoch 101, val_loss = 0.0645\n",
      "Run 4, Epoch 102, val_loss = 0.0633\n",
      "Run 4, Epoch 103, val_loss = 0.0681\n",
      "Run 4, Epoch 104, val_loss = 0.0615\n",
      "Run 4, Epoch 105, val_loss = 0.0640\n",
      "Run 4, Epoch 106, val_loss = 0.0730\n",
      "Run 4, Epoch 107, val_loss = 0.0684\n",
      "Run 4, Epoch 108, val_loss = 0.0486\n",
      "Run 4, Epoch 109, val_loss = 0.0619\n",
      "Run 4, Epoch 110, val_loss = 0.0657\n",
      "Run 4, Epoch 111, val_loss = 0.0535\n",
      "Run 4, Epoch 112, val_loss = 0.0493\n",
      "Run 4, Epoch 113, val_loss = 0.0519\n",
      "Run 4, Epoch 114, val_loss = 0.0652\n",
      "Run 4, Epoch 115, val_loss = 0.0590\n",
      "✅ Run 4: Acc = 99.69%, MDE = 0.0118\n",
      "Run 5/5 - Alpha = 0.5\n",
      "Run 5, Epoch 1, val_loss = 2.8240\n",
      "Run 5, Epoch 2, val_loss = 1.2420\n",
      "Run 5, Epoch 3, val_loss = 0.7961\n",
      "Run 5, Epoch 4, val_loss = 0.7202\n",
      "Run 5, Epoch 5, val_loss = 0.5259\n",
      "Run 5, Epoch 6, val_loss = 0.4790\n",
      "Run 5, Epoch 7, val_loss = 0.3816\n",
      "Run 5, Epoch 8, val_loss = 0.3497\n",
      "Run 5, Epoch 9, val_loss = 0.5231\n",
      "Run 5, Epoch 10, val_loss = 0.3256\n",
      "Run 5, Epoch 11, val_loss = 0.3201\n",
      "Run 5, Epoch 12, val_loss = 0.2826\n",
      "Run 5, Epoch 13, val_loss = 0.2169\n",
      "Run 5, Epoch 14, val_loss = 0.2152\n",
      "Run 5, Epoch 15, val_loss = 0.1969\n",
      "Run 5, Epoch 16, val_loss = 0.1745\n",
      "Run 5, Epoch 17, val_loss = 0.1744\n",
      "Run 5, Epoch 18, val_loss = 0.1812\n",
      "Run 5, Epoch 19, val_loss = 0.1765\n",
      "Run 5, Epoch 20, val_loss = 0.1519\n",
      "Run 5, Epoch 21, val_loss = 0.1701\n",
      "Run 5, Epoch 22, val_loss = 0.1487\n",
      "Run 5, Epoch 23, val_loss = 0.1498\n",
      "Run 5, Epoch 24, val_loss = 0.1354\n",
      "Run 5, Epoch 25, val_loss = 0.2107\n",
      "Run 5, Epoch 26, val_loss = 0.1619\n",
      "Run 5, Epoch 27, val_loss = 0.1346\n",
      "Run 5, Epoch 28, val_loss = 0.1219\n",
      "Run 5, Epoch 29, val_loss = 0.1117\n",
      "Run 5, Epoch 30, val_loss = 0.1063\n",
      "Run 5, Epoch 31, val_loss = 0.1241\n",
      "Run 5, Epoch 32, val_loss = 0.1122\n",
      "Run 5, Epoch 33, val_loss = 0.1477\n",
      "Run 5, Epoch 34, val_loss = 0.0815\n",
      "Run 5, Epoch 35, val_loss = 0.1135\n",
      "Run 5, Epoch 36, val_loss = 0.1048\n",
      "Run 5, Epoch 37, val_loss = 0.0982\n",
      "Run 5, Epoch 38, val_loss = 0.0913\n",
      "Run 5, Epoch 39, val_loss = 0.0897\n",
      "Run 5, Epoch 40, val_loss = 0.1694\n",
      "Run 5, Epoch 41, val_loss = 0.0884\n",
      "Run 5, Epoch 42, val_loss = 0.1015\n",
      "Run 5, Epoch 43, val_loss = 0.0944\n",
      "Run 5, Epoch 44, val_loss = 0.1074\n",
      "Run 5, Epoch 45, val_loss = 0.0777\n",
      "Run 5, Epoch 46, val_loss = 0.0698\n",
      "Run 5, Epoch 47, val_loss = 0.0740\n",
      "Run 5, Epoch 48, val_loss = 0.1151\n",
      "Run 5, Epoch 49, val_loss = 0.0836\n",
      "Run 5, Epoch 50, val_loss = 0.0923\n",
      "Run 5, Epoch 51, val_loss = 0.0916\n",
      "Run 5, Epoch 52, val_loss = 0.0653\n",
      "Run 5, Epoch 53, val_loss = 0.0795\n",
      "Run 5, Epoch 54, val_loss = 0.0635\n",
      "Run 5, Epoch 55, val_loss = 0.0695\n",
      "Run 5, Epoch 56, val_loss = 0.0827\n",
      "Run 5, Epoch 57, val_loss = 0.0684\n",
      "Run 5, Epoch 58, val_loss = 0.0753\n",
      "Run 5, Epoch 59, val_loss = 0.0635\n",
      "Run 5, Epoch 60, val_loss = 0.0665\n",
      "Run 5, Epoch 61, val_loss = 0.0975\n",
      "Run 5, Epoch 62, val_loss = 0.0724\n",
      "Run 5, Epoch 63, val_loss = 0.0714\n",
      "Run 5, Epoch 64, val_loss = 0.0758\n",
      "Run 5, Epoch 65, val_loss = 0.0653\n",
      "Run 5, Epoch 66, val_loss = 0.1049\n",
      "Run 5, Epoch 67, val_loss = 0.0795\n",
      "Run 5, Epoch 68, val_loss = 0.0761\n",
      "Run 5, Epoch 69, val_loss = 0.0675\n",
      "Run 5, Epoch 70, val_loss = 0.0583\n",
      "Run 5, Epoch 71, val_loss = 0.0619\n",
      "Run 5, Epoch 72, val_loss = 0.1082\n",
      "Run 5, Epoch 73, val_loss = 0.0765\n",
      "Run 5, Epoch 74, val_loss = 0.0815\n",
      "Run 5, Epoch 75, val_loss = 0.0905\n",
      "Run 5, Epoch 76, val_loss = 0.0580\n",
      "Run 5, Epoch 77, val_loss = 0.0637\n",
      "Run 5, Epoch 78, val_loss = 0.0836\n",
      "Run 5, Epoch 79, val_loss = 0.0808\n",
      "Run 5, Epoch 80, val_loss = 0.0622\n",
      "Run 5, Epoch 81, val_loss = 0.0694\n",
      "Run 5, Epoch 82, val_loss = 0.0535\n",
      "Run 5, Epoch 83, val_loss = 0.0905\n",
      "Run 5, Epoch 84, val_loss = 0.0701\n",
      "Run 5, Epoch 85, val_loss = 0.0592\n",
      "Run 5, Epoch 86, val_loss = 0.0581\n",
      "Run 5, Epoch 87, val_loss = 0.0622\n",
      "Run 5, Epoch 88, val_loss = 0.0507\n",
      "Run 5, Epoch 89, val_loss = 0.0518\n",
      "Run 5, Epoch 90, val_loss = 0.1094\n",
      "Run 5, Epoch 91, val_loss = 0.0664\n",
      "Run 5, Epoch 92, val_loss = 0.0550\n",
      "Run 5, Epoch 93, val_loss = 0.0669\n",
      "Run 5, Epoch 94, val_loss = 0.0541\n",
      "Run 5, Epoch 95, val_loss = 0.0494\n",
      "Run 5, Epoch 96, val_loss = 0.0626\n",
      "Run 5, Epoch 97, val_loss = 0.0506\n",
      "Run 5, Epoch 98, val_loss = 0.0593\n",
      "Run 5, Epoch 99, val_loss = 0.1030\n",
      "Run 5, Epoch 100, val_loss = 0.0639\n",
      "Run 5, Epoch 101, val_loss = 0.1061\n",
      "Run 5, Epoch 102, val_loss = 0.0533\n",
      "Run 5, Epoch 103, val_loss = 0.0684\n",
      "Run 5, Epoch 104, val_loss = 0.0611\n",
      "Run 5, Epoch 105, val_loss = 0.0479\n",
      "Run 5, Epoch 106, val_loss = 0.0552\n",
      "Run 5, Epoch 107, val_loss = 0.0484\n",
      "Run 5, Epoch 108, val_loss = 0.0617\n",
      "Run 5, Epoch 109, val_loss = 0.0419\n",
      "Run 5, Epoch 110, val_loss = 0.0537\n",
      "Run 5, Epoch 111, val_loss = 0.0579\n",
      "Run 5, Epoch 112, val_loss = 0.0646\n",
      "Run 5, Epoch 113, val_loss = 0.0562\n",
      "Run 5, Epoch 114, val_loss = 0.0630\n",
      "Run 5, Epoch 115, val_loss = 0.0536\n",
      "Run 5, Epoch 116, val_loss = 0.0503\n",
      "Run 5, Epoch 117, val_loss = 0.0467\n",
      "Run 5, Epoch 118, val_loss = 0.0521\n",
      "Run 5, Epoch 119, val_loss = 0.0635\n",
      "Run 5, Epoch 120, val_loss = 0.0506\n",
      "Run 5, Epoch 121, val_loss = 0.0583\n",
      "Run 5, Epoch 122, val_loss = 0.0553\n",
      "Run 5, Epoch 123, val_loss = 0.0550\n",
      "Run 5, Epoch 124, val_loss = 0.0543\n",
      "Run 5, Epoch 125, val_loss = 0.0467\n",
      "Run 5, Epoch 126, val_loss = 0.0406\n",
      "Run 5, Epoch 127, val_loss = 0.0509\n",
      "Run 5, Epoch 128, val_loss = 0.0342\n",
      "Run 5, Epoch 129, val_loss = 0.0385\n",
      "Run 5, Epoch 130, val_loss = 0.0360\n",
      "Run 5, Epoch 131, val_loss = 0.0462\n",
      "Run 5, Epoch 132, val_loss = 0.0396\n",
      "Run 5, Epoch 133, val_loss = 0.0398\n",
      "Run 5, Epoch 134, val_loss = 0.0422\n",
      "Run 5, Epoch 135, val_loss = 0.0399\n",
      "Run 5, Epoch 136, val_loss = 0.0356\n",
      "Run 5, Epoch 137, val_loss = 0.0359\n",
      "Run 5, Epoch 138, val_loss = 0.0393\n",
      "Run 5, Epoch 139, val_loss = 0.0375\n",
      "Run 5, Epoch 140, val_loss = 0.0349\n",
      "Run 5, Epoch 141, val_loss = 0.0386\n",
      "Run 5, Epoch 142, val_loss = 0.0400\n",
      "Run 5, Epoch 143, val_loss = 0.0398\n",
      "Run 5, Epoch 144, val_loss = 0.0692\n",
      "Run 5, Epoch 145, val_loss = 0.0368\n",
      "Run 5, Epoch 146, val_loss = 0.0397\n",
      "Run 5, Epoch 147, val_loss = 0.0384\n",
      "Run 5, Epoch 148, val_loss = 0.0372\n",
      "✅ Run 5: Acc = 99.75%, MDE = 0.0080\n",
      "📁 Results & all errors saved to repeat_copy/05\n",
      "\n",
      "[Alpha = 0.6] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.6\n",
      "Run 1, Epoch 1, val_loss = 2.9139\n",
      "Run 1, Epoch 2, val_loss = 1.3091\n",
      "Run 1, Epoch 3, val_loss = 0.8821\n",
      "Run 1, Epoch 4, val_loss = 0.7189\n",
      "Run 1, Epoch 5, val_loss = 0.4640\n",
      "Run 1, Epoch 6, val_loss = 0.4965\n",
      "Run 1, Epoch 7, val_loss = 0.5164\n",
      "Run 1, Epoch 8, val_loss = 0.5274\n",
      "Run 1, Epoch 9, val_loss = 0.4375\n",
      "Run 1, Epoch 10, val_loss = 0.2952\n",
      "Run 1, Epoch 11, val_loss = 0.3058\n",
      "Run 1, Epoch 12, val_loss = 0.3143\n",
      "Run 1, Epoch 13, val_loss = 0.2604\n",
      "Run 1, Epoch 14, val_loss = 0.2158\n",
      "Run 1, Epoch 15, val_loss = 0.3076\n",
      "Run 1, Epoch 16, val_loss = 0.1592\n",
      "Run 1, Epoch 17, val_loss = 0.1811\n",
      "Run 1, Epoch 18, val_loss = 0.2100\n",
      "Run 1, Epoch 19, val_loss = 0.1911\n",
      "Run 1, Epoch 20, val_loss = 0.2164\n",
      "Run 1, Epoch 21, val_loss = 0.1986\n",
      "Run 1, Epoch 22, val_loss = 0.1870\n",
      "Run 1, Epoch 23, val_loss = 0.2075\n",
      "Run 1, Epoch 24, val_loss = 0.1470\n",
      "Run 1, Epoch 25, val_loss = 0.1405\n",
      "Run 1, Epoch 26, val_loss = 0.1148\n",
      "Run 1, Epoch 27, val_loss = 0.1252\n",
      "Run 1, Epoch 28, val_loss = 0.2040\n",
      "Run 1, Epoch 29, val_loss = 0.1428\n",
      "Run 1, Epoch 30, val_loss = 0.2039\n",
      "Run 1, Epoch 31, val_loss = 0.1329\n",
      "Run 1, Epoch 32, val_loss = 0.1138\n",
      "Run 1, Epoch 33, val_loss = 0.1243\n",
      "Run 1, Epoch 34, val_loss = 0.0982\n",
      "Run 1, Epoch 35, val_loss = 0.0903\n",
      "Run 1, Epoch 36, val_loss = 0.1423\n",
      "Run 1, Epoch 37, val_loss = 0.1035\n",
      "Run 1, Epoch 38, val_loss = 0.1223\n",
      "Run 1, Epoch 39, val_loss = 0.0640\n",
      "Run 1, Epoch 40, val_loss = 0.0891\n",
      "Run 1, Epoch 41, val_loss = 0.0977\n",
      "Run 1, Epoch 42, val_loss = 0.0679\n",
      "Run 1, Epoch 43, val_loss = 0.1064\n",
      "Run 1, Epoch 44, val_loss = 0.1282\n",
      "Run 1, Epoch 45, val_loss = 0.0768\n",
      "Run 1, Epoch 46, val_loss = 0.0883\n",
      "Run 1, Epoch 47, val_loss = 0.0755\n",
      "Run 1, Epoch 48, val_loss = 0.0805\n",
      "Run 1, Epoch 49, val_loss = 0.0617\n",
      "Run 1, Epoch 50, val_loss = 0.0705\n",
      "Run 1, Epoch 51, val_loss = 0.0676\n",
      "Run 1, Epoch 52, val_loss = 0.0976\n",
      "Run 1, Epoch 53, val_loss = 0.0709\n",
      "Run 1, Epoch 54, val_loss = 0.0904\n",
      "Run 1, Epoch 55, val_loss = 0.0802\n",
      "Run 1, Epoch 56, val_loss = 0.1048\n",
      "Run 1, Epoch 57, val_loss = 0.0844\n",
      "Run 1, Epoch 58, val_loss = 0.0994\n",
      "Run 1, Epoch 59, val_loss = 0.1108\n",
      "Run 1, Epoch 60, val_loss = 0.0899\n",
      "Run 1, Epoch 61, val_loss = 0.0765\n",
      "Run 1, Epoch 62, val_loss = 0.0831\n",
      "Run 1, Epoch 63, val_loss = 0.0866\n",
      "Run 1, Epoch 64, val_loss = 0.0791\n",
      "Run 1, Epoch 65, val_loss = 0.0816\n",
      "Run 1, Epoch 66, val_loss = 0.0736\n",
      "Run 1, Epoch 67, val_loss = 0.0759\n",
      "Run 1, Epoch 68, val_loss = 0.0657\n",
      "Run 1, Epoch 69, val_loss = 0.0663\n",
      "✅ Run 1: Acc = 99.69%, MDE = 0.0106\n",
      "Run 2/5 - Alpha = 0.6\n",
      "Run 2, Epoch 1, val_loss = 3.0356\n",
      "Run 2, Epoch 2, val_loss = 1.2790\n",
      "Run 2, Epoch 3, val_loss = 0.8622\n",
      "Run 2, Epoch 4, val_loss = 0.6568\n",
      "Run 2, Epoch 5, val_loss = 0.5414\n",
      "Run 2, Epoch 6, val_loss = 0.5798\n",
      "Run 2, Epoch 7, val_loss = 0.5002\n",
      "Run 2, Epoch 8, val_loss = 0.4089\n",
      "Run 2, Epoch 9, val_loss = 0.4158\n",
      "Run 2, Epoch 10, val_loss = 0.4341\n",
      "Run 2, Epoch 11, val_loss = 0.2557\n",
      "Run 2, Epoch 12, val_loss = 0.2811\n",
      "Run 2, Epoch 13, val_loss = 0.2403\n",
      "Run 2, Epoch 14, val_loss = 0.2625\n",
      "Run 2, Epoch 15, val_loss = 0.2308\n",
      "Run 2, Epoch 16, val_loss = 0.4668\n",
      "Run 2, Epoch 17, val_loss = 0.1959\n",
      "Run 2, Epoch 18, val_loss = 0.1904\n",
      "Run 2, Epoch 19, val_loss = 0.1863\n",
      "Run 2, Epoch 20, val_loss = 0.3623\n",
      "Run 2, Epoch 21, val_loss = 0.1689\n",
      "Run 2, Epoch 22, val_loss = 0.1342\n",
      "Run 2, Epoch 23, val_loss = 0.3531\n",
      "Run 2, Epoch 24, val_loss = 0.1555\n",
      "Run 2, Epoch 25, val_loss = 0.1675\n",
      "Run 2, Epoch 26, val_loss = 0.1039\n",
      "Run 2, Epoch 27, val_loss = 0.1143\n",
      "Run 2, Epoch 28, val_loss = 0.1453\n",
      "Run 2, Epoch 29, val_loss = 0.1178\n",
      "Run 2, Epoch 30, val_loss = 0.1449\n",
      "Run 2, Epoch 31, val_loss = 0.1066\n",
      "Run 2, Epoch 32, val_loss = 0.1561\n",
      "Run 2, Epoch 33, val_loss = 0.0909\n",
      "Run 2, Epoch 34, val_loss = 0.1056\n",
      "Run 2, Epoch 35, val_loss = 0.0899\n",
      "Run 2, Epoch 36, val_loss = 0.1039\n",
      "Run 2, Epoch 37, val_loss = 0.1390\n",
      "Run 2, Epoch 38, val_loss = 0.1144\n",
      "Run 2, Epoch 39, val_loss = 0.1137\n",
      "Run 2, Epoch 40, val_loss = 0.0822\n",
      "Run 2, Epoch 41, val_loss = 0.1013\n",
      "Run 2, Epoch 42, val_loss = 0.1138\n",
      "Run 2, Epoch 43, val_loss = 0.0977\n",
      "Run 2, Epoch 44, val_loss = 0.0812\n",
      "Run 2, Epoch 45, val_loss = 0.1083\n",
      "Run 2, Epoch 46, val_loss = 0.0825\n",
      "Run 2, Epoch 47, val_loss = 0.1524\n",
      "Run 2, Epoch 48, val_loss = 0.0652\n",
      "Run 2, Epoch 49, val_loss = 0.0866\n",
      "Run 2, Epoch 50, val_loss = 0.0759\n",
      "Run 2, Epoch 51, val_loss = 0.0779\n",
      "Run 2, Epoch 52, val_loss = 0.0655\n",
      "Run 2, Epoch 53, val_loss = 0.0806\n",
      "Run 2, Epoch 54, val_loss = 0.0837\n",
      "Run 2, Epoch 55, val_loss = 0.0643\n",
      "Run 2, Epoch 56, val_loss = 0.0806\n",
      "Run 2, Epoch 57, val_loss = 0.0569\n",
      "Run 2, Epoch 58, val_loss = 0.0765\n",
      "Run 2, Epoch 59, val_loss = 0.0698\n",
      "Run 2, Epoch 60, val_loss = 0.0838\n",
      "Run 2, Epoch 61, val_loss = 0.1699\n",
      "Run 2, Epoch 62, val_loss = 0.0934\n",
      "Run 2, Epoch 63, val_loss = 0.0595\n",
      "Run 2, Epoch 64, val_loss = 0.0674\n",
      "Run 2, Epoch 65, val_loss = 0.0873\n",
      "Run 2, Epoch 66, val_loss = 0.0757\n",
      "Run 2, Epoch 67, val_loss = 0.0618\n",
      "Run 2, Epoch 68, val_loss = 0.0631\n",
      "Run 2, Epoch 69, val_loss = 0.0749\n",
      "Run 2, Epoch 70, val_loss = 0.0907\n",
      "Run 2, Epoch 71, val_loss = 0.0664\n",
      "Run 2, Epoch 72, val_loss = 0.0718\n",
      "Run 2, Epoch 73, val_loss = 0.0628\n",
      "Run 2, Epoch 74, val_loss = 0.0506\n",
      "Run 2, Epoch 75, val_loss = 0.0495\n",
      "Run 2, Epoch 76, val_loss = 0.0483\n",
      "Run 2, Epoch 77, val_loss = 0.0410\n",
      "Run 2, Epoch 78, val_loss = 0.0455\n",
      "Run 2, Epoch 79, val_loss = 0.0439\n",
      "Run 2, Epoch 80, val_loss = 0.0540\n",
      "Run 2, Epoch 81, val_loss = 0.0653\n",
      "Run 2, Epoch 82, val_loss = 0.0464\n",
      "Run 2, Epoch 83, val_loss = 0.0497\n",
      "Run 2, Epoch 84, val_loss = 0.0455\n",
      "Run 2, Epoch 85, val_loss = 0.0439\n",
      "Run 2, Epoch 86, val_loss = 0.0531\n",
      "Run 2, Epoch 87, val_loss = 0.0996\n",
      "Run 2, Epoch 88, val_loss = 0.0856\n",
      "Run 2, Epoch 89, val_loss = 0.0467\n",
      "Run 2, Epoch 90, val_loss = 0.0494\n",
      "Run 2, Epoch 91, val_loss = 0.0725\n",
      "Run 2, Epoch 92, val_loss = 0.0428\n",
      "Run 2, Epoch 93, val_loss = 0.0526\n",
      "Run 2, Epoch 94, val_loss = 0.0566\n",
      "Run 2, Epoch 95, val_loss = 0.0410\n",
      "Run 2, Epoch 96, val_loss = 0.0420\n",
      "Run 2, Epoch 97, val_loss = 0.0434\n",
      "✅ Run 2: Acc = 99.69%, MDE = 0.0088\n",
      "Run 3/5 - Alpha = 0.6\n",
      "Run 3, Epoch 1, val_loss = 2.7014\n",
      "Run 3, Epoch 2, val_loss = 1.4253\n",
      "Run 3, Epoch 3, val_loss = 0.7960\n",
      "Run 3, Epoch 4, val_loss = 0.6889\n",
      "Run 3, Epoch 5, val_loss = 0.5719\n",
      "Run 3, Epoch 6, val_loss = 0.4916\n",
      "Run 3, Epoch 7, val_loss = 0.4700\n",
      "Run 3, Epoch 8, val_loss = 0.3345\n",
      "Run 3, Epoch 9, val_loss = 0.3529\n",
      "Run 3, Epoch 10, val_loss = 0.4546\n",
      "Run 3, Epoch 11, val_loss = 0.3626\n",
      "Run 3, Epoch 12, val_loss = 0.3447\n",
      "Run 3, Epoch 13, val_loss = 0.2564\n",
      "Run 3, Epoch 14, val_loss = 0.2049\n",
      "Run 3, Epoch 15, val_loss = 0.2063\n",
      "Run 3, Epoch 16, val_loss = 0.2200\n",
      "Run 3, Epoch 17, val_loss = 0.1724\n",
      "Run 3, Epoch 18, val_loss = 0.2450\n",
      "Run 3, Epoch 19, val_loss = 0.2207\n",
      "Run 3, Epoch 20, val_loss = 0.1655\n",
      "Run 3, Epoch 21, val_loss = 0.1683\n",
      "Run 3, Epoch 22, val_loss = 0.1570\n",
      "Run 3, Epoch 23, val_loss = 0.1290\n",
      "Run 3, Epoch 24, val_loss = 0.1481\n",
      "Run 3, Epoch 25, val_loss = 0.1513\n",
      "Run 3, Epoch 26, val_loss = 0.1767\n",
      "Run 3, Epoch 27, val_loss = 0.1416\n",
      "Run 3, Epoch 28, val_loss = 0.1478\n",
      "Run 3, Epoch 29, val_loss = 0.1121\n",
      "Run 3, Epoch 30, val_loss = 0.1261\n",
      "Run 3, Epoch 31, val_loss = 0.1482\n",
      "Run 3, Epoch 32, val_loss = 0.1141\n",
      "Run 3, Epoch 33, val_loss = 0.1272\n",
      "Run 3, Epoch 34, val_loss = 0.1347\n",
      "Run 3, Epoch 35, val_loss = 0.1427\n",
      "Run 3, Epoch 36, val_loss = 0.1006\n",
      "Run 3, Epoch 37, val_loss = 0.0976\n",
      "Run 3, Epoch 38, val_loss = 0.1150\n",
      "Run 3, Epoch 39, val_loss = 0.0968\n",
      "Run 3, Epoch 40, val_loss = 0.1223\n",
      "Run 3, Epoch 41, val_loss = 0.1120\n",
      "Run 3, Epoch 42, val_loss = 0.0923\n",
      "Run 3, Epoch 43, val_loss = 0.1077\n",
      "Run 3, Epoch 44, val_loss = 0.0993\n",
      "Run 3, Epoch 45, val_loss = 0.0925\n",
      "Run 3, Epoch 46, val_loss = 0.0915\n",
      "Run 3, Epoch 47, val_loss = 0.0887\n",
      "Run 3, Epoch 48, val_loss = 0.0998\n",
      "Run 3, Epoch 49, val_loss = 0.1435\n",
      "Run 3, Epoch 50, val_loss = 0.0936\n",
      "Run 3, Epoch 51, val_loss = 0.0985\n",
      "Run 3, Epoch 52, val_loss = 0.0893\n",
      "Run 3, Epoch 53, val_loss = 0.1069\n",
      "Run 3, Epoch 54, val_loss = 0.0875\n",
      "Run 3, Epoch 55, val_loss = 0.1006\n",
      "Run 3, Epoch 56, val_loss = 0.0966\n",
      "Run 3, Epoch 57, val_loss = 0.0942\n",
      "Run 3, Epoch 58, val_loss = 0.0838\n",
      "Run 3, Epoch 59, val_loss = 0.0937\n",
      "Run 3, Epoch 60, val_loss = 0.0765\n",
      "Run 3, Epoch 61, val_loss = 0.0664\n",
      "Run 3, Epoch 62, val_loss = 0.0672\n",
      "Run 3, Epoch 63, val_loss = 0.0711\n",
      "Run 3, Epoch 64, val_loss = 0.0988\n",
      "Run 3, Epoch 65, val_loss = 0.0772\n",
      "Run 3, Epoch 66, val_loss = 0.0768\n",
      "Run 3, Epoch 67, val_loss = 0.0974\n",
      "Run 3, Epoch 68, val_loss = 0.0719\n",
      "Run 3, Epoch 69, val_loss = 0.0756\n",
      "Run 3, Epoch 70, val_loss = 0.0871\n",
      "Run 3, Epoch 71, val_loss = 0.0995\n",
      "Run 3, Epoch 72, val_loss = 0.0829\n",
      "Run 3, Epoch 73, val_loss = 0.0540\n",
      "Run 3, Epoch 74, val_loss = 0.0604\n",
      "Run 3, Epoch 75, val_loss = 0.0681\n",
      "Run 3, Epoch 76, val_loss = 0.0866\n",
      "Run 3, Epoch 77, val_loss = 0.0664\n",
      "Run 3, Epoch 78, val_loss = 0.0729\n",
      "Run 3, Epoch 79, val_loss = 0.0895\n",
      "Run 3, Epoch 80, val_loss = 0.0664\n",
      "Run 3, Epoch 81, val_loss = 0.0594\n",
      "Run 3, Epoch 82, val_loss = 0.0721\n",
      "Run 3, Epoch 83, val_loss = 0.0603\n",
      "Run 3, Epoch 84, val_loss = 0.0618\n",
      "Run 3, Epoch 85, val_loss = 0.0690\n",
      "Run 3, Epoch 86, val_loss = 0.0820\n",
      "Run 3, Epoch 87, val_loss = 0.0578\n",
      "Run 3, Epoch 88, val_loss = 0.0513\n",
      "Run 3, Epoch 89, val_loss = 0.0945\n",
      "Run 3, Epoch 90, val_loss = 0.0649\n",
      "Run 3, Epoch 91, val_loss = 0.0927\n",
      "Run 3, Epoch 92, val_loss = 0.0726\n",
      "Run 3, Epoch 93, val_loss = 0.0586\n",
      "Run 3, Epoch 94, val_loss = 0.0581\n",
      "Run 3, Epoch 95, val_loss = 0.0645\n",
      "Run 3, Epoch 96, val_loss = 0.0643\n",
      "Run 3, Epoch 97, val_loss = 0.0739\n",
      "Run 3, Epoch 98, val_loss = 0.0496\n",
      "Run 3, Epoch 99, val_loss = 0.0649\n",
      "Run 3, Epoch 100, val_loss = 0.0593\n",
      "Run 3, Epoch 101, val_loss = 0.0578\n",
      "Run 3, Epoch 102, val_loss = 0.0707\n",
      "Run 3, Epoch 103, val_loss = 0.0550\n",
      "Run 3, Epoch 104, val_loss = 0.0471\n",
      "Run 3, Epoch 105, val_loss = 0.0648\n",
      "Run 3, Epoch 106, val_loss = 0.0732\n",
      "Run 3, Epoch 107, val_loss = 0.0780\n",
      "Run 3, Epoch 108, val_loss = 0.0625\n",
      "Run 3, Epoch 109, val_loss = 0.0511\n",
      "Run 3, Epoch 110, val_loss = 0.0553\n",
      "Run 3, Epoch 111, val_loss = 0.0455\n",
      "Run 3, Epoch 112, val_loss = 0.0520\n",
      "Run 3, Epoch 113, val_loss = 0.0650\n",
      "Run 3, Epoch 114, val_loss = 0.0537\n",
      "Run 3, Epoch 115, val_loss = 0.0511\n",
      "Run 3, Epoch 116, val_loss = 0.0526\n",
      "Run 3, Epoch 117, val_loss = 0.0583\n",
      "Run 3, Epoch 118, val_loss = 0.0568\n",
      "Run 3, Epoch 119, val_loss = 0.0475\n",
      "Run 3, Epoch 120, val_loss = 0.0491\n",
      "Run 3, Epoch 121, val_loss = 0.0584\n",
      "Run 3, Epoch 122, val_loss = 0.0594\n",
      "Run 3, Epoch 123, val_loss = 0.0464\n",
      "Run 3, Epoch 124, val_loss = 0.0567\n",
      "Run 3, Epoch 125, val_loss = 0.0746\n",
      "Run 3, Epoch 126, val_loss = 0.0502\n",
      "Run 3, Epoch 127, val_loss = 0.0542\n",
      "Run 3, Epoch 128, val_loss = 0.0452\n",
      "Run 3, Epoch 129, val_loss = 0.0438\n",
      "Run 3, Epoch 130, val_loss = 0.0490\n",
      "Run 3, Epoch 131, val_loss = 0.0424\n",
      "Run 3, Epoch 132, val_loss = 0.0384\n",
      "Run 3, Epoch 133, val_loss = 0.0430\n",
      "Run 3, Epoch 134, val_loss = 0.0424\n",
      "Run 3, Epoch 135, val_loss = 0.0459\n",
      "Run 3, Epoch 136, val_loss = 0.0463\n",
      "Run 3, Epoch 137, val_loss = 0.0427\n",
      "Run 3, Epoch 138, val_loss = 0.0433\n",
      "Run 3, Epoch 139, val_loss = 0.0419\n",
      "Run 3, Epoch 140, val_loss = 0.0437\n",
      "Run 3, Epoch 141, val_loss = 0.0432\n",
      "Run 3, Epoch 142, val_loss = 0.0467\n",
      "Run 3, Epoch 143, val_loss = 0.0460\n",
      "Run 3, Epoch 144, val_loss = 0.0463\n",
      "Run 3, Epoch 145, val_loss = 0.0411\n",
      "Run 3, Epoch 146, val_loss = 0.0461\n",
      "Run 3, Epoch 147, val_loss = 0.0501\n",
      "Run 3, Epoch 148, val_loss = 0.0445\n",
      "Run 3, Epoch 149, val_loss = 0.0422\n",
      "Run 3, Epoch 150, val_loss = 0.0446\n",
      "Run 3, Epoch 151, val_loss = 0.0410\n",
      "Run 3, Epoch 152, val_loss = 0.0398\n",
      "✅ Run 3: Acc = 99.64%, MDE = 0.0103\n",
      "Run 4/5 - Alpha = 0.6\n",
      "Run 4, Epoch 1, val_loss = 2.6194\n",
      "Run 4, Epoch 2, val_loss = 1.1557\n",
      "Run 4, Epoch 3, val_loss = 0.7949\n",
      "Run 4, Epoch 4, val_loss = 0.6776\n",
      "Run 4, Epoch 5, val_loss = 0.6421\n",
      "Run 4, Epoch 6, val_loss = 0.5620\n",
      "Run 4, Epoch 7, val_loss = 0.3718\n",
      "Run 4, Epoch 8, val_loss = 0.3792\n",
      "Run 4, Epoch 9, val_loss = 0.3481\n",
      "Run 4, Epoch 10, val_loss = 0.3419\n",
      "Run 4, Epoch 11, val_loss = 0.3010\n",
      "Run 4, Epoch 12, val_loss = 0.3051\n",
      "Run 4, Epoch 13, val_loss = 0.3100\n",
      "Run 4, Epoch 14, val_loss = 0.2707\n",
      "Run 4, Epoch 15, val_loss = 0.2379\n",
      "Run 4, Epoch 16, val_loss = 0.2959\n",
      "Run 4, Epoch 17, val_loss = 0.2455\n",
      "Run 4, Epoch 18, val_loss = 0.2026\n",
      "Run 4, Epoch 19, val_loss = 0.1641\n",
      "Run 4, Epoch 20, val_loss = 0.1775\n",
      "Run 4, Epoch 21, val_loss = 0.1803\n",
      "Run 4, Epoch 22, val_loss = 0.1587\n",
      "Run 4, Epoch 23, val_loss = 0.1323\n",
      "Run 4, Epoch 24, val_loss = 0.1652\n",
      "Run 4, Epoch 25, val_loss = 0.1322\n",
      "Run 4, Epoch 26, val_loss = 0.1097\n",
      "Run 4, Epoch 27, val_loss = 0.1237\n",
      "Run 4, Epoch 28, val_loss = 0.1454\n",
      "Run 4, Epoch 29, val_loss = 0.1178\n",
      "Run 4, Epoch 30, val_loss = 0.1160\n",
      "Run 4, Epoch 31, val_loss = 0.1084\n",
      "Run 4, Epoch 32, val_loss = 0.0903\n",
      "Run 4, Epoch 33, val_loss = 0.1066\n",
      "Run 4, Epoch 34, val_loss = 0.1492\n",
      "Run 4, Epoch 35, val_loss = 0.1176\n",
      "Run 4, Epoch 36, val_loss = 0.0931\n",
      "Run 4, Epoch 37, val_loss = 0.0860\n",
      "Run 4, Epoch 38, val_loss = 0.1254\n",
      "Run 4, Epoch 39, val_loss = 0.0850\n",
      "Run 4, Epoch 40, val_loss = 0.0960\n",
      "Run 4, Epoch 41, val_loss = 0.0821\n",
      "Run 4, Epoch 42, val_loss = 0.0744\n",
      "Run 4, Epoch 43, val_loss = 0.0689\n",
      "Run 4, Epoch 44, val_loss = 0.0955\n",
      "Run 4, Epoch 45, val_loss = 0.0695\n",
      "Run 4, Epoch 46, val_loss = 0.0634\n",
      "Run 4, Epoch 47, val_loss = 0.0645\n",
      "Run 4, Epoch 48, val_loss = 0.0754\n",
      "Run 4, Epoch 49, val_loss = 0.1033\n",
      "Run 4, Epoch 50, val_loss = 0.0728\n",
      "Run 4, Epoch 51, val_loss = 0.1112\n",
      "Run 4, Epoch 52, val_loss = 0.0766\n",
      "Run 4, Epoch 53, val_loss = 0.0716\n",
      "Run 4, Epoch 54, val_loss = 0.0633\n",
      "Run 4, Epoch 55, val_loss = 0.0747\n",
      "Run 4, Epoch 56, val_loss = 0.0633\n",
      "Run 4, Epoch 57, val_loss = 0.0647\n",
      "Run 4, Epoch 58, val_loss = 0.1185\n",
      "Run 4, Epoch 59, val_loss = 0.0695\n",
      "Run 4, Epoch 60, val_loss = 0.0663\n",
      "Run 4, Epoch 61, val_loss = 0.1077\n",
      "Run 4, Epoch 62, val_loss = 0.0521\n",
      "Run 4, Epoch 63, val_loss = 0.0642\n",
      "Run 4, Epoch 64, val_loss = 0.0650\n",
      "Run 4, Epoch 65, val_loss = 0.0557\n",
      "Run 4, Epoch 66, val_loss = 0.0742\n",
      "Run 4, Epoch 67, val_loss = 0.0613\n",
      "Run 4, Epoch 68, val_loss = 0.0568\n",
      "Run 4, Epoch 69, val_loss = 0.0525\n",
      "Run 4, Epoch 70, val_loss = 0.0656\n",
      "Run 4, Epoch 71, val_loss = 0.0567\n",
      "Run 4, Epoch 72, val_loss = 0.0539\n",
      "Run 4, Epoch 73, val_loss = 0.0494\n",
      "Run 4, Epoch 74, val_loss = 0.0683\n",
      "Run 4, Epoch 75, val_loss = 0.0533\n",
      "Run 4, Epoch 76, val_loss = 0.0475\n",
      "Run 4, Epoch 77, val_loss = 0.0522\n",
      "Run 4, Epoch 78, val_loss = 0.0484\n",
      "Run 4, Epoch 79, val_loss = 0.0505\n",
      "Run 4, Epoch 80, val_loss = 0.0601\n",
      "Run 4, Epoch 81, val_loss = 0.0527\n",
      "Run 4, Epoch 82, val_loss = 0.0560\n",
      "Run 4, Epoch 83, val_loss = 0.0545\n",
      "Run 4, Epoch 84, val_loss = 0.0461\n",
      "Run 4, Epoch 85, val_loss = 0.0535\n",
      "Run 4, Epoch 86, val_loss = 0.0591\n",
      "Run 4, Epoch 87, val_loss = 0.0519\n",
      "Run 4, Epoch 88, val_loss = 0.0497\n",
      "Run 4, Epoch 89, val_loss = 0.0681\n",
      "Run 4, Epoch 90, val_loss = 0.0537\n",
      "Run 4, Epoch 91, val_loss = 0.0580\n",
      "Run 4, Epoch 92, val_loss = 0.0674\n",
      "Run 4, Epoch 93, val_loss = 0.0522\n",
      "Run 4, Epoch 94, val_loss = 0.0762\n",
      "Run 4, Epoch 95, val_loss = 0.0750\n",
      "Run 4, Epoch 96, val_loss = 0.0582\n",
      "Run 4, Epoch 97, val_loss = 0.0653\n",
      "Run 4, Epoch 98, val_loss = 0.0560\n",
      "Run 4, Epoch 99, val_loss = 0.0731\n",
      "Run 4, Epoch 100, val_loss = 0.0886\n",
      "Run 4, Epoch 101, val_loss = 0.0562\n",
      "Run 4, Epoch 102, val_loss = 0.0522\n",
      "Run 4, Epoch 103, val_loss = 0.0477\n",
      "Run 4, Epoch 104, val_loss = 0.0700\n",
      "✅ Run 4: Acc = 99.59%, MDE = 0.0082\n",
      "Run 5/5 - Alpha = 0.6\n",
      "Run 5, Epoch 1, val_loss = 2.6500\n",
      "Run 5, Epoch 2, val_loss = 1.2763\n",
      "Run 5, Epoch 3, val_loss = 0.8328\n",
      "Run 5, Epoch 4, val_loss = 0.7119\n",
      "Run 5, Epoch 5, val_loss = 0.5987\n",
      "Run 5, Epoch 6, val_loss = 0.4604\n",
      "Run 5, Epoch 7, val_loss = 0.5358\n",
      "Run 5, Epoch 8, val_loss = 0.3510\n",
      "Run 5, Epoch 9, val_loss = 0.3429\n",
      "Run 5, Epoch 10, val_loss = 0.3636\n",
      "Run 5, Epoch 11, val_loss = 0.2610\n",
      "Run 5, Epoch 12, val_loss = 0.2830\n",
      "Run 5, Epoch 13, val_loss = 0.2458\n",
      "Run 5, Epoch 14, val_loss = 0.2305\n",
      "Run 5, Epoch 15, val_loss = 0.2276\n",
      "Run 5, Epoch 16, val_loss = 0.2427\n",
      "Run 5, Epoch 17, val_loss = 0.1925\n",
      "Run 5, Epoch 18, val_loss = 0.2657\n",
      "Run 5, Epoch 19, val_loss = 0.1712\n",
      "Run 5, Epoch 20, val_loss = 0.1824\n",
      "Run 5, Epoch 21, val_loss = 0.1592\n",
      "Run 5, Epoch 22, val_loss = 0.2035\n",
      "Run 5, Epoch 23, val_loss = 0.1266\n",
      "Run 5, Epoch 24, val_loss = 0.1719\n",
      "Run 5, Epoch 25, val_loss = 0.1809\n",
      "Run 5, Epoch 26, val_loss = 0.1497\n",
      "Run 5, Epoch 27, val_loss = 0.2277\n",
      "Run 5, Epoch 28, val_loss = 0.1054\n",
      "Run 5, Epoch 29, val_loss = 0.1029\n",
      "Run 5, Epoch 30, val_loss = 0.1207\n",
      "Run 5, Epoch 31, val_loss = 0.1107\n",
      "Run 5, Epoch 32, val_loss = 0.1205\n",
      "Run 5, Epoch 33, val_loss = 0.1026\n",
      "Run 5, Epoch 34, val_loss = 0.0795\n",
      "Run 5, Epoch 35, val_loss = 0.1009\n",
      "Run 5, Epoch 36, val_loss = 0.1081\n",
      "Run 5, Epoch 37, val_loss = 0.0848\n",
      "Run 5, Epoch 38, val_loss = 0.0995\n",
      "Run 5, Epoch 39, val_loss = 0.1155\n",
      "Run 5, Epoch 40, val_loss = 0.1246\n",
      "Run 5, Epoch 41, val_loss = 0.0780\n",
      "Run 5, Epoch 42, val_loss = 0.0785\n",
      "Run 5, Epoch 43, val_loss = 0.0762\n",
      "Run 5, Epoch 44, val_loss = 0.0658\n",
      "Run 5, Epoch 45, val_loss = 0.0679\n",
      "Run 5, Epoch 46, val_loss = 0.0804\n",
      "Run 5, Epoch 47, val_loss = 0.0685\n",
      "Run 5, Epoch 48, val_loss = 0.0871\n",
      "Run 5, Epoch 49, val_loss = 0.0768\n",
      "Run 5, Epoch 50, val_loss = 0.0641\n",
      "Run 5, Epoch 51, val_loss = 0.0890\n",
      "Run 5, Epoch 52, val_loss = 0.0801\n",
      "Run 5, Epoch 53, val_loss = 0.0687\n",
      "Run 5, Epoch 54, val_loss = 0.1141\n",
      "Run 5, Epoch 55, val_loss = 0.1060\n",
      "Run 5, Epoch 56, val_loss = 0.0706\n",
      "Run 5, Epoch 57, val_loss = 0.0713\n",
      "Run 5, Epoch 58, val_loss = 0.0645\n",
      "Run 5, Epoch 59, val_loss = 0.0770\n",
      "Run 5, Epoch 60, val_loss = 0.0718\n",
      "Run 5, Epoch 61, val_loss = 0.0703\n",
      "Run 5, Epoch 62, val_loss = 0.0627\n",
      "Run 5, Epoch 63, val_loss = 0.0770\n",
      "Run 5, Epoch 64, val_loss = 0.0761\n",
      "Run 5, Epoch 65, val_loss = 0.0709\n",
      "Run 5, Epoch 66, val_loss = 0.0829\n",
      "Run 5, Epoch 67, val_loss = 0.0680\n",
      "Run 5, Epoch 68, val_loss = 0.0718\n",
      "Run 5, Epoch 69, val_loss = 0.0428\n",
      "Run 5, Epoch 70, val_loss = 0.0744\n",
      "Run 5, Epoch 71, val_loss = 0.0534\n",
      "Run 5, Epoch 72, val_loss = 0.0509\n",
      "Run 5, Epoch 73, val_loss = 0.0919\n",
      "Run 5, Epoch 74, val_loss = 0.0640\n",
      "Run 5, Epoch 75, val_loss = 0.0557\n",
      "Run 5, Epoch 76, val_loss = 0.0591\n",
      "Run 5, Epoch 77, val_loss = 0.0556\n",
      "Run 5, Epoch 78, val_loss = 0.0439\n",
      "Run 5, Epoch 79, val_loss = 0.0609\n",
      "Run 5, Epoch 80, val_loss = 0.0477\n",
      "Run 5, Epoch 81, val_loss = 0.0671\n",
      "Run 5, Epoch 82, val_loss = 0.0543\n",
      "Run 5, Epoch 83, val_loss = 0.0470\n",
      "Run 5, Epoch 84, val_loss = 0.0621\n",
      "Run 5, Epoch 85, val_loss = 0.0450\n",
      "Run 5, Epoch 86, val_loss = 0.0426\n",
      "Run 5, Epoch 87, val_loss = 0.0515\n",
      "Run 5, Epoch 88, val_loss = 0.0404\n",
      "Run 5, Epoch 89, val_loss = 0.0398\n",
      "Run 5, Epoch 90, val_loss = 0.0394\n",
      "Run 5, Epoch 91, val_loss = 0.0461\n",
      "Run 5, Epoch 92, val_loss = 0.0443\n",
      "Run 5, Epoch 93, val_loss = 0.0440\n",
      "Run 5, Epoch 94, val_loss = 0.0418\n",
      "Run 5, Epoch 95, val_loss = 0.0447\n",
      "Run 5, Epoch 96, val_loss = 0.0511\n",
      "Run 5, Epoch 97, val_loss = 0.0357\n",
      "Run 5, Epoch 98, val_loss = 0.0408\n",
      "Run 5, Epoch 99, val_loss = 0.0408\n",
      "Run 5, Epoch 100, val_loss = 0.0490\n",
      "Run 5, Epoch 101, val_loss = 0.0452\n",
      "Run 5, Epoch 102, val_loss = 0.0441\n",
      "Run 5, Epoch 103, val_loss = 0.0389\n",
      "Run 5, Epoch 104, val_loss = 0.0468\n",
      "Run 5, Epoch 105, val_loss = 0.0417\n",
      "Run 5, Epoch 106, val_loss = 0.0406\n",
      "Run 5, Epoch 107, val_loss = 0.0474\n",
      "Run 5, Epoch 108, val_loss = 0.0457\n",
      "Run 5, Epoch 109, val_loss = 0.0467\n",
      "Run 5, Epoch 110, val_loss = 0.0413\n",
      "Run 5, Epoch 111, val_loss = 0.0356\n",
      "Run 5, Epoch 112, val_loss = 0.0331\n",
      "Run 5, Epoch 113, val_loss = 0.0783\n",
      "Run 5, Epoch 114, val_loss = 0.0371\n",
      "Run 5, Epoch 115, val_loss = 0.0324\n",
      "Run 5, Epoch 116, val_loss = 0.0593\n",
      "Run 5, Epoch 117, val_loss = 0.0442\n",
      "Run 5, Epoch 118, val_loss = 0.0418\n",
      "Run 5, Epoch 119, val_loss = 0.0487\n",
      "Run 5, Epoch 120, val_loss = 0.0482\n",
      "Run 5, Epoch 121, val_loss = 0.0445\n",
      "Run 5, Epoch 122, val_loss = 0.0450\n",
      "Run 5, Epoch 123, val_loss = 0.0446\n",
      "Run 5, Epoch 124, val_loss = 0.0491\n",
      "Run 5, Epoch 125, val_loss = 0.0424\n",
      "Run 5, Epoch 126, val_loss = 0.0371\n",
      "Run 5, Epoch 127, val_loss = 0.0362\n",
      "Run 5, Epoch 128, val_loss = 0.0413\n",
      "Run 5, Epoch 129, val_loss = 0.0409\n",
      "Run 5, Epoch 130, val_loss = 0.0426\n",
      "Run 5, Epoch 131, val_loss = 0.0423\n",
      "Run 5, Epoch 132, val_loss = 0.0509\n",
      "Run 5, Epoch 133, val_loss = 0.0421\n",
      "Run 5, Epoch 134, val_loss = 0.0369\n",
      "Run 5, Epoch 135, val_loss = 0.0404\n",
      "✅ Run 5: Acc = 99.80%, MDE = 0.0083\n",
      "📁 Results & all errors saved to repeat_copy/06\n",
      "\n",
      "[Alpha = 0.7] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.7\n",
      "Run 1, Epoch 1, val_loss = 3.0976\n",
      "Run 1, Epoch 2, val_loss = 1.6765\n",
      "Run 1, Epoch 3, val_loss = 1.0575\n",
      "Run 1, Epoch 4, val_loss = 0.7129\n",
      "Run 1, Epoch 5, val_loss = 0.7663\n",
      "Run 1, Epoch 6, val_loss = 0.6278\n",
      "Run 1, Epoch 7, val_loss = 0.4694\n",
      "Run 1, Epoch 8, val_loss = 0.4500\n",
      "Run 1, Epoch 9, val_loss = 0.5656\n",
      "Run 1, Epoch 10, val_loss = 0.4414\n",
      "Run 1, Epoch 11, val_loss = 0.3022\n",
      "Run 1, Epoch 12, val_loss = 0.2813\n",
      "Run 1, Epoch 13, val_loss = 0.3178\n",
      "Run 1, Epoch 14, val_loss = 0.2892\n",
      "Run 1, Epoch 15, val_loss = 0.3008\n",
      "Run 1, Epoch 16, val_loss = 0.1951\n",
      "Run 1, Epoch 17, val_loss = 0.2273\n",
      "Run 1, Epoch 18, val_loss = 0.2164\n",
      "Run 1, Epoch 19, val_loss = 0.1500\n",
      "Run 1, Epoch 20, val_loss = 0.1356\n",
      "Run 1, Epoch 21, val_loss = 0.2026\n",
      "Run 1, Epoch 22, val_loss = 0.1936\n",
      "Run 1, Epoch 23, val_loss = 0.2212\n",
      "Run 1, Epoch 24, val_loss = 0.1482\n",
      "Run 1, Epoch 25, val_loss = 0.1417\n",
      "Run 1, Epoch 26, val_loss = 0.1512\n",
      "Run 1, Epoch 27, val_loss = 0.1264\n",
      "Run 1, Epoch 28, val_loss = 0.1307\n",
      "Run 1, Epoch 29, val_loss = 0.1408\n",
      "Run 1, Epoch 30, val_loss = 0.1036\n",
      "Run 1, Epoch 31, val_loss = 0.1183\n",
      "Run 1, Epoch 32, val_loss = 0.1773\n",
      "Run 1, Epoch 33, val_loss = 0.1433\n",
      "Run 1, Epoch 34, val_loss = 0.1387\n",
      "Run 1, Epoch 35, val_loss = 0.1524\n",
      "Run 1, Epoch 36, val_loss = 0.1172\n",
      "Run 1, Epoch 37, val_loss = 0.1459\n",
      "Run 1, Epoch 38, val_loss = 0.1226\n",
      "Run 1, Epoch 39, val_loss = 0.1020\n",
      "Run 1, Epoch 40, val_loss = 0.0919\n",
      "Run 1, Epoch 41, val_loss = 0.1117\n",
      "Run 1, Epoch 42, val_loss = 0.1125\n",
      "Run 1, Epoch 43, val_loss = 0.0836\n",
      "Run 1, Epoch 44, val_loss = 0.0870\n",
      "Run 1, Epoch 45, val_loss = 0.1256\n",
      "Run 1, Epoch 46, val_loss = 0.1157\n",
      "Run 1, Epoch 47, val_loss = 0.0890\n",
      "Run 1, Epoch 48, val_loss = 0.1217\n",
      "Run 1, Epoch 49, val_loss = 0.0842\n",
      "Run 1, Epoch 50, val_loss = 0.1186\n",
      "Run 1, Epoch 51, val_loss = 0.0897\n",
      "Run 1, Epoch 52, val_loss = 0.0904\n",
      "Run 1, Epoch 53, val_loss = 0.0822\n",
      "Run 1, Epoch 54, val_loss = 0.1074\n",
      "Run 1, Epoch 55, val_loss = 0.1094\n",
      "Run 1, Epoch 56, val_loss = 0.0825\n",
      "Run 1, Epoch 57, val_loss = 0.0909\n",
      "Run 1, Epoch 58, val_loss = 0.1076\n",
      "Run 1, Epoch 59, val_loss = 0.0731\n",
      "Run 1, Epoch 60, val_loss = 0.0786\n",
      "Run 1, Epoch 61, val_loss = 0.0613\n",
      "Run 1, Epoch 62, val_loss = 0.0646\n",
      "Run 1, Epoch 63, val_loss = 0.1118\n",
      "Run 1, Epoch 64, val_loss = 0.1129\n",
      "Run 1, Epoch 65, val_loss = 0.0818\n",
      "Run 1, Epoch 66, val_loss = 0.1021\n",
      "Run 1, Epoch 67, val_loss = 0.0835\n",
      "Run 1, Epoch 68, val_loss = 0.0689\n",
      "Run 1, Epoch 69, val_loss = 0.0680\n",
      "Run 1, Epoch 70, val_loss = 0.0836\n",
      "Run 1, Epoch 71, val_loss = 0.0822\n",
      "Run 1, Epoch 72, val_loss = 0.0763\n",
      "Run 1, Epoch 73, val_loss = 0.0700\n",
      "Run 1, Epoch 74, val_loss = 0.0751\n",
      "Run 1, Epoch 75, val_loss = 0.0820\n",
      "Run 1, Epoch 76, val_loss = 0.0829\n",
      "Run 1, Epoch 77, val_loss = 0.0832\n",
      "Run 1, Epoch 78, val_loss = 0.0746\n",
      "Run 1, Epoch 79, val_loss = 0.0583\n",
      "Run 1, Epoch 80, val_loss = 0.0533\n",
      "Run 1, Epoch 81, val_loss = 0.0768\n",
      "Run 1, Epoch 82, val_loss = 0.0626\n",
      "Run 1, Epoch 83, val_loss = 0.0553\n",
      "Run 1, Epoch 84, val_loss = 0.0602\n",
      "Run 1, Epoch 85, val_loss = 0.0484\n",
      "Run 1, Epoch 86, val_loss = 0.0596\n",
      "Run 1, Epoch 87, val_loss = 0.0762\n",
      "Run 1, Epoch 88, val_loss = 0.0515\n",
      "Run 1, Epoch 89, val_loss = 0.0794\n",
      "Run 1, Epoch 90, val_loss = 0.0481\n",
      "Run 1, Epoch 91, val_loss = 0.0512\n",
      "Run 1, Epoch 92, val_loss = 0.0591\n",
      "Run 1, Epoch 93, val_loss = 0.0505\n",
      "Run 1, Epoch 94, val_loss = 0.0508\n",
      "Run 1, Epoch 95, val_loss = 0.0472\n",
      "Run 1, Epoch 96, val_loss = 0.0654\n",
      "Run 1, Epoch 97, val_loss = 0.0709\n",
      "Run 1, Epoch 98, val_loss = 0.0430\n",
      "Run 1, Epoch 99, val_loss = 0.0647\n",
      "Run 1, Epoch 100, val_loss = 0.0507\n",
      "Run 1, Epoch 101, val_loss = 0.0447\n",
      "Run 1, Epoch 102, val_loss = 0.0549\n",
      "Run 1, Epoch 103, val_loss = 0.0520\n",
      "Run 1, Epoch 104, val_loss = 0.0518\n",
      "Run 1, Epoch 105, val_loss = 0.0640\n",
      "Run 1, Epoch 106, val_loss = 0.0629\n",
      "Run 1, Epoch 107, val_loss = 0.0516\n",
      "Run 1, Epoch 108, val_loss = 0.0510\n",
      "Run 1, Epoch 109, val_loss = 0.0527\n",
      "Run 1, Epoch 110, val_loss = 0.0535\n",
      "Run 1, Epoch 111, val_loss = 0.0561\n",
      "Run 1, Epoch 112, val_loss = 0.0615\n",
      "Run 1, Epoch 113, val_loss = 0.0512\n",
      "Run 1, Epoch 114, val_loss = 0.0695\n",
      "Run 1, Epoch 115, val_loss = 0.0467\n",
      "Run 1, Epoch 116, val_loss = 0.0498\n",
      "Run 1, Epoch 117, val_loss = 0.0442\n",
      "Run 1, Epoch 118, val_loss = 0.0460\n",
      "✅ Run 1: Acc = 99.64%, MDE = 0.0089\n",
      "Run 2/5 - Alpha = 0.7\n",
      "Run 2, Epoch 1, val_loss = 3.1972\n",
      "Run 2, Epoch 2, val_loss = 1.5274\n",
      "Run 2, Epoch 3, val_loss = 1.0667\n",
      "Run 2, Epoch 4, val_loss = 0.7550\n",
      "Run 2, Epoch 5, val_loss = 0.5322\n",
      "Run 2, Epoch 6, val_loss = 0.6211\n",
      "Run 2, Epoch 7, val_loss = 0.5520\n",
      "Run 2, Epoch 8, val_loss = 0.4924\n",
      "Run 2, Epoch 9, val_loss = 0.6017\n",
      "Run 2, Epoch 10, val_loss = 0.4513\n",
      "Run 2, Epoch 11, val_loss = 0.3069\n",
      "Run 2, Epoch 12, val_loss = 0.4113\n",
      "Run 2, Epoch 13, val_loss = 0.4224\n",
      "Run 2, Epoch 14, val_loss = 0.2219\n",
      "Run 2, Epoch 15, val_loss = 0.2468\n",
      "Run 2, Epoch 16, val_loss = 0.3091\n",
      "Run 2, Epoch 17, val_loss = 0.2437\n",
      "Run 2, Epoch 18, val_loss = 0.2470\n",
      "Run 2, Epoch 19, val_loss = 0.2634\n",
      "Run 2, Epoch 20, val_loss = 0.1768\n",
      "Run 2, Epoch 21, val_loss = 0.1587\n",
      "Run 2, Epoch 22, val_loss = 0.2126\n",
      "Run 2, Epoch 23, val_loss = 0.2170\n",
      "Run 2, Epoch 24, val_loss = 0.1190\n",
      "Run 2, Epoch 25, val_loss = 0.1831\n",
      "Run 2, Epoch 26, val_loss = 0.1475\n",
      "Run 2, Epoch 27, val_loss = 0.1697\n",
      "Run 2, Epoch 28, val_loss = 0.1628\n",
      "Run 2, Epoch 29, val_loss = 0.1768\n",
      "Run 2, Epoch 30, val_loss = 0.1228\n",
      "Run 2, Epoch 31, val_loss = 0.1173\n",
      "Run 2, Epoch 32, val_loss = 0.1073\n",
      "Run 2, Epoch 33, val_loss = 0.1856\n",
      "Run 2, Epoch 34, val_loss = 0.1078\n",
      "Run 2, Epoch 35, val_loss = 0.2252\n",
      "Run 2, Epoch 36, val_loss = 0.1309\n",
      "Run 2, Epoch 37, val_loss = 0.0960\n",
      "Run 2, Epoch 38, val_loss = 0.1029\n",
      "Run 2, Epoch 39, val_loss = 0.2493\n",
      "Run 2, Epoch 40, val_loss = 0.0984\n",
      "Run 2, Epoch 41, val_loss = 0.0959\n",
      "Run 2, Epoch 42, val_loss = 0.0864\n",
      "Run 2, Epoch 43, val_loss = 0.0969\n",
      "Run 2, Epoch 44, val_loss = 0.0937\n",
      "Run 2, Epoch 45, val_loss = 0.0936\n",
      "Run 2, Epoch 46, val_loss = 0.0959\n",
      "Run 2, Epoch 47, val_loss = 0.0700\n",
      "Run 2, Epoch 48, val_loss = 0.1001\n",
      "Run 2, Epoch 49, val_loss = 0.0853\n",
      "Run 2, Epoch 50, val_loss = 0.0822\n",
      "Run 2, Epoch 51, val_loss = 0.0727\n",
      "Run 2, Epoch 52, val_loss = 0.0710\n",
      "Run 2, Epoch 53, val_loss = 0.0724\n",
      "Run 2, Epoch 54, val_loss = 0.0650\n",
      "Run 2, Epoch 55, val_loss = 0.0663\n",
      "Run 2, Epoch 56, val_loss = 0.0789\n",
      "Run 2, Epoch 57, val_loss = 0.0957\n",
      "Run 2, Epoch 58, val_loss = 0.0765\n",
      "Run 2, Epoch 59, val_loss = 0.0886\n",
      "Run 2, Epoch 60, val_loss = 0.0693\n",
      "Run 2, Epoch 61, val_loss = 0.0672\n",
      "Run 2, Epoch 62, val_loss = 0.0697\n",
      "Run 2, Epoch 63, val_loss = 0.0771\n",
      "Run 2, Epoch 64, val_loss = 0.0654\n",
      "Run 2, Epoch 65, val_loss = 0.0529\n",
      "Run 2, Epoch 66, val_loss = 0.0815\n",
      "Run 2, Epoch 67, val_loss = 0.0782\n",
      "Run 2, Epoch 68, val_loss = 0.0813\n",
      "Run 2, Epoch 69, val_loss = 0.1022\n",
      "Run 2, Epoch 70, val_loss = 0.1096\n",
      "Run 2, Epoch 71, val_loss = 0.0486\n",
      "Run 2, Epoch 72, val_loss = 0.0751\n",
      "Run 2, Epoch 73, val_loss = 0.0610\n",
      "Run 2, Epoch 74, val_loss = 0.0935\n",
      "Run 2, Epoch 75, val_loss = 0.0921\n",
      "Run 2, Epoch 76, val_loss = 0.0707\n",
      "Run 2, Epoch 77, val_loss = 0.0701\n",
      "Run 2, Epoch 78, val_loss = 0.0644\n",
      "Run 2, Epoch 79, val_loss = 0.0521\n",
      "Run 2, Epoch 80, val_loss = 0.0584\n",
      "Run 2, Epoch 81, val_loss = 0.0728\n",
      "Run 2, Epoch 82, val_loss = 0.0563\n",
      "Run 2, Epoch 83, val_loss = 0.0548\n",
      "Run 2, Epoch 84, val_loss = 0.0852\n",
      "Run 2, Epoch 85, val_loss = 0.0719\n",
      "Run 2, Epoch 86, val_loss = 0.0816\n",
      "Run 2, Epoch 87, val_loss = 0.0692\n",
      "Run 2, Epoch 88, val_loss = 0.0526\n",
      "Run 2, Epoch 89, val_loss = 0.0469\n",
      "Run 2, Epoch 90, val_loss = 0.0474\n",
      "Run 2, Epoch 91, val_loss = 0.0493\n",
      "Run 2, Epoch 92, val_loss = 0.0467\n",
      "Run 2, Epoch 93, val_loss = 0.0595\n",
      "Run 2, Epoch 94, val_loss = 0.0518\n",
      "Run 2, Epoch 95, val_loss = 0.0487\n",
      "Run 2, Epoch 96, val_loss = 0.0419\n",
      "Run 2, Epoch 97, val_loss = 0.0402\n",
      "Run 2, Epoch 98, val_loss = 0.0476\n",
      "Run 2, Epoch 99, val_loss = 0.0443\n",
      "Run 2, Epoch 100, val_loss = 0.0759\n",
      "Run 2, Epoch 101, val_loss = 0.0383\n",
      "Run 2, Epoch 102, val_loss = 0.0442\n",
      "Run 2, Epoch 103, val_loss = 0.0440\n",
      "Run 2, Epoch 104, val_loss = 0.0616\n",
      "Run 2, Epoch 105, val_loss = 0.0496\n",
      "Run 2, Epoch 106, val_loss = 0.0741\n",
      "Run 2, Epoch 107, val_loss = 0.0664\n",
      "Run 2, Epoch 108, val_loss = 0.0617\n",
      "Run 2, Epoch 109, val_loss = 0.0499\n",
      "Run 2, Epoch 110, val_loss = 0.0622\n",
      "Run 2, Epoch 111, val_loss = 0.0531\n",
      "Run 2, Epoch 112, val_loss = 0.0522\n",
      "Run 2, Epoch 113, val_loss = 0.0516\n",
      "Run 2, Epoch 114, val_loss = 0.0473\n",
      "Run 2, Epoch 115, val_loss = 0.0479\n",
      "Run 2, Epoch 116, val_loss = 0.0796\n",
      "Run 2, Epoch 117, val_loss = 0.0536\n",
      "Run 2, Epoch 118, val_loss = 0.0501\n",
      "Run 2, Epoch 119, val_loss = 0.0583\n",
      "Run 2, Epoch 120, val_loss = 0.0479\n",
      "Run 2, Epoch 121, val_loss = 0.0497\n",
      "✅ Run 2: Acc = 99.64%, MDE = 0.0105\n",
      "Run 3/5 - Alpha = 0.7\n",
      "Run 3, Epoch 1, val_loss = 3.0102\n",
      "Run 3, Epoch 2, val_loss = 1.5013\n",
      "Run 3, Epoch 3, val_loss = 1.2745\n",
      "Run 3, Epoch 4, val_loss = 0.9351\n",
      "Run 3, Epoch 5, val_loss = 0.6731\n",
      "Run 3, Epoch 6, val_loss = 0.6784\n",
      "Run 3, Epoch 7, val_loss = 0.5205\n",
      "Run 3, Epoch 8, val_loss = 0.4896\n",
      "Run 3, Epoch 9, val_loss = 0.4171\n",
      "Run 3, Epoch 10, val_loss = 0.4239\n",
      "Run 3, Epoch 11, val_loss = 0.3713\n",
      "Run 3, Epoch 12, val_loss = 0.3938\n",
      "Run 3, Epoch 13, val_loss = 0.2994\n",
      "Run 3, Epoch 14, val_loss = 0.2736\n",
      "Run 3, Epoch 15, val_loss = 0.2523\n",
      "Run 3, Epoch 16, val_loss = 0.2541\n",
      "Run 3, Epoch 17, val_loss = 0.2473\n",
      "Run 3, Epoch 18, val_loss = 0.1994\n",
      "Run 3, Epoch 19, val_loss = 0.1790\n",
      "Run 3, Epoch 20, val_loss = 0.1910\n",
      "Run 3, Epoch 21, val_loss = 0.2132\n",
      "Run 3, Epoch 22, val_loss = 0.2004\n",
      "Run 3, Epoch 23, val_loss = 0.1891\n",
      "Run 3, Epoch 24, val_loss = 0.1964\n",
      "Run 3, Epoch 25, val_loss = 0.1609\n",
      "Run 3, Epoch 26, val_loss = 0.1109\n",
      "Run 3, Epoch 27, val_loss = 0.1694\n",
      "Run 3, Epoch 28, val_loss = 0.1156\n",
      "Run 3, Epoch 29, val_loss = 0.1660\n",
      "Run 3, Epoch 30, val_loss = 0.1219\n",
      "Run 3, Epoch 31, val_loss = 0.1280\n",
      "Run 3, Epoch 32, val_loss = 0.1427\n",
      "Run 3, Epoch 33, val_loss = 0.1335\n",
      "Run 3, Epoch 34, val_loss = 0.1180\n",
      "Run 3, Epoch 35, val_loss = 0.0990\n",
      "Run 3, Epoch 36, val_loss = 0.0997\n",
      "Run 3, Epoch 37, val_loss = 0.1108\n",
      "Run 3, Epoch 38, val_loss = 0.1110\n",
      "Run 3, Epoch 39, val_loss = 0.1353\n",
      "Run 3, Epoch 40, val_loss = 0.0914\n",
      "Run 3, Epoch 41, val_loss = 0.0974\n",
      "Run 3, Epoch 42, val_loss = 0.1017\n",
      "Run 3, Epoch 43, val_loss = 0.1231\n",
      "Run 3, Epoch 44, val_loss = 0.1028\n",
      "Run 3, Epoch 45, val_loss = 0.1040\n",
      "Run 3, Epoch 46, val_loss = 0.0961\n",
      "Run 3, Epoch 47, val_loss = 0.0736\n",
      "Run 3, Epoch 48, val_loss = 0.0957\n",
      "Run 3, Epoch 49, val_loss = 0.0987\n",
      "Run 3, Epoch 50, val_loss = 0.1168\n",
      "Run 3, Epoch 51, val_loss = 0.0804\n",
      "Run 3, Epoch 52, val_loss = 0.0815\n",
      "Run 3, Epoch 53, val_loss = 0.0962\n",
      "Run 3, Epoch 54, val_loss = 0.0740\n",
      "Run 3, Epoch 55, val_loss = 0.0943\n",
      "Run 3, Epoch 56, val_loss = 0.1295\n",
      "Run 3, Epoch 57, val_loss = 0.0700\n",
      "Run 3, Epoch 58, val_loss = 0.0804\n",
      "Run 3, Epoch 59, val_loss = 0.0954\n",
      "Run 3, Epoch 60, val_loss = 0.0849\n",
      "Run 3, Epoch 61, val_loss = 0.0651\n",
      "Run 3, Epoch 62, val_loss = 0.0728\n",
      "Run 3, Epoch 63, val_loss = 0.1275\n",
      "Run 3, Epoch 64, val_loss = 0.0622\n",
      "Run 3, Epoch 65, val_loss = 0.0889\n",
      "Run 3, Epoch 66, val_loss = 0.0616\n",
      "Run 3, Epoch 67, val_loss = 0.0931\n",
      "Run 3, Epoch 68, val_loss = 0.0985\n",
      "Run 3, Epoch 69, val_loss = 0.0941\n",
      "Run 3, Epoch 70, val_loss = 0.0621\n",
      "Run 3, Epoch 71, val_loss = 0.0629\n",
      "Run 3, Epoch 72, val_loss = 0.0666\n",
      "Run 3, Epoch 73, val_loss = 0.1047\n",
      "Run 3, Epoch 74, val_loss = 0.0619\n",
      "Run 3, Epoch 75, val_loss = 0.0588\n",
      "Run 3, Epoch 76, val_loss = 0.0604\n",
      "Run 3, Epoch 77, val_loss = 0.0558\n",
      "Run 3, Epoch 78, val_loss = 0.0577\n",
      "Run 3, Epoch 79, val_loss = 0.0568\n",
      "Run 3, Epoch 80, val_loss = 0.0726\n",
      "Run 3, Epoch 81, val_loss = 0.0600\n",
      "Run 3, Epoch 82, val_loss = 0.0548\n",
      "Run 3, Epoch 83, val_loss = 0.0592\n",
      "Run 3, Epoch 84, val_loss = 0.1006\n",
      "Run 3, Epoch 85, val_loss = 0.0557\n",
      "Run 3, Epoch 86, val_loss = 0.0526\n",
      "Run 3, Epoch 87, val_loss = 0.0671\n",
      "Run 3, Epoch 88, val_loss = 0.0646\n",
      "Run 3, Epoch 89, val_loss = 0.0696\n",
      "Run 3, Epoch 90, val_loss = 0.0530\n",
      "Run 3, Epoch 91, val_loss = 0.0579\n",
      "Run 3, Epoch 92, val_loss = 0.0593\n",
      "Run 3, Epoch 93, val_loss = 0.0559\n",
      "Run 3, Epoch 94, val_loss = 0.0658\n",
      "Run 3, Epoch 95, val_loss = 0.0660\n",
      "Run 3, Epoch 96, val_loss = 0.0623\n",
      "Run 3, Epoch 97, val_loss = 0.0602\n",
      "Run 3, Epoch 98, val_loss = 0.0650\n",
      "Run 3, Epoch 99, val_loss = 0.0640\n",
      "Run 3, Epoch 100, val_loss = 0.0625\n",
      "Run 3, Epoch 101, val_loss = 0.0677\n",
      "Run 3, Epoch 102, val_loss = 0.0723\n",
      "Run 3, Epoch 103, val_loss = 0.0485\n",
      "Run 3, Epoch 104, val_loss = 0.0499\n",
      "Run 3, Epoch 105, val_loss = 0.0576\n",
      "Run 3, Epoch 106, val_loss = 0.0631\n",
      "Run 3, Epoch 107, val_loss = 0.0542\n",
      "Run 3, Epoch 108, val_loss = 0.0488\n",
      "Run 3, Epoch 109, val_loss = 0.0443\n",
      "Run 3, Epoch 110, val_loss = 0.0499\n",
      "Run 3, Epoch 111, val_loss = 0.0498\n",
      "Run 3, Epoch 112, val_loss = 0.0432\n",
      "Run 3, Epoch 113, val_loss = 0.0443\n",
      "Run 3, Epoch 114, val_loss = 0.0567\n",
      "Run 3, Epoch 115, val_loss = 0.0463\n",
      "Run 3, Epoch 116, val_loss = 0.0425\n",
      "Run 3, Epoch 117, val_loss = 0.0483\n",
      "Run 3, Epoch 118, val_loss = 0.0468\n",
      "Run 3, Epoch 119, val_loss = 0.0634\n",
      "Run 3, Epoch 120, val_loss = 0.0665\n",
      "Run 3, Epoch 121, val_loss = 0.0558\n",
      "Run 3, Epoch 122, val_loss = 0.0468\n",
      "Run 3, Epoch 123, val_loss = 0.0407\n",
      "Run 3, Epoch 124, val_loss = 0.0479\n",
      "Run 3, Epoch 125, val_loss = 0.0652\n",
      "Run 3, Epoch 126, val_loss = 0.0354\n",
      "Run 3, Epoch 127, val_loss = 0.0501\n",
      "Run 3, Epoch 128, val_loss = 0.0476\n",
      "Run 3, Epoch 129, val_loss = 0.0482\n",
      "Run 3, Epoch 130, val_loss = 0.0469\n",
      "Run 3, Epoch 131, val_loss = 0.0565\n",
      "Run 3, Epoch 132, val_loss = 0.0473\n",
      "Run 3, Epoch 133, val_loss = 0.0505\n",
      "Run 3, Epoch 134, val_loss = 0.0458\n",
      "Run 3, Epoch 135, val_loss = 0.0429\n",
      "Run 3, Epoch 136, val_loss = 0.0466\n",
      "Run 3, Epoch 137, val_loss = 0.0494\n",
      "Run 3, Epoch 138, val_loss = 0.0470\n",
      "Run 3, Epoch 139, val_loss = 0.0665\n",
      "Run 3, Epoch 140, val_loss = 0.0433\n",
      "Run 3, Epoch 141, val_loss = 0.0454\n",
      "Run 3, Epoch 142, val_loss = 0.0530\n",
      "Run 3, Epoch 143, val_loss = 0.0447\n",
      "Run 3, Epoch 144, val_loss = 0.0471\n",
      "Run 3, Epoch 145, val_loss = 0.0464\n",
      "Run 3, Epoch 146, val_loss = 0.0483\n",
      "✅ Run 3: Acc = 99.75%, MDE = 0.0125\n",
      "Run 4/5 - Alpha = 0.7\n",
      "Run 4, Epoch 1, val_loss = 3.4125\n",
      "Run 4, Epoch 2, val_loss = 1.4604\n",
      "Run 4, Epoch 3, val_loss = 1.0393\n",
      "Run 4, Epoch 4, val_loss = 0.7312\n",
      "Run 4, Epoch 5, val_loss = 0.5920\n",
      "Run 4, Epoch 6, val_loss = 0.4903\n",
      "Run 4, Epoch 7, val_loss = 0.3820\n",
      "Run 4, Epoch 8, val_loss = 0.5121\n",
      "Run 4, Epoch 9, val_loss = 0.3391\n",
      "Run 4, Epoch 10, val_loss = 0.4006\n",
      "Run 4, Epoch 11, val_loss = 0.3001\n",
      "Run 4, Epoch 12, val_loss = 0.3389\n",
      "Run 4, Epoch 13, val_loss = 0.3182\n",
      "Run 4, Epoch 14, val_loss = 0.3265\n",
      "Run 4, Epoch 15, val_loss = 0.2368\n",
      "Run 4, Epoch 16, val_loss = 0.2146\n",
      "Run 4, Epoch 17, val_loss = 0.2711\n",
      "Run 4, Epoch 18, val_loss = 0.1865\n",
      "Run 4, Epoch 19, val_loss = 0.1662\n",
      "Run 4, Epoch 20, val_loss = 0.1813\n",
      "Run 4, Epoch 21, val_loss = 0.1788\n",
      "Run 4, Epoch 22, val_loss = 0.1676\n",
      "Run 4, Epoch 23, val_loss = 0.1900\n",
      "Run 4, Epoch 24, val_loss = 0.2075\n",
      "Run 4, Epoch 25, val_loss = 0.1918\n",
      "Run 4, Epoch 26, val_loss = 0.1629\n",
      "Run 4, Epoch 27, val_loss = 0.1650\n",
      "Run 4, Epoch 28, val_loss = 0.1581\n",
      "Run 4, Epoch 29, val_loss = 0.1806\n",
      "Run 4, Epoch 30, val_loss = 0.1294\n",
      "Run 4, Epoch 31, val_loss = 0.1257\n",
      "Run 4, Epoch 32, val_loss = 0.1241\n",
      "Run 4, Epoch 33, val_loss = 0.1131\n",
      "Run 4, Epoch 34, val_loss = 0.1141\n",
      "Run 4, Epoch 35, val_loss = 0.1287\n",
      "Run 4, Epoch 36, val_loss = 0.1191\n",
      "Run 4, Epoch 37, val_loss = 0.0826\n",
      "Run 4, Epoch 38, val_loss = 0.0917\n",
      "Run 4, Epoch 39, val_loss = 0.0982\n",
      "Run 4, Epoch 40, val_loss = 0.0944\n",
      "Run 4, Epoch 41, val_loss = 0.0967\n",
      "Run 4, Epoch 42, val_loss = 0.1088\n",
      "Run 4, Epoch 43, val_loss = 0.1532\n",
      "Run 4, Epoch 44, val_loss = 0.0860\n",
      "Run 4, Epoch 45, val_loss = 0.0821\n",
      "Run 4, Epoch 46, val_loss = 0.0927\n",
      "Run 4, Epoch 47, val_loss = 0.0656\n",
      "Run 4, Epoch 48, val_loss = 0.0786\n",
      "Run 4, Epoch 49, val_loss = 0.0822\n",
      "Run 4, Epoch 50, val_loss = 0.0782\n",
      "Run 4, Epoch 51, val_loss = 0.0636\n",
      "Run 4, Epoch 52, val_loss = 0.0670\n",
      "Run 4, Epoch 53, val_loss = 0.0826\n",
      "Run 4, Epoch 54, val_loss = 0.0677\n",
      "Run 4, Epoch 55, val_loss = 0.0769\n",
      "Run 4, Epoch 56, val_loss = 0.0821\n",
      "Run 4, Epoch 57, val_loss = 0.0605\n",
      "Run 4, Epoch 58, val_loss = 0.0684\n",
      "Run 4, Epoch 59, val_loss = 0.0885\n",
      "Run 4, Epoch 60, val_loss = 0.0591\n",
      "Run 4, Epoch 61, val_loss = 0.0794\n",
      "Run 4, Epoch 62, val_loss = 0.0759\n",
      "Run 4, Epoch 63, val_loss = 0.0767\n",
      "Run 4, Epoch 64, val_loss = 0.0653\n",
      "Run 4, Epoch 65, val_loss = 0.0654\n",
      "Run 4, Epoch 66, val_loss = 0.0709\n",
      "Run 4, Epoch 67, val_loss = 0.0687\n",
      "Run 4, Epoch 68, val_loss = 0.0709\n",
      "Run 4, Epoch 69, val_loss = 0.0599\n",
      "Run 4, Epoch 70, val_loss = 0.0544\n",
      "Run 4, Epoch 71, val_loss = 0.0705\n",
      "Run 4, Epoch 72, val_loss = 0.0793\n",
      "Run 4, Epoch 73, val_loss = 0.0703\n",
      "Run 4, Epoch 74, val_loss = 0.0672\n",
      "Run 4, Epoch 75, val_loss = 0.0785\n",
      "Run 4, Epoch 76, val_loss = 0.0702\n",
      "Run 4, Epoch 77, val_loss = 0.0931\n",
      "Run 4, Epoch 78, val_loss = 0.0819\n",
      "Run 4, Epoch 79, val_loss = 0.0622\n",
      "Run 4, Epoch 80, val_loss = 0.0794\n",
      "Run 4, Epoch 81, val_loss = 0.0728\n",
      "Run 4, Epoch 82, val_loss = 0.0783\n",
      "Run 4, Epoch 83, val_loss = 0.0459\n",
      "Run 4, Epoch 84, val_loss = 0.0712\n",
      "Run 4, Epoch 85, val_loss = 0.0654\n",
      "Run 4, Epoch 86, val_loss = 0.0619\n",
      "Run 4, Epoch 87, val_loss = 0.0639\n",
      "Run 4, Epoch 88, val_loss = 0.0600\n",
      "Run 4, Epoch 89, val_loss = 0.0805\n",
      "Run 4, Epoch 90, val_loss = 0.1119\n",
      "Run 4, Epoch 91, val_loss = 0.0735\n",
      "Run 4, Epoch 92, val_loss = 0.0678\n",
      "Run 4, Epoch 93, val_loss = 0.0579\n",
      "Run 4, Epoch 94, val_loss = 0.0826\n",
      "Run 4, Epoch 95, val_loss = 0.0915\n",
      "Run 4, Epoch 96, val_loss = 0.0648\n",
      "Run 4, Epoch 97, val_loss = 0.0634\n",
      "Run 4, Epoch 98, val_loss = 0.0608\n",
      "Run 4, Epoch 99, val_loss = 0.0465\n",
      "Run 4, Epoch 100, val_loss = 0.0380\n",
      "Run 4, Epoch 101, val_loss = 0.0398\n",
      "Run 4, Epoch 102, val_loss = 0.0514\n",
      "Run 4, Epoch 103, val_loss = 0.0411\n",
      "Run 4, Epoch 104, val_loss = 0.0556\n",
      "Run 4, Epoch 105, val_loss = 0.0392\n",
      "Run 4, Epoch 106, val_loss = 0.0429\n",
      "Run 4, Epoch 107, val_loss = 0.0483\n",
      "Run 4, Epoch 108, val_loss = 0.0409\n",
      "Run 4, Epoch 109, val_loss = 0.0414\n",
      "Run 4, Epoch 110, val_loss = 0.0437\n",
      "Run 4, Epoch 111, val_loss = 0.0431\n",
      "Run 4, Epoch 112, val_loss = 0.0453\n",
      "Run 4, Epoch 113, val_loss = 0.0435\n",
      "Run 4, Epoch 114, val_loss = 0.0490\n",
      "Run 4, Epoch 115, val_loss = 0.0501\n",
      "Run 4, Epoch 116, val_loss = 0.0405\n",
      "Run 4, Epoch 117, val_loss = 0.0405\n",
      "Run 4, Epoch 118, val_loss = 0.0406\n",
      "Run 4, Epoch 119, val_loss = 0.0418\n",
      "Run 4, Epoch 120, val_loss = 0.0425\n",
      "✅ Run 4: Acc = 99.64%, MDE = 0.0093\n",
      "Run 5/5 - Alpha = 0.7\n",
      "Run 5, Epoch 1, val_loss = 3.1990\n",
      "Run 5, Epoch 2, val_loss = 1.8385\n",
      "Run 5, Epoch 3, val_loss = 1.1642\n",
      "Run 5, Epoch 4, val_loss = 0.8099\n",
      "Run 5, Epoch 5, val_loss = 0.7126\n",
      "Run 5, Epoch 6, val_loss = 0.6153\n",
      "Run 5, Epoch 7, val_loss = 0.5268\n",
      "Run 5, Epoch 8, val_loss = 0.4107\n",
      "Run 5, Epoch 9, val_loss = 0.3751\n",
      "Run 5, Epoch 10, val_loss = 0.3816\n",
      "Run 5, Epoch 11, val_loss = 0.3626\n",
      "Run 5, Epoch 12, val_loss = 0.3273\n",
      "Run 5, Epoch 13, val_loss = 0.3165\n",
      "Run 5, Epoch 14, val_loss = 0.4082\n",
      "Run 5, Epoch 15, val_loss = 0.2535\n",
      "Run 5, Epoch 16, val_loss = 0.2117\n",
      "Run 5, Epoch 17, val_loss = 0.1922\n",
      "Run 5, Epoch 18, val_loss = 0.2149\n",
      "Run 5, Epoch 19, val_loss = 0.2241\n",
      "Run 5, Epoch 20, val_loss = 0.2241\n",
      "Run 5, Epoch 21, val_loss = 0.1630\n",
      "Run 5, Epoch 22, val_loss = 0.1748\n",
      "Run 5, Epoch 23, val_loss = 0.1852\n",
      "Run 5, Epoch 24, val_loss = 0.1479\n",
      "Run 5, Epoch 25, val_loss = 0.1514\n",
      "Run 5, Epoch 26, val_loss = 0.2092\n",
      "Run 5, Epoch 27, val_loss = 0.1497\n",
      "Run 5, Epoch 28, val_loss = 0.1349\n",
      "Run 5, Epoch 29, val_loss = 0.1380\n",
      "Run 5, Epoch 30, val_loss = 0.1015\n",
      "Run 5, Epoch 31, val_loss = 0.2261\n",
      "Run 5, Epoch 32, val_loss = 0.1473\n",
      "Run 5, Epoch 33, val_loss = 0.1317\n",
      "Run 5, Epoch 34, val_loss = 0.1339\n",
      "Run 5, Epoch 35, val_loss = 0.1102\n",
      "Run 5, Epoch 36, val_loss = 0.1211\n",
      "Run 5, Epoch 37, val_loss = 0.1108\n",
      "Run 5, Epoch 38, val_loss = 0.1269\n",
      "Run 5, Epoch 39, val_loss = 0.1272\n",
      "Run 5, Epoch 40, val_loss = 0.0838\n",
      "Run 5, Epoch 41, val_loss = 0.1161\n",
      "Run 5, Epoch 42, val_loss = 0.1482\n",
      "Run 5, Epoch 43, val_loss = 0.1090\n",
      "Run 5, Epoch 44, val_loss = 0.0855\n",
      "Run 5, Epoch 45, val_loss = 0.0872\n",
      "Run 5, Epoch 46, val_loss = 0.1052\n",
      "Run 5, Epoch 47, val_loss = 0.1116\n",
      "Run 5, Epoch 48, val_loss = 0.0964\n",
      "Run 5, Epoch 49, val_loss = 0.0954\n",
      "Run 5, Epoch 50, val_loss = 0.0878\n",
      "Run 5, Epoch 51, val_loss = 0.0767\n",
      "Run 5, Epoch 52, val_loss = 0.0708\n",
      "Run 5, Epoch 53, val_loss = 0.0678\n",
      "Run 5, Epoch 54, val_loss = 0.0914\n",
      "Run 5, Epoch 55, val_loss = 0.0818\n",
      "Run 5, Epoch 56, val_loss = 0.0830\n",
      "Run 5, Epoch 57, val_loss = 0.1047\n",
      "Run 5, Epoch 58, val_loss = 0.0827\n",
      "Run 5, Epoch 59, val_loss = 0.1239\n",
      "Run 5, Epoch 60, val_loss = 0.0864\n",
      "Run 5, Epoch 61, val_loss = 0.0749\n",
      "Run 5, Epoch 62, val_loss = 0.0921\n",
      "Run 5, Epoch 63, val_loss = 0.0864\n",
      "Run 5, Epoch 64, val_loss = 0.0905\n",
      "Run 5, Epoch 65, val_loss = 0.0831\n",
      "Run 5, Epoch 66, val_loss = 0.0804\n",
      "Run 5, Epoch 67, val_loss = 0.0892\n",
      "Run 5, Epoch 68, val_loss = 0.1015\n",
      "Run 5, Epoch 69, val_loss = 0.0855\n",
      "Run 5, Epoch 70, val_loss = 0.0575\n",
      "Run 5, Epoch 71, val_loss = 0.0595\n",
      "Run 5, Epoch 72, val_loss = 0.0621\n",
      "Run 5, Epoch 73, val_loss = 0.0593\n",
      "Run 5, Epoch 74, val_loss = 0.1023\n",
      "Run 5, Epoch 75, val_loss = 0.0685\n",
      "Run 5, Epoch 76, val_loss = 0.0617\n",
      "Run 5, Epoch 77, val_loss = 0.0603\n",
      "Run 5, Epoch 78, val_loss = 0.0554\n",
      "Run 5, Epoch 79, val_loss = 0.0620\n",
      "Run 5, Epoch 80, val_loss = 0.0594\n",
      "Run 5, Epoch 81, val_loss = 0.0448\n",
      "Run 5, Epoch 82, val_loss = 0.0637\n",
      "Run 5, Epoch 83, val_loss = 0.0806\n",
      "Run 5, Epoch 84, val_loss = 0.1050\n",
      "Run 5, Epoch 85, val_loss = 0.0740\n",
      "Run 5, Epoch 86, val_loss = 0.0586\n",
      "Run 5, Epoch 87, val_loss = 0.0604\n",
      "Run 5, Epoch 88, val_loss = 0.0754\n",
      "Run 5, Epoch 89, val_loss = 0.0534\n",
      "Run 5, Epoch 90, val_loss = 0.0679\n",
      "Run 5, Epoch 91, val_loss = 0.0638\n",
      "Run 5, Epoch 92, val_loss = 0.0645\n",
      "Run 5, Epoch 93, val_loss = 0.0559\n",
      "Run 5, Epoch 94, val_loss = 0.0887\n",
      "Run 5, Epoch 95, val_loss = 0.0732\n",
      "Run 5, Epoch 96, val_loss = 0.0603\n",
      "Run 5, Epoch 97, val_loss = 0.0664\n",
      "Run 5, Epoch 98, val_loss = 0.0606\n",
      "Run 5, Epoch 99, val_loss = 0.0663\n",
      "Run 5, Epoch 100, val_loss = 0.0714\n",
      "Run 5, Epoch 101, val_loss = 0.0582\n",
      "✅ Run 5: Acc = 99.69%, MDE = 0.0125\n",
      "📁 Results & all errors saved to repeat_copy/07\n",
      "\n",
      "[Alpha = 0.8] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.8\n",
      "Run 1, Epoch 1, val_loss = 3.7331\n",
      "Run 1, Epoch 2, val_loss = 1.9679\n",
      "Run 1, Epoch 3, val_loss = 1.2741\n",
      "Run 1, Epoch 4, val_loss = 0.8414\n",
      "Run 1, Epoch 5, val_loss = 0.8768\n",
      "Run 1, Epoch 6, val_loss = 0.8568\n",
      "Run 1, Epoch 7, val_loss = 0.6805\n",
      "Run 1, Epoch 8, val_loss = 0.5581\n",
      "Run 1, Epoch 9, val_loss = 0.4235\n",
      "Run 1, Epoch 10, val_loss = 0.4080\n",
      "Run 1, Epoch 11, val_loss = 0.5271\n",
      "Run 1, Epoch 12, val_loss = 0.4188\n",
      "Run 1, Epoch 13, val_loss = 0.3082\n",
      "Run 1, Epoch 14, val_loss = 0.2512\n",
      "Run 1, Epoch 15, val_loss = 0.2881\n",
      "Run 1, Epoch 16, val_loss = 0.3167\n",
      "Run 1, Epoch 17, val_loss = 0.3468\n",
      "Run 1, Epoch 18, val_loss = 0.2315\n",
      "Run 1, Epoch 19, val_loss = 0.2344\n",
      "Run 1, Epoch 20, val_loss = 0.2054\n",
      "Run 1, Epoch 21, val_loss = 0.2058\n",
      "Run 1, Epoch 22, val_loss = 0.2242\n",
      "Run 1, Epoch 23, val_loss = 0.2021\n",
      "Run 1, Epoch 24, val_loss = 0.2419\n",
      "Run 1, Epoch 25, val_loss = 0.1667\n",
      "Run 1, Epoch 26, val_loss = 0.1863\n",
      "Run 1, Epoch 27, val_loss = 0.2471\n",
      "Run 1, Epoch 28, val_loss = 0.2197\n",
      "Run 1, Epoch 29, val_loss = 0.1296\n",
      "Run 1, Epoch 30, val_loss = 0.1723\n",
      "Run 1, Epoch 31, val_loss = 0.1585\n",
      "Run 1, Epoch 32, val_loss = 0.2244\n",
      "Run 1, Epoch 33, val_loss = 0.1634\n",
      "Run 1, Epoch 34, val_loss = 0.1265\n",
      "Run 1, Epoch 35, val_loss = 0.1740\n",
      "Run 1, Epoch 36, val_loss = 0.1240\n",
      "Run 1, Epoch 37, val_loss = 0.1560\n",
      "Run 1, Epoch 38, val_loss = 0.1297\n",
      "Run 1, Epoch 39, val_loss = 0.1247\n",
      "Run 1, Epoch 40, val_loss = 0.1140\n",
      "Run 1, Epoch 41, val_loss = 0.1451\n",
      "Run 1, Epoch 42, val_loss = 0.0875\n",
      "Run 1, Epoch 43, val_loss = 0.1776\n",
      "Run 1, Epoch 44, val_loss = 0.1373\n",
      "Run 1, Epoch 45, val_loss = 0.1119\n",
      "Run 1, Epoch 46, val_loss = 0.1245\n",
      "Run 1, Epoch 47, val_loss = 0.1398\n",
      "Run 1, Epoch 48, val_loss = 0.1448\n",
      "Run 1, Epoch 49, val_loss = 0.1414\n",
      "Run 1, Epoch 50, val_loss = 0.0921\n",
      "Run 1, Epoch 51, val_loss = 0.0946\n",
      "Run 1, Epoch 52, val_loss = 0.1323\n",
      "Run 1, Epoch 53, val_loss = 0.1065\n",
      "Run 1, Epoch 54, val_loss = 0.1023\n",
      "Run 1, Epoch 55, val_loss = 0.1307\n",
      "Run 1, Epoch 56, val_loss = 0.0961\n",
      "Run 1, Epoch 57, val_loss = 0.0890\n",
      "Run 1, Epoch 58, val_loss = 0.1071\n",
      "Run 1, Epoch 59, val_loss = 0.0793\n",
      "Run 1, Epoch 60, val_loss = 0.0721\n",
      "Run 1, Epoch 61, val_loss = 0.0780\n",
      "Run 1, Epoch 62, val_loss = 0.0904\n",
      "Run 1, Epoch 63, val_loss = 0.0783\n",
      "Run 1, Epoch 64, val_loss = 0.0670\n",
      "Run 1, Epoch 65, val_loss = 0.0719\n",
      "Run 1, Epoch 66, val_loss = 0.0746\n",
      "Run 1, Epoch 67, val_loss = 0.0677\n",
      "Run 1, Epoch 68, val_loss = 0.0857\n",
      "Run 1, Epoch 69, val_loss = 0.0687\n",
      "Run 1, Epoch 70, val_loss = 0.0563\n",
      "Run 1, Epoch 71, val_loss = 0.0636\n",
      "Run 1, Epoch 72, val_loss = 0.0622\n",
      "Run 1, Epoch 73, val_loss = 0.0575\n",
      "Run 1, Epoch 74, val_loss = 0.0983\n",
      "Run 1, Epoch 75, val_loss = 0.0614\n",
      "Run 1, Epoch 76, val_loss = 0.0682\n",
      "Run 1, Epoch 77, val_loss = 0.0695\n",
      "Run 1, Epoch 78, val_loss = 0.0968\n",
      "Run 1, Epoch 79, val_loss = 0.0755\n",
      "Run 1, Epoch 80, val_loss = 0.0650\n",
      "Run 1, Epoch 81, val_loss = 0.0664\n",
      "Run 1, Epoch 82, val_loss = 0.0636\n",
      "Run 1, Epoch 83, val_loss = 0.0563\n",
      "Run 1, Epoch 84, val_loss = 0.0749\n",
      "Run 1, Epoch 85, val_loss = 0.0597\n",
      "Run 1, Epoch 86, val_loss = 0.0809\n",
      "Run 1, Epoch 87, val_loss = 0.0641\n",
      "Run 1, Epoch 88, val_loss = 0.0592\n",
      "Run 1, Epoch 89, val_loss = 0.0720\n",
      "Run 1, Epoch 90, val_loss = 0.0669\n",
      "Run 1, Epoch 91, val_loss = 0.0835\n",
      "Run 1, Epoch 92, val_loss = 0.0668\n",
      "Run 1, Epoch 93, val_loss = 0.1117\n",
      "Run 1, Epoch 94, val_loss = 0.1385\n",
      "Run 1, Epoch 95, val_loss = 0.0590\n",
      "Run 1, Epoch 96, val_loss = 0.0651\n",
      "Run 1, Epoch 97, val_loss = 0.0646\n",
      "Run 1, Epoch 98, val_loss = 0.0505\n",
      "Run 1, Epoch 99, val_loss = 0.1266\n",
      "Run 1, Epoch 100, val_loss = 0.0657\n",
      "Run 1, Epoch 101, val_loss = 0.0617\n",
      "Run 1, Epoch 102, val_loss = 0.0655\n",
      "Run 1, Epoch 103, val_loss = 0.0619\n",
      "Run 1, Epoch 104, val_loss = 0.0619\n",
      "Run 1, Epoch 105, val_loss = 0.0511\n",
      "Run 1, Epoch 106, val_loss = 0.0593\n",
      "Run 1, Epoch 107, val_loss = 0.0518\n",
      "Run 1, Epoch 108, val_loss = 0.0577\n",
      "Run 1, Epoch 109, val_loss = 0.0549\n",
      "Run 1, Epoch 110, val_loss = 0.0477\n",
      "Run 1, Epoch 111, val_loss = 0.0540\n",
      "Run 1, Epoch 112, val_loss = 0.0583\n",
      "Run 1, Epoch 113, val_loss = 0.0689\n",
      "Run 1, Epoch 114, val_loss = 0.0532\n",
      "Run 1, Epoch 115, val_loss = 0.0609\n",
      "Run 1, Epoch 116, val_loss = 0.0623\n",
      "Run 1, Epoch 117, val_loss = 0.0542\n",
      "Run 1, Epoch 118, val_loss = 0.0567\n",
      "Run 1, Epoch 119, val_loss = 0.0720\n",
      "Run 1, Epoch 120, val_loss = 0.0587\n",
      "Run 1, Epoch 121, val_loss = 0.0527\n",
      "Run 1, Epoch 122, val_loss = 0.0772\n",
      "Run 1, Epoch 123, val_loss = 0.0543\n",
      "Run 1, Epoch 124, val_loss = 0.0536\n",
      "Run 1, Epoch 125, val_loss = 0.0525\n",
      "Run 1, Epoch 126, val_loss = 0.0709\n",
      "Run 1, Epoch 127, val_loss = 0.0568\n",
      "Run 1, Epoch 128, val_loss = 0.0524\n",
      "Run 1, Epoch 129, val_loss = 0.0472\n",
      "Run 1, Epoch 130, val_loss = 0.0492\n",
      "Run 1, Epoch 131, val_loss = 0.0543\n",
      "Run 1, Epoch 132, val_loss = 0.0502\n",
      "Run 1, Epoch 133, val_loss = 0.0512\n",
      "Run 1, Epoch 134, val_loss = 0.0590\n",
      "Run 1, Epoch 135, val_loss = 0.0498\n",
      "Run 1, Epoch 136, val_loss = 0.0555\n",
      "Run 1, Epoch 137, val_loss = 0.0568\n",
      "Run 1, Epoch 138, val_loss = 0.0461\n",
      "Run 1, Epoch 139, val_loss = 0.0531\n",
      "Run 1, Epoch 140, val_loss = 0.0491\n",
      "Run 1, Epoch 141, val_loss = 0.0476\n",
      "Run 1, Epoch 142, val_loss = 0.0558\n",
      "Run 1, Epoch 143, val_loss = 0.0514\n",
      "Run 1, Epoch 144, val_loss = 0.0518\n",
      "Run 1, Epoch 145, val_loss = 0.0505\n",
      "Run 1, Epoch 146, val_loss = 0.0470\n",
      "Run 1, Epoch 147, val_loss = 0.0481\n",
      "Run 1, Epoch 148, val_loss = 0.0605\n",
      "Run 1, Epoch 149, val_loss = 0.0566\n",
      "Run 1, Epoch 150, val_loss = 0.0511\n",
      "Run 1, Epoch 151, val_loss = 0.0517\n",
      "Run 1, Epoch 152, val_loss = 0.0468\n",
      "Run 1, Epoch 153, val_loss = 0.0508\n",
      "Run 1, Epoch 154, val_loss = 0.0515\n",
      "Run 1, Epoch 155, val_loss = 0.0563\n",
      "Run 1, Epoch 156, val_loss = 0.0445\n",
      "Run 1, Epoch 157, val_loss = 0.0498\n",
      "Run 1, Epoch 158, val_loss = 0.0603\n",
      "Run 1, Epoch 159, val_loss = 0.0428\n",
      "Run 1, Epoch 160, val_loss = 0.0458\n",
      "Run 1, Epoch 161, val_loss = 0.0449\n",
      "Run 1, Epoch 162, val_loss = 0.0463\n",
      "Run 1, Epoch 163, val_loss = 0.0472\n",
      "Run 1, Epoch 164, val_loss = 0.0432\n",
      "Run 1, Epoch 165, val_loss = 0.0479\n",
      "Run 1, Epoch 166, val_loss = 0.0443\n",
      "Run 1, Epoch 167, val_loss = 0.0510\n",
      "Run 1, Epoch 168, val_loss = 0.0462\n",
      "Run 1, Epoch 169, val_loss = 0.0459\n",
      "Run 1, Epoch 170, val_loss = 0.0508\n",
      "Run 1, Epoch 171, val_loss = 0.0584\n",
      "Run 1, Epoch 172, val_loss = 0.0437\n",
      "Run 1, Epoch 173, val_loss = 0.0461\n",
      "Run 1, Epoch 174, val_loss = 0.0475\n",
      "Run 1, Epoch 175, val_loss = 0.0442\n",
      "Run 1, Epoch 176, val_loss = 0.0421\n",
      "Run 1, Epoch 177, val_loss = 0.0683\n",
      "Run 1, Epoch 178, val_loss = 0.0463\n",
      "Run 1, Epoch 179, val_loss = 0.0882\n",
      "Run 1, Epoch 180, val_loss = 0.0447\n",
      "Run 1, Epoch 181, val_loss = 0.0440\n",
      "Run 1, Epoch 182, val_loss = 0.0777\n",
      "Run 1, Epoch 183, val_loss = 0.1101\n",
      "Run 1, Epoch 184, val_loss = 0.0462\n",
      "Run 1, Epoch 185, val_loss = 0.0430\n",
      "Run 1, Epoch 186, val_loss = 0.0800\n",
      "Run 1, Epoch 187, val_loss = 0.0433\n",
      "Run 1, Epoch 188, val_loss = 0.0463\n",
      "Run 1, Epoch 189, val_loss = 0.0516\n",
      "Run 1, Epoch 190, val_loss = 0.0435\n",
      "Run 1, Epoch 191, val_loss = 0.0620\n",
      "Run 1, Epoch 192, val_loss = 0.0451\n",
      "Run 1, Epoch 193, val_loss = 0.0464\n",
      "Run 1, Epoch 194, val_loss = 0.0594\n",
      "Run 1, Epoch 195, val_loss = 0.0432\n",
      "Run 1, Epoch 196, val_loss = 0.0436\n",
      "✅ Run 1: Acc = 99.64%, MDE = 0.0116\n",
      "Run 2/5 - Alpha = 0.8\n",
      "Run 2, Epoch 1, val_loss = 3.5877\n",
      "Run 2, Epoch 2, val_loss = 1.7333\n",
      "Run 2, Epoch 3, val_loss = 1.3407\n",
      "Run 2, Epoch 4, val_loss = 0.8078\n",
      "Run 2, Epoch 5, val_loss = 0.6220\n",
      "Run 2, Epoch 6, val_loss = 0.6528\n",
      "Run 2, Epoch 7, val_loss = 0.6625\n",
      "Run 2, Epoch 8, val_loss = 0.6441\n",
      "Run 2, Epoch 9, val_loss = 0.4267\n",
      "Run 2, Epoch 10, val_loss = 0.4104\n",
      "Run 2, Epoch 11, val_loss = 0.2821\n",
      "Run 2, Epoch 12, val_loss = 0.3054\n",
      "Run 2, Epoch 13, val_loss = 0.2492\n",
      "Run 2, Epoch 14, val_loss = 0.3360\n",
      "Run 2, Epoch 15, val_loss = 0.3686\n",
      "Run 2, Epoch 16, val_loss = 0.2534\n",
      "Run 2, Epoch 17, val_loss = 0.2154\n",
      "Run 2, Epoch 18, val_loss = 0.1981\n",
      "Run 2, Epoch 19, val_loss = 0.1942\n",
      "Run 2, Epoch 20, val_loss = 0.3194\n",
      "Run 2, Epoch 21, val_loss = 0.1837\n",
      "Run 2, Epoch 22, val_loss = 0.2463\n",
      "Run 2, Epoch 23, val_loss = 0.1690\n",
      "Run 2, Epoch 24, val_loss = 0.1560\n",
      "Run 2, Epoch 25, val_loss = 0.1553\n",
      "Run 2, Epoch 26, val_loss = 0.1708\n",
      "Run 2, Epoch 27, val_loss = 0.1692\n",
      "Run 2, Epoch 28, val_loss = 0.1625\n",
      "Run 2, Epoch 29, val_loss = 0.1639\n",
      "Run 2, Epoch 30, val_loss = 0.2196\n",
      "Run 2, Epoch 31, val_loss = 0.1454\n",
      "Run 2, Epoch 32, val_loss = 0.1330\n",
      "Run 2, Epoch 33, val_loss = 0.0946\n",
      "Run 2, Epoch 34, val_loss = 0.0908\n",
      "Run 2, Epoch 35, val_loss = 0.1470\n",
      "Run 2, Epoch 36, val_loss = 0.1426\n",
      "Run 2, Epoch 37, val_loss = 0.1132\n",
      "Run 2, Epoch 38, val_loss = 0.1109\n",
      "Run 2, Epoch 39, val_loss = 0.1019\n",
      "Run 2, Epoch 40, val_loss = 0.1319\n",
      "Run 2, Epoch 41, val_loss = 0.0995\n",
      "Run 2, Epoch 42, val_loss = 0.1091\n",
      "Run 2, Epoch 43, val_loss = 0.1012\n",
      "Run 2, Epoch 44, val_loss = 0.0971\n",
      "Run 2, Epoch 45, val_loss = 0.0849\n",
      "Run 2, Epoch 46, val_loss = 0.0877\n",
      "Run 2, Epoch 47, val_loss = 0.1195\n",
      "Run 2, Epoch 48, val_loss = 0.1063\n",
      "Run 2, Epoch 49, val_loss = 0.0677\n",
      "Run 2, Epoch 50, val_loss = 0.0760\n",
      "Run 2, Epoch 51, val_loss = 0.0999\n",
      "Run 2, Epoch 52, val_loss = 0.0999\n",
      "Run 2, Epoch 53, val_loss = 0.0632\n",
      "Run 2, Epoch 54, val_loss = 0.0975\n",
      "Run 2, Epoch 55, val_loss = 0.0986\n",
      "Run 2, Epoch 56, val_loss = 0.0985\n",
      "Run 2, Epoch 57, val_loss = 0.0881\n",
      "Run 2, Epoch 58, val_loss = 0.0875\n",
      "Run 2, Epoch 59, val_loss = 0.0970\n",
      "Run 2, Epoch 60, val_loss = 0.0690\n",
      "Run 2, Epoch 61, val_loss = 0.0829\n",
      "Run 2, Epoch 62, val_loss = 0.0746\n",
      "Run 2, Epoch 63, val_loss = 0.0855\n",
      "Run 2, Epoch 64, val_loss = 0.0748\n",
      "Run 2, Epoch 65, val_loss = 0.0805\n",
      "Run 2, Epoch 66, val_loss = 0.1071\n",
      "Run 2, Epoch 67, val_loss = 0.0780\n",
      "Run 2, Epoch 68, val_loss = 0.0766\n",
      "Run 2, Epoch 69, val_loss = 0.0788\n",
      "Run 2, Epoch 70, val_loss = 0.0663\n",
      "Run 2, Epoch 71, val_loss = 0.0645\n",
      "Run 2, Epoch 72, val_loss = 0.0591\n",
      "Run 2, Epoch 73, val_loss = 0.0509\n",
      "Run 2, Epoch 74, val_loss = 0.0563\n",
      "Run 2, Epoch 75, val_loss = 0.0496\n",
      "Run 2, Epoch 76, val_loss = 0.0559\n",
      "Run 2, Epoch 77, val_loss = 0.0596\n",
      "Run 2, Epoch 78, val_loss = 0.0589\n",
      "Run 2, Epoch 79, val_loss = 0.0528\n",
      "Run 2, Epoch 80, val_loss = 0.0470\n",
      "Run 2, Epoch 81, val_loss = 0.0439\n",
      "Run 2, Epoch 82, val_loss = 0.0572\n",
      "Run 2, Epoch 83, val_loss = 0.0519\n",
      "Run 2, Epoch 84, val_loss = 0.0715\n",
      "Run 2, Epoch 85, val_loss = 0.0482\n",
      "Run 2, Epoch 86, val_loss = 0.0509\n",
      "Run 2, Epoch 87, val_loss = 0.0436\n",
      "Run 2, Epoch 88, val_loss = 0.0412\n",
      "Run 2, Epoch 89, val_loss = 0.0894\n",
      "Run 2, Epoch 90, val_loss = 0.0540\n",
      "Run 2, Epoch 91, val_loss = 0.0448\n",
      "Run 2, Epoch 92, val_loss = 0.0407\n",
      "Run 2, Epoch 93, val_loss = 0.0684\n",
      "Run 2, Epoch 94, val_loss = 0.0537\n",
      "Run 2, Epoch 95, val_loss = 0.0579\n",
      "Run 2, Epoch 96, val_loss = 0.0440\n",
      "Run 2, Epoch 97, val_loss = 0.0515\n",
      "Run 2, Epoch 98, val_loss = 0.0492\n",
      "Run 2, Epoch 99, val_loss = 0.0556\n",
      "Run 2, Epoch 100, val_loss = 0.0482\n",
      "Run 2, Epoch 101, val_loss = 0.0518\n",
      "Run 2, Epoch 102, val_loss = 0.0470\n",
      "Run 2, Epoch 103, val_loss = 0.0555\n",
      "Run 2, Epoch 104, val_loss = 0.0477\n",
      "Run 2, Epoch 105, val_loss = 0.0510\n",
      "Run 2, Epoch 106, val_loss = 0.0567\n",
      "Run 2, Epoch 107, val_loss = 0.0532\n",
      "Run 2, Epoch 108, val_loss = 0.0590\n",
      "Run 2, Epoch 109, val_loss = 0.0454\n",
      "Run 2, Epoch 110, val_loss = 0.0403\n",
      "Run 2, Epoch 111, val_loss = 0.0436\n",
      "Run 2, Epoch 112, val_loss = 0.0398\n",
      "Run 2, Epoch 113, val_loss = 0.0561\n",
      "Run 2, Epoch 114, val_loss = 0.0487\n",
      "Run 2, Epoch 115, val_loss = 0.0389\n",
      "Run 2, Epoch 116, val_loss = 0.0506\n",
      "Run 2, Epoch 117, val_loss = 0.0666\n",
      "Run 2, Epoch 118, val_loss = 0.0408\n",
      "Run 2, Epoch 119, val_loss = 0.0487\n",
      "Run 2, Epoch 120, val_loss = 0.0445\n",
      "Run 2, Epoch 121, val_loss = 0.0392\n",
      "Run 2, Epoch 122, val_loss = 0.0391\n",
      "Run 2, Epoch 123, val_loss = 0.0569\n",
      "Run 2, Epoch 124, val_loss = 0.0379\n",
      "Run 2, Epoch 125, val_loss = 0.0369\n",
      "Run 2, Epoch 126, val_loss = 0.0483\n",
      "Run 2, Epoch 127, val_loss = 0.0356\n",
      "Run 2, Epoch 128, val_loss = 0.0508\n",
      "Run 2, Epoch 129, val_loss = 0.0626\n",
      "Run 2, Epoch 130, val_loss = 0.0329\n",
      "Run 2, Epoch 131, val_loss = 0.0345\n",
      "Run 2, Epoch 132, val_loss = 0.0370\n",
      "Run 2, Epoch 133, val_loss = 0.0488\n",
      "Run 2, Epoch 134, val_loss = 0.0357\n",
      "Run 2, Epoch 135, val_loss = 0.0433\n",
      "Run 2, Epoch 136, val_loss = 0.0393\n",
      "Run 2, Epoch 137, val_loss = 0.0371\n",
      "Run 2, Epoch 138, val_loss = 0.0322\n",
      "Run 2, Epoch 139, val_loss = 0.0421\n",
      "Run 2, Epoch 140, val_loss = 0.0382\n",
      "Run 2, Epoch 141, val_loss = 0.0393\n",
      "Run 2, Epoch 142, val_loss = 0.0385\n",
      "Run 2, Epoch 143, val_loss = 0.0455\n",
      "Run 2, Epoch 144, val_loss = 0.0374\n",
      "Run 2, Epoch 145, val_loss = 0.0411\n",
      "Run 2, Epoch 146, val_loss = 0.0384\n",
      "Run 2, Epoch 147, val_loss = 0.0379\n",
      "Run 2, Epoch 148, val_loss = 0.0384\n",
      "Run 2, Epoch 149, val_loss = 0.0402\n",
      "Run 2, Epoch 150, val_loss = 0.0373\n",
      "Run 2, Epoch 151, val_loss = 0.0367\n",
      "Run 2, Epoch 152, val_loss = 0.0381\n",
      "Run 2, Epoch 153, val_loss = 0.0414\n",
      "Run 2, Epoch 154, val_loss = 0.0387\n",
      "Run 2, Epoch 155, val_loss = 0.0596\n",
      "Run 2, Epoch 156, val_loss = 0.0383\n",
      "Run 2, Epoch 157, val_loss = 0.0399\n",
      "Run 2, Epoch 158, val_loss = 0.0386\n",
      "✅ Run 2: Acc = 99.69%, MDE = 0.0129\n",
      "Run 3/5 - Alpha = 0.8\n",
      "Run 3, Epoch 1, val_loss = 3.7288\n",
      "Run 3, Epoch 2, val_loss = 1.9562\n",
      "Run 3, Epoch 3, val_loss = 1.3592\n",
      "Run 3, Epoch 4, val_loss = 1.0650\n",
      "Run 3, Epoch 5, val_loss = 0.8949\n",
      "Run 3, Epoch 6, val_loss = 0.5597\n",
      "Run 3, Epoch 7, val_loss = 0.5746\n",
      "Run 3, Epoch 8, val_loss = 0.7404\n",
      "Run 3, Epoch 9, val_loss = 0.4503\n",
      "Run 3, Epoch 10, val_loss = 0.4182\n",
      "Run 3, Epoch 11, val_loss = 0.5591\n",
      "Run 3, Epoch 12, val_loss = 0.3946\n",
      "Run 3, Epoch 13, val_loss = 0.4571\n",
      "Run 3, Epoch 14, val_loss = 0.3293\n",
      "Run 3, Epoch 15, val_loss = 0.4208\n",
      "Run 3, Epoch 16, val_loss = 0.2689\n",
      "Run 3, Epoch 17, val_loss = 0.2535\n",
      "Run 3, Epoch 18, val_loss = 0.2330\n",
      "Run 3, Epoch 19, val_loss = 0.2011\n",
      "Run 3, Epoch 20, val_loss = 0.2136\n",
      "Run 3, Epoch 21, val_loss = 0.2190\n",
      "Run 3, Epoch 22, val_loss = 0.2480\n",
      "Run 3, Epoch 23, val_loss = 0.2156\n",
      "Run 3, Epoch 24, val_loss = 0.2788\n",
      "Run 3, Epoch 25, val_loss = 0.2362\n",
      "Run 3, Epoch 26, val_loss = 0.1581\n",
      "Run 3, Epoch 27, val_loss = 0.1809\n",
      "Run 3, Epoch 28, val_loss = 0.1667\n",
      "Run 3, Epoch 29, val_loss = 0.1496\n",
      "Run 3, Epoch 30, val_loss = 0.1343\n",
      "Run 3, Epoch 31, val_loss = 0.1888\n",
      "Run 3, Epoch 32, val_loss = 0.1357\n",
      "Run 3, Epoch 33, val_loss = 0.1494\n",
      "Run 3, Epoch 34, val_loss = 0.2417\n",
      "Run 3, Epoch 35, val_loss = 0.1080\n",
      "Run 3, Epoch 36, val_loss = 0.1032\n",
      "Run 3, Epoch 37, val_loss = 0.1190\n",
      "Run 3, Epoch 38, val_loss = 0.1270\n",
      "Run 3, Epoch 39, val_loss = 0.1301\n",
      "Run 3, Epoch 40, val_loss = 0.0977\n",
      "Run 3, Epoch 41, val_loss = 0.1126\n",
      "Run 3, Epoch 42, val_loss = 0.1121\n",
      "Run 3, Epoch 43, val_loss = 0.1550\n",
      "Run 3, Epoch 44, val_loss = 0.1095\n",
      "Run 3, Epoch 45, val_loss = 0.0973\n",
      "Run 3, Epoch 46, val_loss = 0.0962\n",
      "Run 3, Epoch 47, val_loss = 0.1018\n",
      "Run 3, Epoch 48, val_loss = 0.1183\n",
      "Run 3, Epoch 49, val_loss = 0.1140\n",
      "Run 3, Epoch 50, val_loss = 0.0900\n",
      "Run 3, Epoch 51, val_loss = 0.1157\n",
      "Run 3, Epoch 52, val_loss = 0.1312\n",
      "Run 3, Epoch 53, val_loss = 0.1058\n",
      "Run 3, Epoch 54, val_loss = 0.1420\n",
      "Run 3, Epoch 55, val_loss = 0.1183\n",
      "Run 3, Epoch 56, val_loss = 0.1107\n",
      "Run 3, Epoch 57, val_loss = 0.0980\n",
      "Run 3, Epoch 58, val_loss = 0.1212\n",
      "Run 3, Epoch 59, val_loss = 0.0844\n",
      "Run 3, Epoch 60, val_loss = 0.0911\n",
      "Run 3, Epoch 61, val_loss = 0.0938\n",
      "Run 3, Epoch 62, val_loss = 0.0882\n",
      "Run 3, Epoch 63, val_loss = 0.0933\n",
      "Run 3, Epoch 64, val_loss = 0.0834\n",
      "Run 3, Epoch 65, val_loss = 0.0845\n",
      "Run 3, Epoch 66, val_loss = 0.0800\n",
      "Run 3, Epoch 67, val_loss = 0.0739\n",
      "Run 3, Epoch 68, val_loss = 0.0843\n",
      "Run 3, Epoch 69, val_loss = 0.0880\n",
      "Run 3, Epoch 70, val_loss = 0.0989\n",
      "Run 3, Epoch 71, val_loss = 0.0746\n",
      "Run 3, Epoch 72, val_loss = 0.1132\n",
      "Run 3, Epoch 73, val_loss = 0.1003\n",
      "Run 3, Epoch 74, val_loss = 0.0822\n",
      "Run 3, Epoch 75, val_loss = 0.0923\n",
      "Run 3, Epoch 76, val_loss = 0.0855\n",
      "Run 3, Epoch 77, val_loss = 0.0853\n",
      "Run 3, Epoch 78, val_loss = 0.0829\n",
      "Run 3, Epoch 79, val_loss = 0.0819\n",
      "Run 3, Epoch 80, val_loss = 0.1293\n",
      "Run 3, Epoch 81, val_loss = 0.0866\n",
      "Run 3, Epoch 82, val_loss = 0.0784\n",
      "Run 3, Epoch 83, val_loss = 0.1016\n",
      "Run 3, Epoch 84, val_loss = 0.0763\n",
      "Run 3, Epoch 85, val_loss = 0.0751\n",
      "Run 3, Epoch 86, val_loss = 0.0743\n",
      "Run 3, Epoch 87, val_loss = 0.0705\n",
      "Run 3, Epoch 88, val_loss = 0.0742\n",
      "Run 3, Epoch 89, val_loss = 0.0646\n",
      "Run 3, Epoch 90, val_loss = 0.0679\n",
      "Run 3, Epoch 91, val_loss = 0.0666\n",
      "Run 3, Epoch 92, val_loss = 0.1361\n",
      "Run 3, Epoch 93, val_loss = 0.0566\n",
      "Run 3, Epoch 94, val_loss = 0.0730\n",
      "Run 3, Epoch 95, val_loss = 0.0602\n",
      "Run 3, Epoch 96, val_loss = 0.0555\n",
      "Run 3, Epoch 97, val_loss = 0.0931\n",
      "Run 3, Epoch 98, val_loss = 0.0752\n",
      "Run 3, Epoch 99, val_loss = 0.0662\n",
      "Run 3, Epoch 100, val_loss = 0.0682\n",
      "Run 3, Epoch 101, val_loss = 0.0680\n",
      "Run 3, Epoch 102, val_loss = 0.0662\n",
      "Run 3, Epoch 103, val_loss = 0.0733\n",
      "Run 3, Epoch 104, val_loss = 0.0740\n",
      "Run 3, Epoch 105, val_loss = 0.0883\n",
      "Run 3, Epoch 106, val_loss = 0.0859\n",
      "Run 3, Epoch 107, val_loss = 0.0658\n",
      "Run 3, Epoch 108, val_loss = 0.0727\n",
      "Run 3, Epoch 109, val_loss = 0.0703\n",
      "Run 3, Epoch 110, val_loss = 0.0691\n",
      "Run 3, Epoch 111, val_loss = 0.0801\n",
      "Run 3, Epoch 112, val_loss = 0.0643\n",
      "Run 3, Epoch 113, val_loss = 0.0694\n",
      "Run 3, Epoch 114, val_loss = 0.0861\n",
      "Run 3, Epoch 115, val_loss = 0.0688\n",
      "Run 3, Epoch 116, val_loss = 0.0708\n",
      "✅ Run 3: Acc = 99.64%, MDE = 0.0117\n",
      "Run 4/5 - Alpha = 0.8\n",
      "Run 4, Epoch 1, val_loss = 3.9528\n",
      "Run 4, Epoch 2, val_loss = 2.0132\n",
      "Run 4, Epoch 3, val_loss = 1.4529\n",
      "Run 4, Epoch 4, val_loss = 0.9767\n",
      "Run 4, Epoch 5, val_loss = 0.9464\n",
      "Run 4, Epoch 6, val_loss = 0.9107\n",
      "Run 4, Epoch 7, val_loss = 0.6130\n",
      "Run 4, Epoch 8, val_loss = 0.6398\n",
      "Run 4, Epoch 9, val_loss = 0.4070\n",
      "Run 4, Epoch 10, val_loss = 0.3829\n",
      "Run 4, Epoch 11, val_loss = 0.3879\n",
      "Run 4, Epoch 12, val_loss = 0.3833\n",
      "Run 4, Epoch 13, val_loss = 0.4554\n",
      "Run 4, Epoch 14, val_loss = 0.2871\n",
      "Run 4, Epoch 15, val_loss = 0.3125\n",
      "Run 4, Epoch 16, val_loss = 0.2272\n",
      "Run 4, Epoch 17, val_loss = 0.2446\n",
      "Run 4, Epoch 18, val_loss = 0.3326\n",
      "Run 4, Epoch 19, val_loss = 0.2214\n",
      "Run 4, Epoch 20, val_loss = 0.2347\n",
      "Run 4, Epoch 21, val_loss = 0.1878\n",
      "Run 4, Epoch 22, val_loss = 0.1922\n",
      "Run 4, Epoch 23, val_loss = 0.2033\n",
      "Run 4, Epoch 24, val_loss = 0.1765\n",
      "Run 4, Epoch 25, val_loss = 0.1781\n",
      "Run 4, Epoch 26, val_loss = 0.1594\n",
      "Run 4, Epoch 27, val_loss = 0.1616\n",
      "Run 4, Epoch 28, val_loss = 0.1707\n",
      "Run 4, Epoch 29, val_loss = 0.1343\n",
      "Run 4, Epoch 30, val_loss = 0.1612\n",
      "Run 4, Epoch 31, val_loss = 0.1539\n",
      "Run 4, Epoch 32, val_loss = 0.1272\n",
      "Run 4, Epoch 33, val_loss = 0.1115\n",
      "Run 4, Epoch 34, val_loss = 0.1398\n",
      "Run 4, Epoch 35, val_loss = 0.1507\n",
      "Run 4, Epoch 36, val_loss = 0.1260\n",
      "Run 4, Epoch 37, val_loss = 0.1275\n",
      "Run 4, Epoch 38, val_loss = 0.1257\n",
      "Run 4, Epoch 39, val_loss = 0.1151\n",
      "Run 4, Epoch 40, val_loss = 0.1516\n",
      "Run 4, Epoch 41, val_loss = 0.1047\n",
      "Run 4, Epoch 42, val_loss = 0.1252\n",
      "Run 4, Epoch 43, val_loss = 0.1098\n",
      "Run 4, Epoch 44, val_loss = 0.0986\n",
      "Run 4, Epoch 45, val_loss = 0.0990\n",
      "Run 4, Epoch 46, val_loss = 0.1047\n",
      "Run 4, Epoch 47, val_loss = 0.1343\n",
      "Run 4, Epoch 48, val_loss = 0.0971\n",
      "Run 4, Epoch 49, val_loss = 0.0809\n",
      "Run 4, Epoch 50, val_loss = 0.0706\n",
      "Run 4, Epoch 51, val_loss = 0.0873\n",
      "Run 4, Epoch 52, val_loss = 0.1143\n",
      "Run 4, Epoch 53, val_loss = 0.1127\n",
      "Run 4, Epoch 54, val_loss = 0.1468\n",
      "Run 4, Epoch 55, val_loss = 0.0710\n",
      "Run 4, Epoch 56, val_loss = 0.0857\n",
      "Run 4, Epoch 57, val_loss = 0.0796\n",
      "Run 4, Epoch 58, val_loss = 0.0754\n",
      "Run 4, Epoch 59, val_loss = 0.0920\n",
      "Run 4, Epoch 60, val_loss = 0.0861\n",
      "Run 4, Epoch 61, val_loss = 0.0908\n",
      "Run 4, Epoch 62, val_loss = 0.1163\n",
      "Run 4, Epoch 63, val_loss = 0.0774\n",
      "Run 4, Epoch 64, val_loss = 0.1097\n",
      "Run 4, Epoch 65, val_loss = 0.0845\n",
      "Run 4, Epoch 66, val_loss = 0.0830\n",
      "Run 4, Epoch 67, val_loss = 0.0490\n",
      "Run 4, Epoch 68, val_loss = 0.0514\n",
      "Run 4, Epoch 69, val_loss = 0.0605\n",
      "Run 4, Epoch 70, val_loss = 0.0598\n",
      "Run 4, Epoch 71, val_loss = 0.0580\n",
      "Run 4, Epoch 72, val_loss = 0.0581\n",
      "Run 4, Epoch 73, val_loss = 0.0557\n",
      "Run 4, Epoch 74, val_loss = 0.0628\n",
      "Run 4, Epoch 75, val_loss = 0.0554\n",
      "Run 4, Epoch 76, val_loss = 0.0771\n",
      "Run 4, Epoch 77, val_loss = 0.0707\n",
      "Run 4, Epoch 78, val_loss = 0.0527\n",
      "Run 4, Epoch 79, val_loss = 0.0830\n",
      "Run 4, Epoch 80, val_loss = 0.0503\n",
      "Run 4, Epoch 81, val_loss = 0.0504\n",
      "Run 4, Epoch 82, val_loss = 0.0587\n",
      "Run 4, Epoch 83, val_loss = 0.0664\n",
      "Run 4, Epoch 84, val_loss = 0.0527\n",
      "Run 4, Epoch 85, val_loss = 0.0695\n",
      "Run 4, Epoch 86, val_loss = 0.0603\n",
      "Run 4, Epoch 87, val_loss = 0.0501\n",
      "✅ Run 4: Acc = 99.59%, MDE = 0.0147\n",
      "Run 5/5 - Alpha = 0.8\n",
      "Run 5, Epoch 1, val_loss = 2.8948\n",
      "Run 5, Epoch 2, val_loss = 1.4994\n",
      "Run 5, Epoch 3, val_loss = 1.0681\n",
      "Run 5, Epoch 4, val_loss = 0.9536\n",
      "Run 5, Epoch 5, val_loss = 0.7721\n",
      "Run 5, Epoch 6, val_loss = 0.5716\n",
      "Run 5, Epoch 7, val_loss = 0.6703\n",
      "Run 5, Epoch 8, val_loss = 0.4192\n",
      "Run 5, Epoch 9, val_loss = 0.4056\n",
      "Run 5, Epoch 10, val_loss = 0.5264\n",
      "Run 5, Epoch 11, val_loss = 0.3293\n",
      "Run 5, Epoch 12, val_loss = 0.3720\n",
      "Run 5, Epoch 13, val_loss = 0.3372\n",
      "Run 5, Epoch 14, val_loss = 0.3134\n",
      "Run 5, Epoch 15, val_loss = 0.2845\n",
      "Run 5, Epoch 16, val_loss = 0.2591\n",
      "Run 5, Epoch 17, val_loss = 0.2234\n",
      "Run 5, Epoch 18, val_loss = 0.1679\n",
      "Run 5, Epoch 19, val_loss = 0.1890\n",
      "Run 5, Epoch 20, val_loss = 0.2169\n",
      "Run 5, Epoch 21, val_loss = 0.1799\n",
      "Run 5, Epoch 22, val_loss = 0.1309\n",
      "Run 5, Epoch 23, val_loss = 0.2243\n",
      "Run 5, Epoch 24, val_loss = 0.1521\n",
      "Run 5, Epoch 25, val_loss = 0.1579\n",
      "Run 5, Epoch 26, val_loss = 0.1467\n",
      "Run 5, Epoch 27, val_loss = 0.1136\n",
      "Run 5, Epoch 28, val_loss = 0.1371\n",
      "Run 5, Epoch 29, val_loss = 0.1293\n",
      "Run 5, Epoch 30, val_loss = 0.1666\n",
      "Run 5, Epoch 31, val_loss = 0.1216\n",
      "Run 5, Epoch 32, val_loss = 0.0914\n",
      "Run 5, Epoch 33, val_loss = 0.1377\n",
      "Run 5, Epoch 34, val_loss = 0.1423\n",
      "Run 5, Epoch 35, val_loss = 0.1043\n",
      "Run 5, Epoch 36, val_loss = 0.0914\n",
      "Run 5, Epoch 37, val_loss = 0.0947\n",
      "Run 5, Epoch 38, val_loss = 0.1026\n",
      "Run 5, Epoch 39, val_loss = 0.0994\n",
      "Run 5, Epoch 40, val_loss = 0.0937\n",
      "Run 5, Epoch 41, val_loss = 0.0773\n",
      "Run 5, Epoch 42, val_loss = 0.0769\n",
      "Run 5, Epoch 43, val_loss = 0.0912\n",
      "Run 5, Epoch 44, val_loss = 0.0975\n",
      "Run 5, Epoch 45, val_loss = 0.0744\n",
      "Run 5, Epoch 46, val_loss = 0.0884\n",
      "Run 5, Epoch 47, val_loss = 0.0819\n",
      "Run 5, Epoch 48, val_loss = 0.0784\n",
      "Run 5, Epoch 49, val_loss = 0.0958\n",
      "Run 5, Epoch 50, val_loss = 0.0836\n",
      "Run 5, Epoch 51, val_loss = 0.0822\n",
      "Run 5, Epoch 52, val_loss = 0.1050\n",
      "Run 5, Epoch 53, val_loss = 0.0886\n",
      "Run 5, Epoch 54, val_loss = 0.0913\n",
      "Run 5, Epoch 55, val_loss = 0.0982\n",
      "Run 5, Epoch 56, val_loss = 0.0825\n",
      "Run 5, Epoch 57, val_loss = 0.1112\n",
      "Run 5, Epoch 58, val_loss = 0.0808\n",
      "Run 5, Epoch 59, val_loss = 0.0906\n",
      "Run 5, Epoch 60, val_loss = 0.0823\n",
      "Run 5, Epoch 61, val_loss = 0.0871\n",
      "Run 5, Epoch 62, val_loss = 0.0663\n",
      "Run 5, Epoch 63, val_loss = 0.0661\n",
      "Run 5, Epoch 64, val_loss = 0.0696\n",
      "Run 5, Epoch 65, val_loss = 0.0801\n",
      "Run 5, Epoch 66, val_loss = 0.0716\n",
      "Run 5, Epoch 67, val_loss = 0.0632\n",
      "Run 5, Epoch 68, val_loss = 0.0756\n",
      "Run 5, Epoch 69, val_loss = 0.0896\n",
      "Run 5, Epoch 70, val_loss = 0.0678\n",
      "Run 5, Epoch 71, val_loss = 0.0743\n",
      "Run 5, Epoch 72, val_loss = 0.0682\n",
      "Run 5, Epoch 73, val_loss = 0.0641\n",
      "Run 5, Epoch 74, val_loss = 0.0581\n",
      "Run 5, Epoch 75, val_loss = 0.0723\n",
      "Run 5, Epoch 76, val_loss = 0.0543\n",
      "Run 5, Epoch 77, val_loss = 0.0628\n",
      "Run 5, Epoch 78, val_loss = 0.0550\n",
      "Run 5, Epoch 79, val_loss = 0.0846\n",
      "Run 5, Epoch 80, val_loss = 0.0593\n",
      "Run 5, Epoch 81, val_loss = 0.0502\n",
      "Run 5, Epoch 82, val_loss = 0.0578\n",
      "Run 5, Epoch 83, val_loss = 0.0585\n",
      "Run 5, Epoch 84, val_loss = 0.0612\n",
      "Run 5, Epoch 85, val_loss = 0.0507\n",
      "Run 5, Epoch 86, val_loss = 0.0577\n",
      "Run 5, Epoch 87, val_loss = 0.0620\n",
      "Run 5, Epoch 88, val_loss = 0.0633\n",
      "Run 5, Epoch 89, val_loss = 0.0715\n",
      "Run 5, Epoch 90, val_loss = 0.0622\n",
      "Run 5, Epoch 91, val_loss = 0.0606\n",
      "Run 5, Epoch 92, val_loss = 0.0621\n",
      "Run 5, Epoch 93, val_loss = 0.0523\n",
      "Run 5, Epoch 94, val_loss = 0.0550\n",
      "Run 5, Epoch 95, val_loss = 0.0699\n",
      "Run 5, Epoch 96, val_loss = 0.0670\n",
      "Run 5, Epoch 97, val_loss = 0.0479\n",
      "Run 5, Epoch 98, val_loss = 0.0571\n",
      "Run 5, Epoch 99, val_loss = 0.0456\n",
      "Run 5, Epoch 100, val_loss = 0.0544\n",
      "Run 5, Epoch 101, val_loss = 0.0572\n",
      "Run 5, Epoch 102, val_loss = 0.0470\n",
      "Run 5, Epoch 103, val_loss = 0.0393\n",
      "Run 5, Epoch 104, val_loss = 0.0551\n",
      "Run 5, Epoch 105, val_loss = 0.0577\n",
      "Run 5, Epoch 106, val_loss = 0.0538\n",
      "Run 5, Epoch 107, val_loss = 0.0603\n",
      "Run 5, Epoch 108, val_loss = 0.0598\n",
      "Run 5, Epoch 109, val_loss = 0.0613\n",
      "Run 5, Epoch 110, val_loss = 0.0585\n",
      "Run 5, Epoch 111, val_loss = 0.0809\n",
      "Run 5, Epoch 112, val_loss = 0.0663\n",
      "Run 5, Epoch 113, val_loss = 0.0918\n",
      "Run 5, Epoch 114, val_loss = 0.0511\n",
      "Run 5, Epoch 115, val_loss = 0.0640\n",
      "Run 5, Epoch 116, val_loss = 0.0631\n",
      "Run 5, Epoch 117, val_loss = 0.0585\n",
      "Run 5, Epoch 118, val_loss = 0.0671\n",
      "Run 5, Epoch 119, val_loss = 0.0441\n",
      "Run 5, Epoch 120, val_loss = 0.0691\n",
      "Run 5, Epoch 121, val_loss = 0.0643\n",
      "Run 5, Epoch 122, val_loss = 0.0535\n",
      "Run 5, Epoch 123, val_loss = 0.0510\n",
      "✅ Run 5: Acc = 99.64%, MDE = 0.0117\n",
      "📁 Results & all errors saved to repeat_copy/08\n",
      "\n",
      "[Alpha = 0.9] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.9\n",
      "Run 1, Epoch 1, val_loss = 3.7843\n",
      "Run 1, Epoch 2, val_loss = 2.3513\n",
      "Run 1, Epoch 3, val_loss = 1.5679\n",
      "Run 1, Epoch 4, val_loss = 1.1638\n",
      "Run 1, Epoch 5, val_loss = 0.9489\n",
      "Run 1, Epoch 6, val_loss = 0.8604\n",
      "Run 1, Epoch 7, val_loss = 0.7645\n",
      "Run 1, Epoch 8, val_loss = 0.5377\n",
      "Run 1, Epoch 9, val_loss = 0.5486\n",
      "Run 1, Epoch 10, val_loss = 0.5517\n",
      "Run 1, Epoch 11, val_loss = 0.4827\n",
      "Run 1, Epoch 12, val_loss = 0.3809\n",
      "Run 1, Epoch 13, val_loss = 0.4040\n",
      "Run 1, Epoch 14, val_loss = 0.3487\n",
      "Run 1, Epoch 15, val_loss = 0.3706\n",
      "Run 1, Epoch 16, val_loss = 0.3360\n",
      "Run 1, Epoch 17, val_loss = 0.3728\n",
      "Run 1, Epoch 18, val_loss = 0.2872\n",
      "Run 1, Epoch 19, val_loss = 0.3441\n",
      "Run 1, Epoch 20, val_loss = 0.2649\n",
      "Run 1, Epoch 21, val_loss = 0.2092\n",
      "Run 1, Epoch 22, val_loss = 0.2367\n",
      "Run 1, Epoch 23, val_loss = 0.3145\n",
      "Run 1, Epoch 24, val_loss = 0.1560\n",
      "Run 1, Epoch 25, val_loss = 0.1434\n",
      "Run 1, Epoch 26, val_loss = 0.2059\n",
      "Run 1, Epoch 27, val_loss = 0.1767\n",
      "Run 1, Epoch 28, val_loss = 0.1922\n",
      "Run 1, Epoch 29, val_loss = 0.1792\n",
      "Run 1, Epoch 30, val_loss = 0.1530\n",
      "Run 1, Epoch 31, val_loss = 0.1771\n",
      "Run 1, Epoch 32, val_loss = 0.1626\n",
      "Run 1, Epoch 33, val_loss = 0.1281\n",
      "Run 1, Epoch 34, val_loss = 0.1158\n",
      "Run 1, Epoch 35, val_loss = 0.1158\n",
      "Run 1, Epoch 36, val_loss = 0.1188\n",
      "Run 1, Epoch 37, val_loss = 0.1594\n",
      "Run 1, Epoch 38, val_loss = 0.1569\n",
      "Run 1, Epoch 39, val_loss = 0.1354\n",
      "Run 1, Epoch 40, val_loss = 0.0935\n",
      "Run 1, Epoch 41, val_loss = 0.1512\n",
      "Run 1, Epoch 42, val_loss = 0.0999\n",
      "Run 1, Epoch 43, val_loss = 0.1283\n",
      "Run 1, Epoch 44, val_loss = 0.1133\n",
      "Run 1, Epoch 45, val_loss = 0.0988\n",
      "Run 1, Epoch 46, val_loss = 0.0989\n",
      "Run 1, Epoch 47, val_loss = 0.1024\n",
      "Run 1, Epoch 48, val_loss = 0.1344\n",
      "Run 1, Epoch 49, val_loss = 0.0942\n",
      "Run 1, Epoch 50, val_loss = 0.0940\n",
      "Run 1, Epoch 51, val_loss = 0.1014\n",
      "Run 1, Epoch 52, val_loss = 0.0847\n",
      "Run 1, Epoch 53, val_loss = 0.0941\n",
      "Run 1, Epoch 54, val_loss = 0.1212\n",
      "Run 1, Epoch 55, val_loss = 0.1829\n",
      "Run 1, Epoch 56, val_loss = 0.1263\n",
      "Run 1, Epoch 57, val_loss = 0.0895\n",
      "Run 1, Epoch 58, val_loss = 0.1035\n",
      "Run 1, Epoch 59, val_loss = 0.0889\n",
      "Run 1, Epoch 60, val_loss = 0.0689\n",
      "Run 1, Epoch 61, val_loss = 0.0832\n",
      "Run 1, Epoch 62, val_loss = 0.0802\n",
      "Run 1, Epoch 63, val_loss = 0.0920\n",
      "Run 1, Epoch 64, val_loss = 0.1509\n",
      "Run 1, Epoch 65, val_loss = 0.0848\n",
      "Run 1, Epoch 66, val_loss = 0.0802\n",
      "Run 1, Epoch 67, val_loss = 0.0913\n",
      "Run 1, Epoch 68, val_loss = 0.1307\n",
      "Run 1, Epoch 69, val_loss = 0.0962\n",
      "Run 1, Epoch 70, val_loss = 0.0643\n",
      "Run 1, Epoch 71, val_loss = 0.1082\n",
      "Run 1, Epoch 72, val_loss = 0.0954\n",
      "Run 1, Epoch 73, val_loss = 0.0632\n",
      "Run 1, Epoch 74, val_loss = 0.0802\n",
      "Run 1, Epoch 75, val_loss = 0.0740\n",
      "Run 1, Epoch 76, val_loss = 0.0725\n",
      "Run 1, Epoch 77, val_loss = 0.0825\n",
      "Run 1, Epoch 78, val_loss = 0.0620\n",
      "Run 1, Epoch 79, val_loss = 0.0836\n",
      "Run 1, Epoch 80, val_loss = 0.0783\n",
      "Run 1, Epoch 81, val_loss = 0.1026\n",
      "Run 1, Epoch 82, val_loss = 0.0974\n",
      "Run 1, Epoch 83, val_loss = 0.0696\n",
      "Run 1, Epoch 84, val_loss = 0.1120\n",
      "Run 1, Epoch 85, val_loss = 0.0705\n",
      "Run 1, Epoch 86, val_loss = 0.0784\n",
      "Run 1, Epoch 87, val_loss = 0.0749\n",
      "Run 1, Epoch 88, val_loss = 0.0781\n",
      "Run 1, Epoch 89, val_loss = 0.0717\n",
      "Run 1, Epoch 90, val_loss = 0.0742\n",
      "Run 1, Epoch 91, val_loss = 0.0703\n",
      "Run 1, Epoch 92, val_loss = 0.0675\n",
      "Run 1, Epoch 93, val_loss = 0.0671\n",
      "Run 1, Epoch 94, val_loss = 0.1046\n",
      "Run 1, Epoch 95, val_loss = 0.0814\n",
      "Run 1, Epoch 96, val_loss = 0.0676\n",
      "Run 1, Epoch 97, val_loss = 0.0607\n",
      "Run 1, Epoch 98, val_loss = 0.0562\n",
      "Run 1, Epoch 99, val_loss = 0.0742\n",
      "Run 1, Epoch 100, val_loss = 0.0637\n",
      "Run 1, Epoch 101, val_loss = 0.0682\n",
      "Run 1, Epoch 102, val_loss = 0.0504\n",
      "Run 1, Epoch 103, val_loss = 0.0593\n",
      "Run 1, Epoch 104, val_loss = 0.0547\n",
      "Run 1, Epoch 105, val_loss = 0.0457\n",
      "Run 1, Epoch 106, val_loss = 0.0579\n",
      "Run 1, Epoch 107, val_loss = 0.0613\n",
      "Run 1, Epoch 108, val_loss = 0.0565\n",
      "Run 1, Epoch 109, val_loss = 0.0515\n",
      "Run 1, Epoch 110, val_loss = 0.0611\n",
      "Run 1, Epoch 111, val_loss = 0.0502\n",
      "Run 1, Epoch 112, val_loss = 0.0618\n",
      "Run 1, Epoch 113, val_loss = 0.0675\n",
      "Run 1, Epoch 114, val_loss = 0.0739\n",
      "Run 1, Epoch 115, val_loss = 0.0811\n",
      "Run 1, Epoch 116, val_loss = 0.0857\n",
      "Run 1, Epoch 117, val_loss = 0.0732\n",
      "Run 1, Epoch 118, val_loss = 0.0819\n",
      "Run 1, Epoch 119, val_loss = 0.0627\n",
      "Run 1, Epoch 120, val_loss = 0.0494\n",
      "Run 1, Epoch 121, val_loss = 0.1259\n",
      "Run 1, Epoch 122, val_loss = 0.0635\n",
      "Run 1, Epoch 123, val_loss = 0.0514\n",
      "Run 1, Epoch 124, val_loss = 0.0521\n",
      "Run 1, Epoch 125, val_loss = 0.0526\n",
      "✅ Run 1: Acc = 99.69%, MDE = 0.0095\n",
      "Run 2/5 - Alpha = 0.9\n",
      "Run 2, Epoch 1, val_loss = 3.6604\n",
      "Run 2, Epoch 2, val_loss = 2.1922\n",
      "Run 2, Epoch 3, val_loss = 1.3763\n",
      "Run 2, Epoch 4, val_loss = 1.0600\n",
      "Run 2, Epoch 5, val_loss = 0.9450\n",
      "Run 2, Epoch 6, val_loss = 0.8628\n",
      "Run 2, Epoch 7, val_loss = 0.5384\n",
      "Run 2, Epoch 8, val_loss = 0.5088\n",
      "Run 2, Epoch 9, val_loss = 0.4681\n",
      "Run 2, Epoch 10, val_loss = 0.5881\n",
      "Run 2, Epoch 11, val_loss = 0.6728\n",
      "Run 2, Epoch 12, val_loss = 0.3526\n",
      "Run 2, Epoch 13, val_loss = 0.5748\n",
      "Run 2, Epoch 14, val_loss = 0.4381\n",
      "Run 2, Epoch 15, val_loss = 0.3159\n",
      "Run 2, Epoch 16, val_loss = 0.3192\n",
      "Run 2, Epoch 17, val_loss = 0.3098\n",
      "Run 2, Epoch 18, val_loss = 0.3654\n",
      "Run 2, Epoch 19, val_loss = 0.2572\n",
      "Run 2, Epoch 20, val_loss = 0.2966\n",
      "Run 2, Epoch 21, val_loss = 0.2119\n",
      "Run 2, Epoch 22, val_loss = 0.2354\n",
      "Run 2, Epoch 23, val_loss = 0.1999\n",
      "Run 2, Epoch 24, val_loss = 0.1889\n",
      "Run 2, Epoch 25, val_loss = 0.2475\n",
      "Run 2, Epoch 26, val_loss = 0.1943\n",
      "Run 2, Epoch 27, val_loss = 0.1479\n",
      "Run 2, Epoch 28, val_loss = 0.1626\n",
      "Run 2, Epoch 29, val_loss = 0.2224\n",
      "Run 2, Epoch 30, val_loss = 0.2155\n",
      "Run 2, Epoch 31, val_loss = 0.1589\n",
      "Run 2, Epoch 32, val_loss = 0.1596\n",
      "Run 2, Epoch 33, val_loss = 0.1286\n",
      "Run 2, Epoch 34, val_loss = 0.1647\n",
      "Run 2, Epoch 35, val_loss = 0.1484\n",
      "Run 2, Epoch 36, val_loss = 0.1146\n",
      "Run 2, Epoch 37, val_loss = 0.1321\n",
      "Run 2, Epoch 38, val_loss = 0.1865\n",
      "Run 2, Epoch 39, val_loss = 0.1993\n",
      "Run 2, Epoch 40, val_loss = 0.1321\n",
      "Run 2, Epoch 41, val_loss = 0.1457\n",
      "Run 2, Epoch 42, val_loss = 0.1305\n",
      "Run 2, Epoch 43, val_loss = 0.1131\n",
      "Run 2, Epoch 44, val_loss = 0.1454\n",
      "Run 2, Epoch 45, val_loss = 0.1025\n",
      "Run 2, Epoch 46, val_loss = 0.1089\n",
      "Run 2, Epoch 47, val_loss = 0.1604\n",
      "Run 2, Epoch 48, val_loss = 0.1140\n",
      "Run 2, Epoch 49, val_loss = 0.1197\n",
      "Run 2, Epoch 50, val_loss = 0.1197\n",
      "Run 2, Epoch 51, val_loss = 0.1294\n",
      "Run 2, Epoch 52, val_loss = 0.1243\n",
      "Run 2, Epoch 53, val_loss = 0.1228\n",
      "Run 2, Epoch 54, val_loss = 0.1245\n",
      "Run 2, Epoch 55, val_loss = 0.1184\n",
      "Run 2, Epoch 56, val_loss = 0.0890\n",
      "Run 2, Epoch 57, val_loss = 0.0895\n",
      "Run 2, Epoch 58, val_loss = 0.0949\n",
      "Run 2, Epoch 59, val_loss = 0.0951\n",
      "Run 2, Epoch 60, val_loss = 0.1020\n",
      "Run 2, Epoch 61, val_loss = 0.0801\n",
      "Run 2, Epoch 62, val_loss = 0.0914\n",
      "Run 2, Epoch 63, val_loss = 0.1020\n",
      "Run 2, Epoch 64, val_loss = 0.1868\n",
      "Run 2, Epoch 65, val_loss = 0.0933\n",
      "Run 2, Epoch 66, val_loss = 0.0986\n",
      "Run 2, Epoch 67, val_loss = 0.1198\n",
      "Run 2, Epoch 68, val_loss = 0.0923\n",
      "Run 2, Epoch 69, val_loss = 0.0683\n",
      "Run 2, Epoch 70, val_loss = 0.0995\n",
      "Run 2, Epoch 71, val_loss = 0.0865\n",
      "Run 2, Epoch 72, val_loss = 0.0688\n",
      "Run 2, Epoch 73, val_loss = 0.0954\n",
      "Run 2, Epoch 74, val_loss = 0.0739\n",
      "Run 2, Epoch 75, val_loss = 0.0853\n",
      "Run 2, Epoch 76, val_loss = 0.0839\n",
      "Run 2, Epoch 77, val_loss = 0.0927\n",
      "Run 2, Epoch 78, val_loss = 0.0691\n",
      "Run 2, Epoch 79, val_loss = 0.0793\n",
      "Run 2, Epoch 80, val_loss = 0.0740\n",
      "Run 2, Epoch 81, val_loss = 0.0762\n",
      "Run 2, Epoch 82, val_loss = 0.0589\n",
      "Run 2, Epoch 83, val_loss = 0.0921\n",
      "Run 2, Epoch 84, val_loss = 0.1302\n",
      "Run 2, Epoch 85, val_loss = 0.0792\n",
      "Run 2, Epoch 86, val_loss = 0.0743\n",
      "Run 2, Epoch 87, val_loss = 0.0802\n",
      "Run 2, Epoch 88, val_loss = 0.0550\n",
      "Run 2, Epoch 89, val_loss = 0.0752\n",
      "Run 2, Epoch 90, val_loss = 0.0720\n",
      "Run 2, Epoch 91, val_loss = 0.0757\n",
      "Run 2, Epoch 92, val_loss = 0.0873\n",
      "Run 2, Epoch 93, val_loss = 0.0755\n",
      "Run 2, Epoch 94, val_loss = 0.0641\n",
      "Run 2, Epoch 95, val_loss = 0.0759\n",
      "Run 2, Epoch 96, val_loss = 0.0591\n",
      "Run 2, Epoch 97, val_loss = 0.1690\n",
      "Run 2, Epoch 98, val_loss = 0.0647\n",
      "Run 2, Epoch 99, val_loss = 0.1606\n",
      "Run 2, Epoch 100, val_loss = 0.0519\n",
      "Run 2, Epoch 101, val_loss = 0.0982\n",
      "Run 2, Epoch 102, val_loss = 0.1053\n",
      "Run 2, Epoch 103, val_loss = 0.0591\n",
      "Run 2, Epoch 104, val_loss = 0.1178\n",
      "Run 2, Epoch 105, val_loss = 0.0782\n",
      "Run 2, Epoch 106, val_loss = 0.0566\n",
      "Run 2, Epoch 107, val_loss = 0.0611\n",
      "Run 2, Epoch 108, val_loss = 0.0595\n",
      "Run 2, Epoch 109, val_loss = 0.0904\n",
      "Run 2, Epoch 110, val_loss = 0.0581\n",
      "Run 2, Epoch 111, val_loss = 0.0992\n",
      "Run 2, Epoch 112, val_loss = 0.0805\n",
      "Run 2, Epoch 113, val_loss = 0.1165\n",
      "Run 2, Epoch 114, val_loss = 0.0756\n",
      "Run 2, Epoch 115, val_loss = 0.0562\n",
      "Run 2, Epoch 116, val_loss = 0.0567\n",
      "Run 2, Epoch 117, val_loss = 0.0436\n",
      "Run 2, Epoch 118, val_loss = 0.0467\n",
      "Run 2, Epoch 119, val_loss = 0.0503\n",
      "Run 2, Epoch 120, val_loss = 0.0525\n",
      "Run 2, Epoch 121, val_loss = 0.0481\n",
      "Run 2, Epoch 122, val_loss = 0.0928\n",
      "Run 2, Epoch 123, val_loss = 0.0539\n",
      "Run 2, Epoch 124, val_loss = 0.0447\n",
      "Run 2, Epoch 125, val_loss = 0.0809\n",
      "Run 2, Epoch 126, val_loss = 0.0397\n",
      "Run 2, Epoch 127, val_loss = 0.0503\n",
      "Run 2, Epoch 128, val_loss = 0.0673\n",
      "Run 2, Epoch 129, val_loss = 0.0471\n",
      "Run 2, Epoch 130, val_loss = 0.0404\n",
      "Run 2, Epoch 131, val_loss = 0.1037\n",
      "Run 2, Epoch 132, val_loss = 0.0534\n",
      "Run 2, Epoch 133, val_loss = 0.0501\n",
      "Run 2, Epoch 134, val_loss = 0.1156\n",
      "Run 2, Epoch 135, val_loss = 0.0499\n",
      "Run 2, Epoch 136, val_loss = 0.0491\n",
      "Run 2, Epoch 137, val_loss = 0.0528\n",
      "Run 2, Epoch 138, val_loss = 0.0543\n",
      "Run 2, Epoch 139, val_loss = 0.0512\n",
      "Run 2, Epoch 140, val_loss = 0.0551\n",
      "Run 2, Epoch 141, val_loss = 0.0551\n",
      "Run 2, Epoch 142, val_loss = 0.0572\n",
      "Run 2, Epoch 143, val_loss = 0.0572\n",
      "Run 2, Epoch 144, val_loss = 0.0515\n",
      "Run 2, Epoch 145, val_loss = 0.0598\n",
      "Run 2, Epoch 146, val_loss = 0.0532\n",
      "✅ Run 2: Acc = 99.64%, MDE = 0.0116\n",
      "Run 3/5 - Alpha = 0.9\n",
      "Run 3, Epoch 1, val_loss = 3.7843\n",
      "Run 3, Epoch 2, val_loss = 2.0610\n",
      "Run 3, Epoch 3, val_loss = 1.2067\n",
      "Run 3, Epoch 4, val_loss = 1.0676\n",
      "Run 3, Epoch 5, val_loss = 0.7134\n",
      "Run 3, Epoch 6, val_loss = 0.6739\n",
      "Run 3, Epoch 7, val_loss = 0.6923\n",
      "Run 3, Epoch 8, val_loss = 0.5992\n",
      "Run 3, Epoch 9, val_loss = 0.5353\n",
      "Run 3, Epoch 10, val_loss = 0.4746\n",
      "Run 3, Epoch 11, val_loss = 0.4760\n",
      "Run 3, Epoch 12, val_loss = 0.4534\n",
      "Run 3, Epoch 13, val_loss = 0.3541\n",
      "Run 3, Epoch 14, val_loss = 0.3246\n",
      "Run 3, Epoch 15, val_loss = 0.4920\n",
      "Run 3, Epoch 16, val_loss = 0.2408\n",
      "Run 3, Epoch 17, val_loss = 0.4359\n",
      "Run 3, Epoch 18, val_loss = 0.2216\n",
      "Run 3, Epoch 19, val_loss = 0.2380\n",
      "Run 3, Epoch 20, val_loss = 0.2263\n",
      "Run 3, Epoch 21, val_loss = 0.2974\n",
      "Run 3, Epoch 22, val_loss = 0.2260\n",
      "Run 3, Epoch 23, val_loss = 0.1929\n",
      "Run 3, Epoch 24, val_loss = 0.1509\n",
      "Run 3, Epoch 25, val_loss = 0.1921\n",
      "Run 3, Epoch 26, val_loss = 0.2799\n",
      "Run 3, Epoch 27, val_loss = 0.1808\n",
      "Run 3, Epoch 28, val_loss = 0.1451\n",
      "Run 3, Epoch 29, val_loss = 0.1613\n",
      "Run 3, Epoch 30, val_loss = 0.1414\n",
      "Run 3, Epoch 31, val_loss = 0.1589\n",
      "Run 3, Epoch 32, val_loss = 0.1289\n",
      "Run 3, Epoch 33, val_loss = 0.1309\n",
      "Run 3, Epoch 34, val_loss = 0.1399\n",
      "Run 3, Epoch 35, val_loss = 0.1506\n",
      "Run 3, Epoch 36, val_loss = 0.2728\n",
      "Run 3, Epoch 37, val_loss = 0.1203\n",
      "Run 3, Epoch 38, val_loss = 0.1175\n",
      "Run 3, Epoch 39, val_loss = 0.1177\n",
      "Run 3, Epoch 40, val_loss = 0.1075\n",
      "Run 3, Epoch 41, val_loss = 0.1113\n",
      "Run 3, Epoch 42, val_loss = 0.1387\n",
      "Run 3, Epoch 43, val_loss = 0.1083\n",
      "Run 3, Epoch 44, val_loss = 0.1148\n",
      "Run 3, Epoch 45, val_loss = 0.1278\n",
      "Run 3, Epoch 46, val_loss = 0.1049\n",
      "Run 3, Epoch 47, val_loss = 0.0942\n",
      "Run 3, Epoch 48, val_loss = 0.0725\n",
      "Run 3, Epoch 49, val_loss = 0.1127\n",
      "Run 3, Epoch 50, val_loss = 0.0940\n",
      "Run 3, Epoch 51, val_loss = 0.1092\n",
      "Run 3, Epoch 52, val_loss = 0.1024\n",
      "Run 3, Epoch 53, val_loss = 0.1183\n",
      "Run 3, Epoch 54, val_loss = 0.0890\n",
      "Run 3, Epoch 55, val_loss = 0.0857\n",
      "Run 3, Epoch 56, val_loss = 0.0935\n",
      "Run 3, Epoch 57, val_loss = 0.0993\n",
      "Run 3, Epoch 58, val_loss = 0.0913\n",
      "Run 3, Epoch 59, val_loss = 0.0948\n",
      "Run 3, Epoch 60, val_loss = 0.0730\n",
      "Run 3, Epoch 61, val_loss = 0.1004\n",
      "Run 3, Epoch 62, val_loss = 0.0994\n",
      "Run 3, Epoch 63, val_loss = 0.0794\n",
      "Run 3, Epoch 64, val_loss = 0.1022\n",
      "Run 3, Epoch 65, val_loss = 0.0676\n",
      "Run 3, Epoch 66, val_loss = 0.0963\n",
      "Run 3, Epoch 67, val_loss = 0.0677\n",
      "Run 3, Epoch 68, val_loss = 0.1303\n",
      "Run 3, Epoch 69, val_loss = 0.0655\n",
      "Run 3, Epoch 70, val_loss = 0.0724\n",
      "Run 3, Epoch 71, val_loss = 0.0628\n",
      "Run 3, Epoch 72, val_loss = 0.0763\n",
      "Run 3, Epoch 73, val_loss = 0.0866\n",
      "Run 3, Epoch 74, val_loss = 0.0653\n",
      "Run 3, Epoch 75, val_loss = 0.0548\n",
      "Run 3, Epoch 76, val_loss = 0.0765\n",
      "Run 3, Epoch 77, val_loss = 0.0765\n",
      "Run 3, Epoch 78, val_loss = 0.0675\n",
      "Run 3, Epoch 79, val_loss = 0.0764\n",
      "Run 3, Epoch 80, val_loss = 0.0811\n",
      "Run 3, Epoch 81, val_loss = 0.0708\n",
      "Run 3, Epoch 82, val_loss = 0.0593\n",
      "Run 3, Epoch 83, val_loss = 0.0683\n",
      "Run 3, Epoch 84, val_loss = 0.0728\n",
      "Run 3, Epoch 85, val_loss = 0.0933\n",
      "Run 3, Epoch 86, val_loss = 0.0585\n",
      "Run 3, Epoch 87, val_loss = 0.0732\n",
      "Run 3, Epoch 88, val_loss = 0.0616\n",
      "Run 3, Epoch 89, val_loss = 0.0694\n",
      "Run 3, Epoch 90, val_loss = 0.0725\n",
      "Run 3, Epoch 91, val_loss = 0.0684\n",
      "Run 3, Epoch 92, val_loss = 0.0546\n",
      "Run 3, Epoch 93, val_loss = 0.0530\n",
      "Run 3, Epoch 94, val_loss = 0.0531\n",
      "Run 3, Epoch 95, val_loss = 0.0604\n",
      "Run 3, Epoch 96, val_loss = 0.0559\n",
      "Run 3, Epoch 97, val_loss = 0.0621\n",
      "Run 3, Epoch 98, val_loss = 0.0636\n",
      "Run 3, Epoch 99, val_loss = 0.0824\n",
      "Run 3, Epoch 100, val_loss = 0.0509\n",
      "Run 3, Epoch 101, val_loss = 0.0514\n",
      "Run 3, Epoch 102, val_loss = 0.0948\n",
      "Run 3, Epoch 103, val_loss = 0.0458\n",
      "Run 3, Epoch 104, val_loss = 0.0578\n",
      "Run 3, Epoch 105, val_loss = 0.0549\n",
      "Run 3, Epoch 106, val_loss = 0.0546\n",
      "Run 3, Epoch 107, val_loss = 0.0433\n",
      "Run 3, Epoch 108, val_loss = 0.0513\n",
      "Run 3, Epoch 109, val_loss = 0.0508\n",
      "Run 3, Epoch 110, val_loss = 0.0545\n",
      "Run 3, Epoch 111, val_loss = 0.0577\n",
      "Run 3, Epoch 112, val_loss = 0.0458\n",
      "Run 3, Epoch 113, val_loss = 0.0500\n",
      "Run 3, Epoch 114, val_loss = 0.0542\n",
      "Run 3, Epoch 115, val_loss = 0.0471\n",
      "Run 3, Epoch 116, val_loss = 0.0544\n",
      "Run 3, Epoch 117, val_loss = 0.0634\n",
      "Run 3, Epoch 118, val_loss = 0.0634\n",
      "Run 3, Epoch 119, val_loss = 0.0614\n",
      "Run 3, Epoch 120, val_loss = 0.0645\n",
      "Run 3, Epoch 121, val_loss = 0.0525\n",
      "Run 3, Epoch 122, val_loss = 0.0608\n",
      "Run 3, Epoch 123, val_loss = 0.0507\n",
      "Run 3, Epoch 124, val_loss = 0.0501\n",
      "Run 3, Epoch 125, val_loss = 0.0515\n",
      "Run 3, Epoch 126, val_loss = 0.0533\n",
      "Run 3, Epoch 127, val_loss = 0.0543\n",
      "✅ Run 3: Acc = 99.75%, MDE = 0.0083\n",
      "Run 4/5 - Alpha = 0.9\n",
      "Run 4, Epoch 1, val_loss = 3.7038\n",
      "Run 4, Epoch 2, val_loss = 1.8891\n",
      "Run 4, Epoch 3, val_loss = 1.5827\n",
      "Run 4, Epoch 4, val_loss = 0.9880\n",
      "Run 4, Epoch 5, val_loss = 0.8187\n",
      "Run 4, Epoch 6, val_loss = 0.9009\n",
      "Run 4, Epoch 7, val_loss = 0.6951\n",
      "Run 4, Epoch 8, val_loss = 0.5594\n",
      "Run 4, Epoch 9, val_loss = 0.5519\n",
      "Run 4, Epoch 10, val_loss = 0.3662\n",
      "Run 4, Epoch 11, val_loss = 0.4696\n",
      "Run 4, Epoch 12, val_loss = 0.4168\n",
      "Run 4, Epoch 13, val_loss = 0.3000\n",
      "Run 4, Epoch 14, val_loss = 0.3617\n",
      "Run 4, Epoch 15, val_loss = 0.4053\n",
      "Run 4, Epoch 16, val_loss = 0.2314\n",
      "Run 4, Epoch 17, val_loss = 0.5585\n",
      "Run 4, Epoch 18, val_loss = 0.2798\n",
      "Run 4, Epoch 19, val_loss = 0.2161\n",
      "Run 4, Epoch 20, val_loss = 0.2098\n",
      "Run 4, Epoch 21, val_loss = 0.2595\n",
      "Run 4, Epoch 22, val_loss = 0.2095\n",
      "Run 4, Epoch 23, val_loss = 0.1693\n",
      "Run 4, Epoch 24, val_loss = 0.1582\n",
      "Run 4, Epoch 25, val_loss = 0.1519\n",
      "Run 4, Epoch 26, val_loss = 0.1617\n",
      "Run 4, Epoch 27, val_loss = 0.1429\n",
      "Run 4, Epoch 28, val_loss = 0.1755\n",
      "Run 4, Epoch 29, val_loss = 0.1701\n",
      "Run 4, Epoch 30, val_loss = 0.1551\n",
      "Run 4, Epoch 31, val_loss = 0.1169\n",
      "Run 4, Epoch 32, val_loss = 0.1539\n",
      "Run 4, Epoch 33, val_loss = 0.1636\n",
      "Run 4, Epoch 34, val_loss = 0.1694\n",
      "Run 4, Epoch 35, val_loss = 0.1307\n",
      "Run 4, Epoch 36, val_loss = 0.1960\n",
      "Run 4, Epoch 37, val_loss = 0.1182\n",
      "Run 4, Epoch 38, val_loss = 0.1001\n",
      "Run 4, Epoch 39, val_loss = 0.1070\n",
      "Run 4, Epoch 40, val_loss = 0.1411\n",
      "Run 4, Epoch 41, val_loss = 0.1447\n",
      "Run 4, Epoch 42, val_loss = 0.1067\n",
      "Run 4, Epoch 43, val_loss = 0.1589\n",
      "Run 4, Epoch 44, val_loss = 0.1389\n",
      "Run 4, Epoch 45, val_loss = 0.1070\n",
      "Run 4, Epoch 46, val_loss = 0.0957\n",
      "Run 4, Epoch 47, val_loss = 0.1055\n",
      "Run 4, Epoch 48, val_loss = 0.1098\n",
      "Run 4, Epoch 49, val_loss = 0.1028\n",
      "Run 4, Epoch 50, val_loss = 0.1217\n",
      "Run 4, Epoch 51, val_loss = 0.1302\n",
      "Run 4, Epoch 52, val_loss = 0.1247\n",
      "Run 4, Epoch 53, val_loss = 0.1307\n",
      "Run 4, Epoch 54, val_loss = 0.0795\n",
      "Run 4, Epoch 55, val_loss = 0.0940\n",
      "Run 4, Epoch 56, val_loss = 0.0981\n",
      "Run 4, Epoch 57, val_loss = 0.0947\n",
      "Run 4, Epoch 58, val_loss = 0.0766\n",
      "Run 4, Epoch 59, val_loss = 0.0964\n",
      "Run 4, Epoch 60, val_loss = 0.0878\n",
      "Run 4, Epoch 61, val_loss = 0.1504\n",
      "Run 4, Epoch 62, val_loss = 0.1025\n",
      "Run 4, Epoch 63, val_loss = 0.1366\n",
      "Run 4, Epoch 64, val_loss = 0.0712\n",
      "Run 4, Epoch 65, val_loss = 0.1201\n",
      "Run 4, Epoch 66, val_loss = 0.1140\n",
      "Run 4, Epoch 67, val_loss = 0.1043\n",
      "Run 4, Epoch 68, val_loss = 0.0858\n",
      "Run 4, Epoch 69, val_loss = 0.0765\n",
      "Run 4, Epoch 70, val_loss = 0.0850\n",
      "Run 4, Epoch 71, val_loss = 0.0892\n",
      "Run 4, Epoch 72, val_loss = 0.0775\n",
      "Run 4, Epoch 73, val_loss = 0.1156\n",
      "Run 4, Epoch 74, val_loss = 0.0952\n",
      "Run 4, Epoch 75, val_loss = 0.0644\n",
      "Run 4, Epoch 76, val_loss = 0.0773\n",
      "Run 4, Epoch 77, val_loss = 0.0706\n",
      "Run 4, Epoch 78, val_loss = 0.0918\n",
      "Run 4, Epoch 79, val_loss = 0.1522\n",
      "Run 4, Epoch 80, val_loss = 0.0744\n",
      "Run 4, Epoch 81, val_loss = 0.0919\n",
      "Run 4, Epoch 82, val_loss = 0.0908\n",
      "Run 4, Epoch 83, val_loss = 0.0858\n",
      "Run 4, Epoch 84, val_loss = 0.0992\n",
      "Run 4, Epoch 85, val_loss = 0.0645\n",
      "Run 4, Epoch 86, val_loss = 0.0707\n",
      "Run 4, Epoch 87, val_loss = 0.1241\n",
      "Run 4, Epoch 88, val_loss = 0.0722\n",
      "Run 4, Epoch 89, val_loss = 0.0952\n",
      "Run 4, Epoch 90, val_loss = 0.1108\n",
      "Run 4, Epoch 91, val_loss = 0.0803\n",
      "Run 4, Epoch 92, val_loss = 0.0593\n",
      "Run 4, Epoch 93, val_loss = 0.0514\n",
      "Run 4, Epoch 94, val_loss = 0.0538\n",
      "Run 4, Epoch 95, val_loss = 0.0715\n",
      "Run 4, Epoch 96, val_loss = 0.0559\n",
      "Run 4, Epoch 97, val_loss = 0.0570\n",
      "Run 4, Epoch 98, val_loss = 0.0804\n",
      "Run 4, Epoch 99, val_loss = 0.0682\n",
      "Run 4, Epoch 100, val_loss = 0.0662\n",
      "Run 4, Epoch 101, val_loss = 0.0603\n",
      "Run 4, Epoch 102, val_loss = 0.0510\n",
      "Run 4, Epoch 103, val_loss = 0.0775\n",
      "Run 4, Epoch 104, val_loss = 0.0547\n",
      "Run 4, Epoch 105, val_loss = 0.0502\n",
      "Run 4, Epoch 106, val_loss = 0.0594\n",
      "Run 4, Epoch 107, val_loss = 0.0599\n",
      "Run 4, Epoch 108, val_loss = 0.0514\n",
      "Run 4, Epoch 109, val_loss = 0.0786\n",
      "Run 4, Epoch 110, val_loss = 0.0594\n",
      "Run 4, Epoch 111, val_loss = 0.0532\n",
      "Run 4, Epoch 112, val_loss = 0.0564\n",
      "Run 4, Epoch 113, val_loss = 0.0575\n",
      "Run 4, Epoch 114, val_loss = 0.0656\n",
      "Run 4, Epoch 115, val_loss = 0.0532\n",
      "Run 4, Epoch 116, val_loss = 0.0621\n",
      "Run 4, Epoch 117, val_loss = 0.0680\n",
      "Run 4, Epoch 118, val_loss = 0.0632\n",
      "Run 4, Epoch 119, val_loss = 0.0793\n",
      "Run 4, Epoch 120, val_loss = 0.0608\n",
      "Run 4, Epoch 121, val_loss = 0.0715\n",
      "Run 4, Epoch 122, val_loss = 0.0542\n",
      "Run 4, Epoch 123, val_loss = 0.0571\n",
      "Run 4, Epoch 124, val_loss = 0.0514\n",
      "Run 4, Epoch 125, val_loss = 0.0590\n",
      "✅ Run 4: Acc = 99.64%, MDE = 0.0111\n",
      "Run 5/5 - Alpha = 0.9\n",
      "Run 5, Epoch 1, val_loss = 3.6588\n",
      "Run 5, Epoch 2, val_loss = 2.1809\n",
      "Run 5, Epoch 3, val_loss = 1.4625\n",
      "Run 5, Epoch 4, val_loss = 1.1125\n",
      "Run 5, Epoch 5, val_loss = 0.9595\n",
      "Run 5, Epoch 6, val_loss = 0.7269\n",
      "Run 5, Epoch 7, val_loss = 0.6410\n",
      "Run 5, Epoch 8, val_loss = 0.6731\n",
      "Run 5, Epoch 9, val_loss = 0.5488\n",
      "Run 5, Epoch 10, val_loss = 0.5077\n",
      "Run 5, Epoch 11, val_loss = 0.4639\n",
      "Run 5, Epoch 12, val_loss = 0.3680\n",
      "Run 5, Epoch 13, val_loss = 0.4339\n",
      "Run 5, Epoch 14, val_loss = 0.3153\n",
      "Run 5, Epoch 15, val_loss = 0.3117\n",
      "Run 5, Epoch 16, val_loss = 0.3852\n",
      "Run 5, Epoch 17, val_loss = 0.2087\n",
      "Run 5, Epoch 18, val_loss = 0.2669\n",
      "Run 5, Epoch 19, val_loss = 0.2349\n",
      "Run 5, Epoch 20, val_loss = 0.2254\n",
      "Run 5, Epoch 21, val_loss = 0.1884\n",
      "Run 5, Epoch 22, val_loss = 0.2467\n",
      "Run 5, Epoch 23, val_loss = 0.1641\n",
      "Run 5, Epoch 24, val_loss = 0.1655\n",
      "Run 5, Epoch 25, val_loss = 0.2125\n",
      "Run 5, Epoch 26, val_loss = 0.1918\n",
      "Run 5, Epoch 27, val_loss = 0.1747\n",
      "Run 5, Epoch 28, val_loss = 0.1173\n",
      "Run 5, Epoch 29, val_loss = 0.1543\n",
      "Run 5, Epoch 30, val_loss = 0.1914\n",
      "Run 5, Epoch 31, val_loss = 0.1543\n",
      "Run 5, Epoch 32, val_loss = 0.1258\n",
      "Run 5, Epoch 33, val_loss = 0.1498\n",
      "Run 5, Epoch 34, val_loss = 0.1329\n",
      "Run 5, Epoch 35, val_loss = 0.1788\n",
      "Run 5, Epoch 36, val_loss = 0.1241\n",
      "Run 5, Epoch 37, val_loss = 0.1469\n",
      "Run 5, Epoch 38, val_loss = 0.1092\n",
      "Run 5, Epoch 39, val_loss = 0.1056\n",
      "Run 5, Epoch 40, val_loss = 0.1377\n",
      "Run 5, Epoch 41, val_loss = 0.1049\n",
      "Run 5, Epoch 42, val_loss = 0.1087\n",
      "Run 5, Epoch 43, val_loss = 0.0971\n",
      "Run 5, Epoch 44, val_loss = 0.1133\n",
      "Run 5, Epoch 45, val_loss = 0.1363\n",
      "Run 5, Epoch 46, val_loss = 0.1360\n",
      "Run 5, Epoch 47, val_loss = 0.1050\n",
      "Run 5, Epoch 48, val_loss = 0.1036\n",
      "Run 5, Epoch 49, val_loss = 0.1115\n",
      "Run 5, Epoch 50, val_loss = 0.0909\n",
      "Run 5, Epoch 51, val_loss = 0.1179\n",
      "Run 5, Epoch 52, val_loss = 0.1070\n",
      "Run 5, Epoch 53, val_loss = 0.1121\n",
      "Run 5, Epoch 54, val_loss = 0.1194\n",
      "Run 5, Epoch 55, val_loss = 0.1045\n",
      "Run 5, Epoch 56, val_loss = 0.1031\n",
      "Run 5, Epoch 57, val_loss = 0.0938\n",
      "Run 5, Epoch 58, val_loss = 0.1099\n",
      "Run 5, Epoch 59, val_loss = 0.0699\n",
      "Run 5, Epoch 60, val_loss = 0.1095\n",
      "Run 5, Epoch 61, val_loss = 0.1449\n",
      "Run 5, Epoch 62, val_loss = 0.1582\n",
      "Run 5, Epoch 63, val_loss = 0.0824\n",
      "Run 5, Epoch 64, val_loss = 0.1038\n",
      "Run 5, Epoch 65, val_loss = 0.1639\n",
      "Run 5, Epoch 66, val_loss = 0.0987\n",
      "Run 5, Epoch 67, val_loss = 0.0738\n",
      "Run 5, Epoch 68, val_loss = 0.0882\n",
      "Run 5, Epoch 69, val_loss = 0.0719\n",
      "Run 5, Epoch 70, val_loss = 0.0849\n",
      "Run 5, Epoch 71, val_loss = 0.0882\n",
      "Run 5, Epoch 72, val_loss = 0.1072\n",
      "Run 5, Epoch 73, val_loss = 0.1626\n",
      "Run 5, Epoch 74, val_loss = 0.0662\n",
      "Run 5, Epoch 75, val_loss = 0.0800\n",
      "Run 5, Epoch 76, val_loss = 0.0855\n",
      "Run 5, Epoch 77, val_loss = 0.0864\n",
      "Run 5, Epoch 78, val_loss = 0.0793\n",
      "Run 5, Epoch 79, val_loss = 0.1147\n",
      "Run 5, Epoch 80, val_loss = 0.0986\n",
      "Run 5, Epoch 81, val_loss = 0.0850\n",
      "Run 5, Epoch 82, val_loss = 0.1174\n",
      "Run 5, Epoch 83, val_loss = 0.1000\n",
      "Run 5, Epoch 84, val_loss = 0.1020\n",
      "Run 5, Epoch 85, val_loss = 0.0803\n",
      "Run 5, Epoch 86, val_loss = 0.0870\n",
      "Run 5, Epoch 87, val_loss = 0.0939\n",
      "Run 5, Epoch 88, val_loss = 0.1238\n",
      "Run 5, Epoch 89, val_loss = 0.0841\n",
      "Run 5, Epoch 90, val_loss = 0.0916\n",
      "Run 5, Epoch 91, val_loss = 0.0639\n",
      "Run 5, Epoch 92, val_loss = 0.0767\n",
      "Run 5, Epoch 93, val_loss = 0.0715\n",
      "Run 5, Epoch 94, val_loss = 0.1242\n",
      "Run 5, Epoch 95, val_loss = 0.0592\n",
      "Run 5, Epoch 96, val_loss = 0.0709\n",
      "Run 5, Epoch 97, val_loss = 0.0747\n",
      "Run 5, Epoch 98, val_loss = 0.0939\n",
      "Run 5, Epoch 99, val_loss = 0.0609\n",
      "Run 5, Epoch 100, val_loss = 0.0625\n",
      "Run 5, Epoch 101, val_loss = 0.0870\n",
      "Run 5, Epoch 102, val_loss = 0.0734\n",
      "Run 5, Epoch 103, val_loss = 0.1021\n",
      "Run 5, Epoch 104, val_loss = 0.0705\n",
      "Run 5, Epoch 105, val_loss = 0.0978\n",
      "Run 5, Epoch 106, val_loss = 0.0674\n",
      "Run 5, Epoch 107, val_loss = 0.0908\n",
      "Run 5, Epoch 108, val_loss = 0.0680\n",
      "Run 5, Epoch 109, val_loss = 0.0718\n",
      "Run 5, Epoch 110, val_loss = 0.0701\n",
      "Run 5, Epoch 111, val_loss = 0.0645\n",
      "Run 5, Epoch 112, val_loss = 0.0525\n",
      "Run 5, Epoch 113, val_loss = 0.0592\n",
      "Run 5, Epoch 114, val_loss = 0.1073\n",
      "Run 5, Epoch 115, val_loss = 0.0673\n",
      "Run 5, Epoch 116, val_loss = 0.0658\n",
      "Run 5, Epoch 117, val_loss = 0.0645\n",
      "Run 5, Epoch 118, val_loss = 0.0622\n",
      "Run 5, Epoch 119, val_loss = 0.0552\n",
      "Run 5, Epoch 120, val_loss = 0.0667\n",
      "Run 5, Epoch 121, val_loss = 0.0656\n",
      "Run 5, Epoch 122, val_loss = 0.1105\n",
      "Run 5, Epoch 123, val_loss = 0.0624\n",
      "Run 5, Epoch 124, val_loss = 0.0571\n",
      "Run 5, Epoch 125, val_loss = 0.0545\n",
      "Run 5, Epoch 126, val_loss = 0.0655\n",
      "Run 5, Epoch 127, val_loss = 0.0788\n",
      "Run 5, Epoch 128, val_loss = 0.0686\n",
      "Run 5, Epoch 129, val_loss = 0.0593\n",
      "Run 5, Epoch 130, val_loss = 0.0816\n",
      "Run 5, Epoch 131, val_loss = 0.0628\n",
      "Run 5, Epoch 132, val_loss = 0.0597\n",
      "✅ Run 5: Acc = 99.59%, MDE = 0.0153\n",
      "📁 Results & all errors saved to repeat_copy/09\n",
      "\n",
      "[Alpha = 1.0] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 1.0\n",
      "Run 1, Epoch 1, val_loss = 3.6918\n",
      "Run 1, Epoch 2, val_loss = 2.3343\n",
      "Run 1, Epoch 3, val_loss = 1.5902\n",
      "Run 1, Epoch 4, val_loss = 1.4575\n",
      "Run 1, Epoch 5, val_loss = 0.8085\n",
      "Run 1, Epoch 6, val_loss = 0.7348\n",
      "Run 1, Epoch 7, val_loss = 0.8872\n",
      "Run 1, Epoch 8, val_loss = 0.8700\n",
      "Run 1, Epoch 9, val_loss = 0.7383\n",
      "Run 1, Epoch 10, val_loss = 0.5451\n",
      "Run 1, Epoch 11, val_loss = 0.5430\n",
      "Run 1, Epoch 12, val_loss = 0.4568\n",
      "Run 1, Epoch 13, val_loss = 0.5320\n",
      "Run 1, Epoch 14, val_loss = 0.3388\n",
      "Run 1, Epoch 15, val_loss = 0.3536\n",
      "Run 1, Epoch 16, val_loss = 0.3830\n",
      "Run 1, Epoch 17, val_loss = 0.2683\n",
      "Run 1, Epoch 18, val_loss = 0.2640\n",
      "Run 1, Epoch 19, val_loss = 0.3051\n",
      "Run 1, Epoch 20, val_loss = 0.3410\n",
      "Run 1, Epoch 21, val_loss = 0.2007\n",
      "Run 1, Epoch 22, val_loss = 0.1978\n",
      "Run 1, Epoch 23, val_loss = 0.2377\n",
      "Run 1, Epoch 24, val_loss = 0.3265\n",
      "Run 1, Epoch 25, val_loss = 0.2175\n",
      "Run 1, Epoch 26, val_loss = 0.2428\n",
      "Run 1, Epoch 27, val_loss = 0.2504\n",
      "Run 1, Epoch 28, val_loss = 0.2066\n",
      "Run 1, Epoch 29, val_loss = 0.1874\n",
      "Run 1, Epoch 30, val_loss = 0.1731\n",
      "Run 1, Epoch 31, val_loss = 0.1832\n",
      "Run 1, Epoch 32, val_loss = 0.1145\n",
      "Run 1, Epoch 33, val_loss = 0.1288\n",
      "Run 1, Epoch 34, val_loss = 0.1833\n",
      "Run 1, Epoch 35, val_loss = 0.1378\n",
      "Run 1, Epoch 36, val_loss = 0.1450\n",
      "Run 1, Epoch 37, val_loss = 0.1548\n",
      "Run 1, Epoch 38, val_loss = 0.1299\n",
      "Run 1, Epoch 39, val_loss = 0.1118\n",
      "Run 1, Epoch 40, val_loss = 0.1363\n",
      "Run 1, Epoch 41, val_loss = 0.1259\n",
      "Run 1, Epoch 42, val_loss = 0.0977\n",
      "Run 1, Epoch 43, val_loss = 0.1329\n",
      "Run 1, Epoch 44, val_loss = 0.1076\n",
      "Run 1, Epoch 45, val_loss = 0.0992\n",
      "Run 1, Epoch 46, val_loss = 0.1097\n",
      "Run 1, Epoch 47, val_loss = 0.1176\n",
      "Run 1, Epoch 48, val_loss = 0.1204\n",
      "Run 1, Epoch 49, val_loss = 0.0966\n",
      "Run 1, Epoch 50, val_loss = 0.0796\n",
      "Run 1, Epoch 51, val_loss = 0.1112\n",
      "Run 1, Epoch 52, val_loss = 0.1106\n",
      "Run 1, Epoch 53, val_loss = 0.1272\n",
      "Run 1, Epoch 54, val_loss = 0.0872\n",
      "Run 1, Epoch 55, val_loss = 0.1254\n",
      "Run 1, Epoch 56, val_loss = 0.1138\n",
      "Run 1, Epoch 57, val_loss = 0.0862\n",
      "Run 1, Epoch 58, val_loss = 0.1519\n",
      "Run 1, Epoch 59, val_loss = 0.0796\n",
      "Run 1, Epoch 60, val_loss = 0.0779\n",
      "Run 1, Epoch 61, val_loss = 0.0972\n",
      "Run 1, Epoch 62, val_loss = 0.0971\n",
      "Run 1, Epoch 63, val_loss = 0.0877\n",
      "Run 1, Epoch 64, val_loss = 0.0809\n",
      "Run 1, Epoch 65, val_loss = 0.0802\n",
      "Run 1, Epoch 66, val_loss = 0.1045\n",
      "Run 1, Epoch 67, val_loss = 0.0889\n",
      "Run 1, Epoch 68, val_loss = 0.0919\n",
      "Run 1, Epoch 69, val_loss = 0.0987\n",
      "Run 1, Epoch 70, val_loss = 0.0964\n",
      "Run 1, Epoch 71, val_loss = 0.2047\n",
      "Run 1, Epoch 72, val_loss = 0.0964\n",
      "Run 1, Epoch 73, val_loss = 0.1179\n",
      "Run 1, Epoch 74, val_loss = 0.1052\n",
      "Run 1, Epoch 75, val_loss = 0.0906\n",
      "Run 1, Epoch 76, val_loss = 0.1020\n",
      "Run 1, Epoch 77, val_loss = 0.0897\n",
      "Run 1, Epoch 78, val_loss = 0.0676\n",
      "Run 1, Epoch 79, val_loss = 0.0685\n",
      "Run 1, Epoch 80, val_loss = 0.0825\n",
      "Run 1, Epoch 81, val_loss = 0.0718\n",
      "Run 1, Epoch 82, val_loss = 0.0650\n",
      "Run 1, Epoch 83, val_loss = 0.0592\n",
      "Run 1, Epoch 84, val_loss = 0.0908\n",
      "Run 1, Epoch 85, val_loss = 0.0716\n",
      "Run 1, Epoch 86, val_loss = 0.0673\n",
      "Run 1, Epoch 87, val_loss = 0.1426\n",
      "Run 1, Epoch 88, val_loss = 0.0757\n",
      "Run 1, Epoch 89, val_loss = 0.0600\n",
      "Run 1, Epoch 90, val_loss = 0.0628\n",
      "Run 1, Epoch 91, val_loss = 0.0618\n",
      "Run 1, Epoch 92, val_loss = 0.0583\n",
      "Run 1, Epoch 93, val_loss = 0.0579\n",
      "Run 1, Epoch 94, val_loss = 0.0530\n",
      "Run 1, Epoch 95, val_loss = 0.0507\n",
      "Run 1, Epoch 96, val_loss = 0.0695\n",
      "Run 1, Epoch 97, val_loss = 0.0634\n",
      "Run 1, Epoch 98, val_loss = 0.0497\n",
      "Run 1, Epoch 99, val_loss = 0.0642\n",
      "Run 1, Epoch 100, val_loss = 0.0627\n",
      "Run 1, Epoch 101, val_loss = 0.0944\n",
      "Run 1, Epoch 102, val_loss = 0.0745\n",
      "Run 1, Epoch 103, val_loss = 0.0539\n",
      "Run 1, Epoch 104, val_loss = 0.0497\n",
      "Run 1, Epoch 105, val_loss = 0.0670\n",
      "Run 1, Epoch 106, val_loss = 0.0605\n",
      "Run 1, Epoch 107, val_loss = 0.0602\n",
      "Run 1, Epoch 108, val_loss = 0.0746\n",
      "Run 1, Epoch 109, val_loss = 0.0583\n",
      "Run 1, Epoch 110, val_loss = 0.0648\n",
      "Run 1, Epoch 111, val_loss = 0.0471\n",
      "Run 1, Epoch 112, val_loss = 0.0708\n",
      "Run 1, Epoch 113, val_loss = 0.0550\n",
      "Run 1, Epoch 114, val_loss = 0.0744\n",
      "Run 1, Epoch 115, val_loss = 0.0507\n",
      "Run 1, Epoch 116, val_loss = 0.0605\n",
      "Run 1, Epoch 117, val_loss = 0.0674\n",
      "Run 1, Epoch 118, val_loss = 0.0671\n",
      "Run 1, Epoch 119, val_loss = 0.0569\n",
      "Run 1, Epoch 120, val_loss = 0.0482\n",
      "Run 1, Epoch 121, val_loss = 0.0630\n",
      "Run 1, Epoch 122, val_loss = 0.0645\n",
      "Run 1, Epoch 123, val_loss = 0.0568\n",
      "Run 1, Epoch 124, val_loss = 0.0664\n",
      "Run 1, Epoch 125, val_loss = 0.0593\n",
      "Run 1, Epoch 126, val_loss = 0.0533\n",
      "Run 1, Epoch 127, val_loss = 0.0759\n",
      "Run 1, Epoch 128, val_loss = 0.0524\n",
      "Run 1, Epoch 129, val_loss = 0.0514\n",
      "Run 1, Epoch 130, val_loss = 0.0689\n",
      "Run 1, Epoch 131, val_loss = 0.0505\n",
      "✅ Run 1: Acc = 99.69%, MDE = 0.0126\n",
      "Run 2/5 - Alpha = 1.0\n",
      "Run 2, Epoch 1, val_loss = 3.7169\n",
      "Run 2, Epoch 2, val_loss = 2.1327\n",
      "Run 2, Epoch 3, val_loss = 1.6053\n",
      "Run 2, Epoch 4, val_loss = 0.9371\n",
      "Run 2, Epoch 5, val_loss = 1.1133\n",
      "Run 2, Epoch 6, val_loss = 0.7370\n",
      "Run 2, Epoch 7, val_loss = 0.8221\n",
      "Run 2, Epoch 8, val_loss = 0.5142\n",
      "Run 2, Epoch 9, val_loss = 0.6168\n",
      "Run 2, Epoch 10, val_loss = 0.5528\n",
      "Run 2, Epoch 11, val_loss = 0.4233\n",
      "Run 2, Epoch 12, val_loss = 0.3959\n",
      "Run 2, Epoch 13, val_loss = 0.3704\n",
      "Run 2, Epoch 14, val_loss = 0.4008\n",
      "Run 2, Epoch 15, val_loss = 0.4752\n",
      "Run 2, Epoch 16, val_loss = 0.3995\n",
      "Run 2, Epoch 17, val_loss = 0.3757\n",
      "Run 2, Epoch 18, val_loss = 0.4015\n",
      "Run 2, Epoch 19, val_loss = 0.3492\n",
      "Run 2, Epoch 20, val_loss = 0.4713\n",
      "Run 2, Epoch 21, val_loss = 0.2401\n",
      "Run 2, Epoch 22, val_loss = 0.2156\n",
      "Run 2, Epoch 23, val_loss = 0.2535\n",
      "Run 2, Epoch 24, val_loss = 0.2752\n",
      "Run 2, Epoch 25, val_loss = 0.1989\n",
      "Run 2, Epoch 26, val_loss = 0.1831\n",
      "Run 2, Epoch 27, val_loss = 0.1541\n",
      "Run 2, Epoch 28, val_loss = 0.1631\n",
      "Run 2, Epoch 29, val_loss = 0.2920\n",
      "Run 2, Epoch 30, val_loss = 0.1614\n",
      "Run 2, Epoch 31, val_loss = 0.1806\n",
      "Run 2, Epoch 32, val_loss = 0.1070\n",
      "Run 2, Epoch 33, val_loss = 0.1469\n",
      "Run 2, Epoch 34, val_loss = 0.1103\n",
      "Run 2, Epoch 35, val_loss = 0.1916\n",
      "Run 2, Epoch 36, val_loss = 0.1467\n",
      "Run 2, Epoch 37, val_loss = 0.1485\n",
      "Run 2, Epoch 38, val_loss = 0.1190\n",
      "Run 2, Epoch 39, val_loss = 0.1529\n",
      "Run 2, Epoch 40, val_loss = 0.1179\n",
      "Run 2, Epoch 41, val_loss = 0.0976\n",
      "Run 2, Epoch 42, val_loss = 0.0826\n",
      "Run 2, Epoch 43, val_loss = 0.0918\n",
      "Run 2, Epoch 44, val_loss = 0.1197\n",
      "Run 2, Epoch 45, val_loss = 0.1008\n",
      "Run 2, Epoch 46, val_loss = 0.1120\n",
      "Run 2, Epoch 47, val_loss = 0.1206\n",
      "Run 2, Epoch 48, val_loss = 0.1058\n",
      "Run 2, Epoch 49, val_loss = 0.1133\n",
      "Run 2, Epoch 50, val_loss = 0.0857\n",
      "Run 2, Epoch 51, val_loss = 0.1079\n",
      "Run 2, Epoch 52, val_loss = 0.0926\n",
      "Run 2, Epoch 53, val_loss = 0.1054\n",
      "Run 2, Epoch 54, val_loss = 0.0743\n",
      "Run 2, Epoch 55, val_loss = 0.0880\n",
      "Run 2, Epoch 56, val_loss = 0.0922\n",
      "Run 2, Epoch 57, val_loss = 0.1019\n",
      "Run 2, Epoch 58, val_loss = 0.1070\n",
      "Run 2, Epoch 59, val_loss = 0.0948\n",
      "Run 2, Epoch 60, val_loss = 0.0831\n",
      "Run 2, Epoch 61, val_loss = 0.0832\n",
      "Run 2, Epoch 62, val_loss = 0.0775\n",
      "Run 2, Epoch 63, val_loss = 0.0714\n",
      "Run 2, Epoch 64, val_loss = 0.0673\n",
      "Run 2, Epoch 65, val_loss = 0.0870\n",
      "Run 2, Epoch 66, val_loss = 0.1832\n",
      "Run 2, Epoch 67, val_loss = 0.0773\n",
      "Run 2, Epoch 68, val_loss = 0.0715\n",
      "Run 2, Epoch 69, val_loss = 0.0931\n",
      "Run 2, Epoch 70, val_loss = 0.0809\n",
      "Run 2, Epoch 71, val_loss = 0.0807\n",
      "Run 2, Epoch 72, val_loss = 0.0892\n",
      "Run 2, Epoch 73, val_loss = 0.0667\n",
      "Run 2, Epoch 74, val_loss = 0.1019\n",
      "Run 2, Epoch 75, val_loss = 0.0798\n",
      "Run 2, Epoch 76, val_loss = 0.0838\n",
      "Run 2, Epoch 77, val_loss = 0.1299\n",
      "Run 2, Epoch 78, val_loss = 0.0883\n",
      "Run 2, Epoch 79, val_loss = 0.0646\n",
      "Run 2, Epoch 80, val_loss = 0.0912\n",
      "Run 2, Epoch 81, val_loss = 0.0854\n",
      "Run 2, Epoch 82, val_loss = 0.0798\n",
      "Run 2, Epoch 83, val_loss = 0.0643\n",
      "Run 2, Epoch 84, val_loss = 0.0662\n",
      "Run 2, Epoch 85, val_loss = 0.0947\n",
      "Run 2, Epoch 86, val_loss = 0.0629\n",
      "Run 2, Epoch 87, val_loss = 0.0684\n",
      "Run 2, Epoch 88, val_loss = 0.0729\n",
      "Run 2, Epoch 89, val_loss = 0.0974\n",
      "Run 2, Epoch 90, val_loss = 0.0814\n",
      "Run 2, Epoch 91, val_loss = 0.0907\n",
      "Run 2, Epoch 92, val_loss = 0.0674\n",
      "Run 2, Epoch 93, val_loss = 0.0825\n",
      "Run 2, Epoch 94, val_loss = 0.0700\n",
      "Run 2, Epoch 95, val_loss = 0.0715\n",
      "Run 2, Epoch 96, val_loss = 0.1007\n",
      "Run 2, Epoch 97, val_loss = 0.0760\n",
      "Run 2, Epoch 98, val_loss = 0.0658\n",
      "Run 2, Epoch 99, val_loss = 0.0880\n",
      "Run 2, Epoch 100, val_loss = 0.1476\n",
      "Run 2, Epoch 101, val_loss = 0.0590\n",
      "Run 2, Epoch 102, val_loss = 0.0731\n",
      "Run 2, Epoch 103, val_loss = 0.0678\n",
      "Run 2, Epoch 104, val_loss = 0.0783\n",
      "Run 2, Epoch 105, val_loss = 0.0646\n",
      "Run 2, Epoch 106, val_loss = 0.0682\n",
      "Run 2, Epoch 107, val_loss = 0.0559\n",
      "Run 2, Epoch 108, val_loss = 0.0861\n",
      "Run 2, Epoch 109, val_loss = 0.0751\n",
      "Run 2, Epoch 110, val_loss = 0.0672\n",
      "Run 2, Epoch 111, val_loss = 0.0791\n",
      "Run 2, Epoch 112, val_loss = 0.0788\n",
      "Run 2, Epoch 113, val_loss = 0.1178\n",
      "Run 2, Epoch 114, val_loss = 0.0770\n",
      "Run 2, Epoch 115, val_loss = 0.0826\n",
      "Run 2, Epoch 116, val_loss = 0.0758\n",
      "Run 2, Epoch 117, val_loss = 0.0766\n",
      "Run 2, Epoch 118, val_loss = 0.0681\n",
      "Run 2, Epoch 119, val_loss = 0.0625\n",
      "Run 2, Epoch 120, val_loss = 0.0728\n",
      "Run 2, Epoch 121, val_loss = 0.1051\n",
      "Run 2, Epoch 122, val_loss = 0.0682\n",
      "Run 2, Epoch 123, val_loss = 0.0835\n",
      "Run 2, Epoch 124, val_loss = 0.0564\n",
      "Run 2, Epoch 125, val_loss = 0.0523\n",
      "Run 2, Epoch 126, val_loss = 0.0557\n",
      "Run 2, Epoch 127, val_loss = 0.0542\n",
      "Run 2, Epoch 128, val_loss = 0.0604\n",
      "Run 2, Epoch 129, val_loss = 0.0607\n",
      "Run 2, Epoch 130, val_loss = 0.0540\n",
      "Run 2, Epoch 131, val_loss = 0.0491\n",
      "Run 2, Epoch 132, val_loss = 0.0511\n",
      "Run 2, Epoch 133, val_loss = 0.0528\n",
      "Run 2, Epoch 134, val_loss = 0.0576\n",
      "Run 2, Epoch 135, val_loss = 0.0557\n",
      "Run 2, Epoch 136, val_loss = 0.0561\n",
      "Run 2, Epoch 137, val_loss = 0.0600\n",
      "Run 2, Epoch 138, val_loss = 0.0660\n",
      "Run 2, Epoch 139, val_loss = 0.0830\n",
      "Run 2, Epoch 140, val_loss = 0.0593\n",
      "Run 2, Epoch 141, val_loss = 0.0558\n",
      "Run 2, Epoch 142, val_loss = 0.0639\n",
      "Run 2, Epoch 143, val_loss = 0.0569\n",
      "Run 2, Epoch 144, val_loss = 0.0650\n",
      "Run 2, Epoch 145, val_loss = 0.0694\n",
      "Run 2, Epoch 146, val_loss = 0.0549\n",
      "Run 2, Epoch 147, val_loss = 0.0609\n",
      "Run 2, Epoch 148, val_loss = 0.0558\n",
      "Run 2, Epoch 149, val_loss = 0.0621\n",
      "Run 2, Epoch 150, val_loss = 0.0655\n",
      "Run 2, Epoch 151, val_loss = 0.0611\n",
      "✅ Run 2: Acc = 99.64%, MDE = 0.0126\n",
      "Run 3/5 - Alpha = 1.0\n",
      "Run 3, Epoch 1, val_loss = 4.0016\n",
      "Run 3, Epoch 2, val_loss = 2.4427\n",
      "Run 3, Epoch 3, val_loss = 1.5326\n",
      "Run 3, Epoch 4, val_loss = 1.1149\n",
      "Run 3, Epoch 5, val_loss = 1.1233\n",
      "Run 3, Epoch 6, val_loss = 0.8678\n",
      "Run 3, Epoch 7, val_loss = 0.9435\n",
      "Run 3, Epoch 8, val_loss = 0.6340\n",
      "Run 3, Epoch 9, val_loss = 0.6350\n",
      "Run 3, Epoch 10, val_loss = 0.5115\n",
      "Run 3, Epoch 11, val_loss = 0.4233\n",
      "Run 3, Epoch 12, val_loss = 0.4124\n",
      "Run 3, Epoch 13, val_loss = 0.5451\n",
      "Run 3, Epoch 14, val_loss = 0.3138\n",
      "Run 3, Epoch 15, val_loss = 0.3461\n",
      "Run 3, Epoch 16, val_loss = 0.3014\n",
      "Run 3, Epoch 17, val_loss = 0.3450\n",
      "Run 3, Epoch 18, val_loss = 0.2897\n",
      "Run 3, Epoch 19, val_loss = 0.3133\n",
      "Run 3, Epoch 20, val_loss = 0.2605\n",
      "Run 3, Epoch 21, val_loss = 0.2443\n",
      "Run 3, Epoch 22, val_loss = 0.3266\n",
      "Run 3, Epoch 23, val_loss = 0.2946\n",
      "Run 3, Epoch 24, val_loss = 0.1672\n",
      "Run 3, Epoch 25, val_loss = 0.2510\n",
      "Run 3, Epoch 26, val_loss = 0.2253\n",
      "Run 3, Epoch 27, val_loss = 0.2003\n",
      "Run 3, Epoch 28, val_loss = 0.1431\n",
      "Run 3, Epoch 29, val_loss = 0.2478\n",
      "Run 3, Epoch 30, val_loss = 0.1764\n",
      "Run 3, Epoch 31, val_loss = 0.1487\n",
      "Run 3, Epoch 32, val_loss = 0.1989\n",
      "Run 3, Epoch 33, val_loss = 0.1380\n",
      "Run 3, Epoch 34, val_loss = 0.1358\n",
      "Run 3, Epoch 35, val_loss = 0.1765\n",
      "Run 3, Epoch 36, val_loss = 0.1216\n",
      "Run 3, Epoch 37, val_loss = 0.1219\n",
      "Run 3, Epoch 38, val_loss = 0.1152\n",
      "Run 3, Epoch 39, val_loss = 0.1618\n",
      "Run 3, Epoch 40, val_loss = 0.1416\n",
      "Run 3, Epoch 41, val_loss = 0.1200\n",
      "Run 3, Epoch 42, val_loss = 0.1326\n",
      "Run 3, Epoch 43, val_loss = 0.1055\n",
      "Run 3, Epoch 44, val_loss = 0.0913\n",
      "Run 3, Epoch 45, val_loss = 0.0805\n",
      "Run 3, Epoch 46, val_loss = 0.1277\n",
      "Run 3, Epoch 47, val_loss = 0.1053\n",
      "Run 3, Epoch 48, val_loss = 0.1019\n",
      "Run 3, Epoch 49, val_loss = 0.1122\n",
      "Run 3, Epoch 50, val_loss = 0.1148\n",
      "Run 3, Epoch 51, val_loss = 0.1672\n",
      "Run 3, Epoch 52, val_loss = 0.0845\n",
      "Run 3, Epoch 53, val_loss = 0.1281\n",
      "Run 3, Epoch 54, val_loss = 0.0844\n",
      "Run 3, Epoch 55, val_loss = 0.1033\n",
      "Run 3, Epoch 56, val_loss = 0.1050\n",
      "Run 3, Epoch 57, val_loss = 0.0952\n",
      "Run 3, Epoch 58, val_loss = 0.0894\n",
      "Run 3, Epoch 59, val_loss = 0.0929\n",
      "Run 3, Epoch 60, val_loss = 0.0902\n",
      "Run 3, Epoch 61, val_loss = 0.0828\n",
      "Run 3, Epoch 62, val_loss = 0.0661\n",
      "Run 3, Epoch 63, val_loss = 0.0564\n",
      "Run 3, Epoch 64, val_loss = 0.0704\n",
      "Run 3, Epoch 65, val_loss = 0.0755\n",
      "Run 3, Epoch 66, val_loss = 0.0583\n",
      "Run 3, Epoch 67, val_loss = 0.0675\n",
      "Run 3, Epoch 68, val_loss = 0.0702\n",
      "Run 3, Epoch 69, val_loss = 0.0619\n",
      "Run 3, Epoch 70, val_loss = 0.0690\n",
      "Run 3, Epoch 71, val_loss = 0.0641\n",
      "Run 3, Epoch 72, val_loss = 0.0631\n",
      "Run 3, Epoch 73, val_loss = 0.0717\n",
      "Run 3, Epoch 74, val_loss = 0.0657\n",
      "Run 3, Epoch 75, val_loss = 0.0623\n",
      "Run 3, Epoch 76, val_loss = 0.0694\n",
      "Run 3, Epoch 77, val_loss = 0.0589\n",
      "Run 3, Epoch 78, val_loss = 0.0557\n",
      "Run 3, Epoch 79, val_loss = 0.0548\n",
      "Run 3, Epoch 80, val_loss = 0.0645\n",
      "Run 3, Epoch 81, val_loss = 0.0634\n",
      "Run 3, Epoch 82, val_loss = 0.0592\n",
      "Run 3, Epoch 83, val_loss = 0.0707\n",
      "Run 3, Epoch 84, val_loss = 0.0534\n",
      "Run 3, Epoch 85, val_loss = 0.0672\n",
      "Run 3, Epoch 86, val_loss = 0.0627\n",
      "Run 3, Epoch 87, val_loss = 0.0707\n",
      "Run 3, Epoch 88, val_loss = 0.0632\n",
      "Run 3, Epoch 89, val_loss = 0.0545\n",
      "Run 3, Epoch 90, val_loss = 0.0532\n",
      "Run 3, Epoch 91, val_loss = 0.0672\n",
      "Run 3, Epoch 92, val_loss = 0.0630\n",
      "Run 3, Epoch 93, val_loss = 0.0466\n",
      "Run 3, Epoch 94, val_loss = 0.0651\n",
      "Run 3, Epoch 95, val_loss = 0.0579\n",
      "Run 3, Epoch 96, val_loss = 0.0586\n",
      "Run 3, Epoch 97, val_loss = 0.0504\n",
      "Run 3, Epoch 98, val_loss = 0.0551\n",
      "Run 3, Epoch 99, val_loss = 0.0720\n",
      "Run 3, Epoch 100, val_loss = 0.0621\n",
      "Run 3, Epoch 101, val_loss = 0.0599\n",
      "Run 3, Epoch 102, val_loss = 0.0587\n",
      "Run 3, Epoch 103, val_loss = 0.0738\n",
      "Run 3, Epoch 104, val_loss = 0.0699\n",
      "Run 3, Epoch 105, val_loss = 0.0605\n",
      "Run 3, Epoch 106, val_loss = 0.0549\n",
      "Run 3, Epoch 107, val_loss = 0.0624\n",
      "Run 3, Epoch 108, val_loss = 0.0615\n",
      "Run 3, Epoch 109, val_loss = 0.0644\n",
      "Run 3, Epoch 110, val_loss = 0.0616\n",
      "Run 3, Epoch 111, val_loss = 0.0627\n",
      "Run 3, Epoch 112, val_loss = 0.0587\n",
      "Run 3, Epoch 113, val_loss = 0.0622\n",
      "✅ Run 3: Acc = 99.69%, MDE = 0.0085\n",
      "Run 4/5 - Alpha = 1.0\n",
      "Run 4, Epoch 1, val_loss = 3.9357\n",
      "Run 4, Epoch 2, val_loss = 2.0537\n",
      "Run 4, Epoch 3, val_loss = 1.8094\n",
      "Run 4, Epoch 4, val_loss = 1.0717\n",
      "Run 4, Epoch 5, val_loss = 0.9518\n",
      "Run 4, Epoch 6, val_loss = 0.9539\n",
      "Run 4, Epoch 7, val_loss = 0.6270\n",
      "Run 4, Epoch 8, val_loss = 0.6613\n",
      "Run 4, Epoch 9, val_loss = 0.7312\n",
      "Run 4, Epoch 10, val_loss = 0.4958\n",
      "Run 4, Epoch 11, val_loss = 0.5801\n",
      "Run 4, Epoch 12, val_loss = 0.4314\n",
      "Run 4, Epoch 13, val_loss = 0.4402\n",
      "Run 4, Epoch 14, val_loss = 0.3608\n",
      "Run 4, Epoch 15, val_loss = 0.3320\n",
      "Run 4, Epoch 16, val_loss = 0.4683\n",
      "Run 4, Epoch 17, val_loss = 0.3126\n",
      "Run 4, Epoch 18, val_loss = 0.3150\n",
      "Run 4, Epoch 19, val_loss = 0.2533\n",
      "Run 4, Epoch 20, val_loss = 0.2847\n",
      "Run 4, Epoch 21, val_loss = 0.2170\n",
      "Run 4, Epoch 22, val_loss = 0.2524\n",
      "Run 4, Epoch 23, val_loss = 0.1916\n",
      "Run 4, Epoch 24, val_loss = 0.1869\n",
      "Run 4, Epoch 25, val_loss = 0.3395\n",
      "Run 4, Epoch 26, val_loss = 0.2218\n",
      "Run 4, Epoch 27, val_loss = 0.1815\n",
      "Run 4, Epoch 28, val_loss = 0.2465\n",
      "Run 4, Epoch 29, val_loss = 0.1670\n",
      "Run 4, Epoch 30, val_loss = 0.3009\n",
      "Run 4, Epoch 31, val_loss = 0.1592\n",
      "Run 4, Epoch 32, val_loss = 0.1601\n",
      "Run 4, Epoch 33, val_loss = 0.1556\n",
      "Run 4, Epoch 34, val_loss = 0.1313\n",
      "Run 4, Epoch 35, val_loss = 0.1591\n",
      "Run 4, Epoch 36, val_loss = 0.1645\n",
      "Run 4, Epoch 37, val_loss = 0.1445\n",
      "Run 4, Epoch 38, val_loss = 0.1593\n",
      "Run 4, Epoch 39, val_loss = 0.1314\n",
      "Run 4, Epoch 40, val_loss = 0.1094\n",
      "Run 4, Epoch 41, val_loss = 0.1119\n",
      "Run 4, Epoch 42, val_loss = 0.1425\n",
      "Run 4, Epoch 43, val_loss = 0.1649\n",
      "Run 4, Epoch 44, val_loss = 0.1296\n",
      "Run 4, Epoch 45, val_loss = 0.1132\n",
      "Run 4, Epoch 46, val_loss = 0.1194\n",
      "Run 4, Epoch 47, val_loss = 0.1532\n",
      "Run 4, Epoch 48, val_loss = 0.1212\n",
      "Run 4, Epoch 49, val_loss = 0.1632\n",
      "Run 4, Epoch 50, val_loss = 0.1235\n",
      "Run 4, Epoch 51, val_loss = 0.1284\n",
      "Run 4, Epoch 52, val_loss = 0.1529\n",
      "Run 4, Epoch 53, val_loss = 0.1027\n",
      "Run 4, Epoch 54, val_loss = 0.1185\n",
      "Run 4, Epoch 55, val_loss = 0.0996\n",
      "Run 4, Epoch 56, val_loss = 0.1166\n",
      "Run 4, Epoch 57, val_loss = 0.1100\n",
      "Run 4, Epoch 58, val_loss = 0.1002\n",
      "Run 4, Epoch 59, val_loss = 0.1046\n",
      "Run 4, Epoch 60, val_loss = 0.0988\n",
      "Run 4, Epoch 61, val_loss = 0.0942\n",
      "Run 4, Epoch 62, val_loss = 0.0892\n",
      "Run 4, Epoch 63, val_loss = 0.1114\n",
      "Run 4, Epoch 64, val_loss = 0.1674\n",
      "Run 4, Epoch 65, val_loss = 0.1118\n",
      "Run 4, Epoch 66, val_loss = 0.1061\n",
      "Run 4, Epoch 67, val_loss = 0.1034\n",
      "Run 4, Epoch 68, val_loss = 0.0925\n",
      "Run 4, Epoch 69, val_loss = 0.1048\n",
      "Run 4, Epoch 70, val_loss = 0.0780\n",
      "Run 4, Epoch 71, val_loss = 0.0860\n",
      "Run 4, Epoch 72, val_loss = 0.0711\n",
      "Run 4, Epoch 73, val_loss = 0.0948\n",
      "Run 4, Epoch 74, val_loss = 0.0755\n",
      "Run 4, Epoch 75, val_loss = 0.0919\n",
      "Run 4, Epoch 76, val_loss = 0.0846\n",
      "Run 4, Epoch 77, val_loss = 0.0836\n",
      "Run 4, Epoch 78, val_loss = 0.0970\n",
      "Run 4, Epoch 79, val_loss = 0.0969\n",
      "Run 4, Epoch 80, val_loss = 0.0933\n",
      "Run 4, Epoch 81, val_loss = 0.0843\n",
      "Run 4, Epoch 82, val_loss = 0.1414\n",
      "Run 4, Epoch 83, val_loss = 0.0890\n",
      "Run 4, Epoch 84, val_loss = 0.1012\n",
      "Run 4, Epoch 85, val_loss = 0.1543\n",
      "Run 4, Epoch 86, val_loss = 0.1161\n",
      "Run 4, Epoch 87, val_loss = 0.0700\n",
      "Run 4, Epoch 88, val_loss = 0.0707\n",
      "Run 4, Epoch 89, val_loss = 0.0819\n",
      "Run 4, Epoch 90, val_loss = 0.1044\n",
      "Run 4, Epoch 91, val_loss = 0.0756\n",
      "Run 4, Epoch 92, val_loss = 0.0802\n",
      "Run 4, Epoch 93, val_loss = 0.1230\n",
      "Run 4, Epoch 94, val_loss = 0.0967\n",
      "Run 4, Epoch 95, val_loss = 0.0804\n",
      "Run 4, Epoch 96, val_loss = 0.0940\n",
      "Run 4, Epoch 97, val_loss = 0.1134\n",
      "Run 4, Epoch 98, val_loss = 0.1170\n",
      "Run 4, Epoch 99, val_loss = 0.0795\n",
      "Run 4, Epoch 100, val_loss = 0.1209\n",
      "Run 4, Epoch 101, val_loss = 0.0725\n",
      "Run 4, Epoch 102, val_loss = 0.1225\n",
      "Run 4, Epoch 103, val_loss = 0.0682\n",
      "Run 4, Epoch 104, val_loss = 0.0793\n",
      "Run 4, Epoch 105, val_loss = 0.0687\n",
      "Run 4, Epoch 106, val_loss = 0.0800\n",
      "Run 4, Epoch 107, val_loss = 0.0715\n",
      "Run 4, Epoch 108, val_loss = 0.0771\n",
      "Run 4, Epoch 109, val_loss = 0.0841\n",
      "Run 4, Epoch 110, val_loss = 0.0719\n",
      "Run 4, Epoch 111, val_loss = 0.1053\n",
      "Run 4, Epoch 112, val_loss = 0.0808\n",
      "Run 4, Epoch 113, val_loss = 0.0810\n",
      "Run 4, Epoch 114, val_loss = 0.0649\n",
      "Run 4, Epoch 115, val_loss = 0.1090\n",
      "Run 4, Epoch 116, val_loss = 0.0777\n",
      "Run 4, Epoch 117, val_loss = 0.0754\n",
      "Run 4, Epoch 118, val_loss = 0.0717\n",
      "Run 4, Epoch 119, val_loss = 0.0819\n",
      "Run 4, Epoch 120, val_loss = 0.0953\n",
      "Run 4, Epoch 121, val_loss = 0.0778\n",
      "Run 4, Epoch 122, val_loss = 0.0772\n",
      "Run 4, Epoch 123, val_loss = 0.0698\n",
      "Run 4, Epoch 124, val_loss = 0.0653\n",
      "Run 4, Epoch 125, val_loss = 0.0721\n",
      "Run 4, Epoch 126, val_loss = 0.0687\n",
      "Run 4, Epoch 127, val_loss = 0.0870\n",
      "Run 4, Epoch 128, val_loss = 0.0718\n",
      "Run 4, Epoch 129, val_loss = 0.0821\n",
      "Run 4, Epoch 130, val_loss = 0.0978\n",
      "Run 4, Epoch 131, val_loss = 0.0664\n",
      "Run 4, Epoch 132, val_loss = 0.0590\n",
      "Run 4, Epoch 133, val_loss = 0.0624\n",
      "Run 4, Epoch 134, val_loss = 0.0713\n",
      "Run 4, Epoch 135, val_loss = 0.0751\n",
      "Run 4, Epoch 136, val_loss = 0.0819\n",
      "Run 4, Epoch 137, val_loss = 0.0710\n",
      "Run 4, Epoch 138, val_loss = 0.0776\n",
      "Run 4, Epoch 139, val_loss = 0.0676\n",
      "Run 4, Epoch 140, val_loss = 0.0725\n",
      "Run 4, Epoch 141, val_loss = 0.0646\n",
      "Run 4, Epoch 142, val_loss = 0.0599\n",
      "Run 4, Epoch 143, val_loss = 0.0644\n",
      "Run 4, Epoch 144, val_loss = 0.0744\n",
      "Run 4, Epoch 145, val_loss = 0.0591\n",
      "Run 4, Epoch 146, val_loss = 0.0865\n",
      "Run 4, Epoch 147, val_loss = 0.0631\n",
      "Run 4, Epoch 148, val_loss = 0.0671\n",
      "Run 4, Epoch 149, val_loss = 0.0568\n",
      "Run 4, Epoch 150, val_loss = 0.0691\n",
      "Run 4, Epoch 151, val_loss = 0.0564\n",
      "Run 4, Epoch 152, val_loss = 0.0573\n",
      "Run 4, Epoch 153, val_loss = 0.0585\n",
      "Run 4, Epoch 154, val_loss = 0.0553\n",
      "Run 4, Epoch 155, val_loss = 0.0538\n",
      "Run 4, Epoch 156, val_loss = 0.0598\n",
      "Run 4, Epoch 157, val_loss = 0.0582\n",
      "Run 4, Epoch 158, val_loss = 0.0578\n",
      "Run 4, Epoch 159, val_loss = 0.0560\n",
      "Run 4, Epoch 160, val_loss = 0.0551\n",
      "Run 4, Epoch 161, val_loss = 0.0674\n",
      "Run 4, Epoch 162, val_loss = 0.0610\n",
      "Run 4, Epoch 163, val_loss = 0.0541\n",
      "Run 4, Epoch 164, val_loss = 0.0504\n",
      "Run 4, Epoch 165, val_loss = 0.0538\n",
      "Run 4, Epoch 166, val_loss = 0.0623\n",
      "Run 4, Epoch 167, val_loss = 0.0670\n",
      "Run 4, Epoch 168, val_loss = 0.0629\n",
      "Run 4, Epoch 169, val_loss = 0.0662\n",
      "Run 4, Epoch 170, val_loss = 0.0556\n",
      "Run 4, Epoch 171, val_loss = 0.0617\n",
      "Run 4, Epoch 172, val_loss = 0.0579\n",
      "Run 4, Epoch 173, val_loss = 0.0596\n",
      "Run 4, Epoch 174, val_loss = 0.0673\n",
      "Run 4, Epoch 175, val_loss = 0.0611\n",
      "Run 4, Epoch 176, val_loss = 0.0739\n",
      "Run 4, Epoch 177, val_loss = 0.0682\n",
      "Run 4, Epoch 178, val_loss = 0.0627\n",
      "Run 4, Epoch 179, val_loss = 0.0667\n",
      "Run 4, Epoch 180, val_loss = 0.0596\n",
      "Run 4, Epoch 181, val_loss = 0.0583\n",
      "Run 4, Epoch 182, val_loss = 0.0555\n",
      "Run 4, Epoch 183, val_loss = 0.0573\n",
      "Run 4, Epoch 184, val_loss = 0.0551\n",
      "✅ Run 4: Acc = 99.69%, MDE = 0.0111\n",
      "Run 5/5 - Alpha = 1.0\n",
      "Run 5, Epoch 1, val_loss = 3.7857\n",
      "Run 5, Epoch 2, val_loss = 2.2442\n",
      "Run 5, Epoch 3, val_loss = 1.7346\n",
      "Run 5, Epoch 4, val_loss = 1.4803\n",
      "Run 5, Epoch 5, val_loss = 1.0043\n",
      "Run 5, Epoch 6, val_loss = 0.9062\n",
      "Run 5, Epoch 7, val_loss = 0.8244\n",
      "Run 5, Epoch 8, val_loss = 0.7069\n",
      "Run 5, Epoch 9, val_loss = 0.5262\n",
      "Run 5, Epoch 10, val_loss = 0.5089\n",
      "Run 5, Epoch 11, val_loss = 0.5910\n",
      "Run 5, Epoch 12, val_loss = 0.4421\n",
      "Run 5, Epoch 13, val_loss = 0.5678\n",
      "Run 5, Epoch 14, val_loss = 0.3754\n",
      "Run 5, Epoch 15, val_loss = 0.4192\n",
      "Run 5, Epoch 16, val_loss = 0.3578\n",
      "Run 5, Epoch 17, val_loss = 0.3397\n",
      "Run 5, Epoch 18, val_loss = 0.3105\n",
      "Run 5, Epoch 19, val_loss = 0.3014\n",
      "Run 5, Epoch 20, val_loss = 0.2975\n",
      "Run 5, Epoch 21, val_loss = 0.2880\n",
      "Run 5, Epoch 22, val_loss = 0.2249\n",
      "Run 5, Epoch 23, val_loss = 0.1976\n",
      "Run 5, Epoch 24, val_loss = 0.2065\n",
      "Run 5, Epoch 25, val_loss = 0.4210\n",
      "Run 5, Epoch 26, val_loss = 0.2183\n",
      "Run 5, Epoch 27, val_loss = 0.1745\n",
      "Run 5, Epoch 28, val_loss = 0.1664\n",
      "Run 5, Epoch 29, val_loss = 0.2045\n",
      "Run 5, Epoch 30, val_loss = 0.1516\n",
      "Run 5, Epoch 31, val_loss = 0.1898\n",
      "Run 5, Epoch 32, val_loss = 0.1336\n",
      "Run 5, Epoch 33, val_loss = 0.2558\n",
      "Run 5, Epoch 34, val_loss = 0.1539\n",
      "Run 5, Epoch 35, val_loss = 0.1799\n",
      "Run 5, Epoch 36, val_loss = 0.1313\n",
      "Run 5, Epoch 37, val_loss = 0.1250\n",
      "Run 5, Epoch 38, val_loss = 0.1694\n",
      "Run 5, Epoch 39, val_loss = 0.1227\n",
      "Run 5, Epoch 40, val_loss = 0.1899\n",
      "Run 5, Epoch 41, val_loss = 0.1207\n",
      "Run 5, Epoch 42, val_loss = 0.1195\n",
      "Run 5, Epoch 43, val_loss = 0.1213\n",
      "Run 5, Epoch 44, val_loss = 0.1252\n",
      "Run 5, Epoch 45, val_loss = 0.1408\n",
      "Run 5, Epoch 46, val_loss = 0.1178\n",
      "Run 5, Epoch 47, val_loss = 0.1281\n",
      "Run 5, Epoch 48, val_loss = 0.1110\n",
      "Run 5, Epoch 49, val_loss = 0.1194\n",
      "Run 5, Epoch 50, val_loss = 0.1247\n",
      "Run 5, Epoch 51, val_loss = 0.1408\n",
      "Run 5, Epoch 52, val_loss = 0.1005\n",
      "Run 5, Epoch 53, val_loss = 0.1041\n",
      "Run 5, Epoch 54, val_loss = 0.1149\n",
      "Run 5, Epoch 55, val_loss = 0.0923\n",
      "Run 5, Epoch 56, val_loss = 0.1277\n",
      "Run 5, Epoch 57, val_loss = 0.1306\n",
      "Run 5, Epoch 58, val_loss = 0.1026\n",
      "Run 5, Epoch 59, val_loss = 0.1446\n",
      "Run 5, Epoch 60, val_loss = 0.0985\n",
      "Run 5, Epoch 61, val_loss = 0.0883\n",
      "Run 5, Epoch 62, val_loss = 0.0987\n",
      "Run 5, Epoch 63, val_loss = 0.1096\n",
      "Run 5, Epoch 64, val_loss = 0.1064\n",
      "Run 5, Epoch 65, val_loss = 0.1202\n",
      "Run 5, Epoch 66, val_loss = 0.1082\n",
      "Run 5, Epoch 67, val_loss = 0.1352\n",
      "Run 5, Epoch 68, val_loss = 0.0759\n",
      "Run 5, Epoch 69, val_loss = 0.1028\n",
      "Run 5, Epoch 70, val_loss = 0.0933\n",
      "Run 5, Epoch 71, val_loss = 0.1113\n",
      "Run 5, Epoch 72, val_loss = 0.0972\n",
      "Run 5, Epoch 73, val_loss = 0.1093\n",
      "Run 5, Epoch 74, val_loss = 0.1149\n",
      "Run 5, Epoch 75, val_loss = 0.0959\n",
      "Run 5, Epoch 76, val_loss = 0.1249\n",
      "Run 5, Epoch 77, val_loss = 0.1079\n",
      "Run 5, Epoch 78, val_loss = 0.1100\n",
      "Run 5, Epoch 79, val_loss = 0.0979\n",
      "Run 5, Epoch 80, val_loss = 0.1083\n",
      "Run 5, Epoch 81, val_loss = 0.1130\n",
      "Run 5, Epoch 82, val_loss = 0.0967\n",
      "Run 5, Epoch 83, val_loss = 0.1255\n",
      "Run 5, Epoch 84, val_loss = 0.0965\n",
      "Run 5, Epoch 85, val_loss = 0.0775\n",
      "Run 5, Epoch 86, val_loss = 0.0889\n",
      "Run 5, Epoch 87, val_loss = 0.0785\n",
      "Run 5, Epoch 88, val_loss = 0.0887\n",
      "✅ Run 5: Acc = 99.64%, MDE = 0.0111\n",
      "📁 Results & all errors saved to repeat_copy/10\n",
      "=== 所有 alpha 執行完畢 ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "import copy\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用裝置:\", device)\n",
    "\n",
    "def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "    errors = []\n",
    "    for true_label, pred_label in zip(y_true, y_pred):\n",
    "        if true_label not in coordinates or pred_label not in coordinates:\n",
    "            print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "            continue\n",
    "        true_coord = np.array(coordinates[true_label])\n",
    "        pred_coord = np.array(coordinates[pred_label])\n",
    "        error = np.linalg.norm(pred_coord - true_coord)\n",
    "        errors.append(error)\n",
    "    return np.mean(errors), errors\n",
    "\n",
    "COORDINATES = {\n",
    "    1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "    36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "    2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4), 6: (0, 3.0),\n",
    "    7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "    12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "    16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "    20: (5.4, 6.0), 21: (6.0, 6.0), 22: (6.0, 5.4), 23: (6.0, 4.8),\n",
    "    24: (6.0, 4.2), 25: (6.0, 3.6), 26: (6.0, 3.0), 27: (6.0, 2.4),\n",
    "    28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "    41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8), 44: (3.0, 2.4),\n",
    "    45: (3.0, 3.0), 46: (3.0, 3.6), 47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "}\n",
    "\n",
    "def labels_to_coords(label_tensor, coord_dict):\n",
    "    coords = []\n",
    "    for label in label_tensor:\n",
    "        coords.append(coord_dict[label.item() + 1])\n",
    "    return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_reg = nn.MSELoss()\n",
    "\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "num_runs = 5\n",
    "epochs = 200\n",
    "patience = 20\n",
    "\n",
    "for alpha in alphas:\n",
    "    test_accs = []\n",
    "    test_mdes = []\n",
    "    all_run_errors = []\n",
    "\n",
    "    print(f\"\\n[Alpha = {alpha:.1f}] 開始 {num_runs} 次訓練\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Run {run}/{num_runs} - Alpha = {alpha:.1f}\")\n",
    "        model = CSIRSSI_DualHead_BN(num_classes=49, rssi_dim=4).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "        best_state_dict = None  # ⭐新增\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for csi_inputs, rssi_inputs, labels in train_loader:\n",
    "                csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                target_class = torch.argmax(labels, dim=1)\n",
    "                optimizer.zero_grad()\n",
    "                class_out, reg_out = model(csi_inputs, rssi_inputs)\n",
    "                loss_cls = criterion(class_out, target_class)\n",
    "                loss_reg = criterion_reg(reg_out, labels_to_coords(target_class, COORDINATES))\n",
    "                loss = loss_cls + alpha * loss_reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for csi_inputs, rssi_inputs, labels in val_loader:\n",
    "                    csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                    target_class = torch.argmax(labels, dim=1)\n",
    "                    class_out, reg_out = model(csi_inputs, rssi_inputs)\n",
    "                    loss_cls = criterion(class_out, target_class)\n",
    "                    loss_reg = criterion_reg(reg_out, labels_to_coords(target_class, COORDINATES))\n",
    "                    val_loss += (loss_cls + alpha * loss_reg).item() * csi_inputs.size(0)\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            print(f\"Run {run}, Epoch {epoch+1}, val_loss = {val_loss:.4f}\")  # ⭐印出val loss\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "                best_state_dict = copy.deepcopy(model.state_dict())  # ⭐記下最佳權重\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    break\n",
    "\n",
    "        # === 測試階段：用當run最佳模型測試 ===\n",
    "        model.load_state_dict(best_state_dict)\n",
    "        model.eval()\n",
    "        all_true, all_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for csi_inputs, rssi_inputs, labels in test_loader:\n",
    "                csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                target_class = torch.argmax(labels, dim=1)\n",
    "                class_out, _ = model(csi_inputs, rssi_inputs)\n",
    "                pred = torch.argmax(class_out, dim=1)\n",
    "                all_pred.extend(pred.cpu().numpy())\n",
    "                all_true.extend(target_class.cpu().numpy())\n",
    "\n",
    "        y_true = np.array(all_true) + 1\n",
    "        y_pred = np.array(all_pred) + 1\n",
    "        acc = 100 * np.mean(y_true == y_pred)\n",
    "        mde, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "        test_accs.append(acc)\n",
    "        test_mdes.append(mde)\n",
    "        all_run_errors.append(errors)\n",
    "        print(f\"✅ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "    # === 儲存 summary 與所有 error，檔名加 _b ===\n",
    "    alpha_id = int(round(alpha * 10))\n",
    "    folder_name = f\"repeat_copy/{alpha_id:02d}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Summary per run\n",
    "    df_runs = pd.DataFrame({\n",
    "        'run': list(range(1, num_runs + 1)),\n",
    "        'accuracy': test_accs,\n",
    "        'mde': test_mdes\n",
    "    })\n",
    "    df_runs.to_csv(f\"{folder_name}/bn2_csirssi_cls_reg_results{alpha_id:02d}_error2_b2.csv\", index=False)\n",
    "\n",
    "    # All errors (long format)\n",
    "    error_records = []\n",
    "    for run_idx, errors in enumerate(all_run_errors):\n",
    "        for sample_idx, e in enumerate(errors):\n",
    "            error_records.append({\n",
    "                \"run\": run_idx + 1,\n",
    "                \"sample_idx\": sample_idx + 1,\n",
    "                \"error\": e\n",
    "            })\n",
    "    df_errors = pd.DataFrame(error_records)\n",
    "    df_errors.to_csv(f\"{folder_name}/bn2_csirssi_cls_reg_all_errors2_{alpha_id:02d}_b2.csv\", index=False)\n",
    "    print(f\"📁 Results & all errors saved to {folder_name}\")\n",
    "\n",
    "print(\"=== 所有 alpha 執行完畢 ===\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Alpha = 0.1] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.1\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0087\n",
      "Run 2/5 - Alpha = 0.1\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0095\n",
      "Run 3/5 - Alpha = 0.1\n",
      "✅ Run 3: Acc = 99.67%, MDE = 0.0082\n",
      "Run 4/5 - Alpha = 0.1\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0092\n",
      "Run 5/5 - Alpha = 0.1\n",
      "✅ Run 5: Acc = 99.63%, MDE = 0.0139\n",
      "\n",
      "[Alpha = 0.2] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.2\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0066\n",
      "Run 2/5 - Alpha = 0.2\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0102\n",
      "Run 3/5 - Alpha = 0.2\n",
      "✅ Run 3: Acc = 99.71%, MDE = 0.0088\n",
      "Run 4/5 - Alpha = 0.2\n",
      "✅ Run 4: Acc = 99.71%, MDE = 0.0092\n",
      "Run 5/5 - Alpha = 0.2\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0099\n",
      "\n",
      "[Alpha = 0.3] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.3\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0095\n",
      "Run 2/5 - Alpha = 0.3\n",
      "✅ Run 2: Acc = 99.59%, MDE = 0.0146\n",
      "Run 3/5 - Alpha = 0.3\n",
      "✅ Run 3: Acc = 99.76%, MDE = 0.0075\n",
      "Run 4/5 - Alpha = 0.3\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0102\n",
      "Run 5/5 - Alpha = 0.3\n",
      "✅ Run 5: Acc = 99.71%, MDE = 0.0074\n",
      "\n",
      "[Alpha = 0.4] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.4\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0098\n",
      "Run 2/5 - Alpha = 0.4\n",
      "✅ Run 2: Acc = 99.76%, MDE = 0.0068\n",
      "Run 3/5 - Alpha = 0.4\n",
      "✅ Run 3: Acc = 99.67%, MDE = 0.0098\n",
      "Run 4/5 - Alpha = 0.4\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0096\n",
      "Run 5/5 - Alpha = 0.4\n",
      "✅ Run 5: Acc = 99.71%, MDE = 0.0081\n",
      "\n",
      "[Alpha = 0.5] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.5\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0084\n",
      "Run 2/5 - Alpha = 0.5\n",
      "✅ Run 2: Acc = 99.63%, MDE = 0.0119\n",
      "Run 3/5 - Alpha = 0.5\n",
      "✅ Run 3: Acc = 99.67%, MDE = 0.0089\n",
      "Run 4/5 - Alpha = 0.5\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0152\n",
      "Run 5/5 - Alpha = 0.5\n",
      "✅ Run 5: Acc = 99.71%, MDE = 0.0100\n",
      "\n",
      "[Alpha = 0.6] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.6\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0075\n",
      "Run 2/5 - Alpha = 0.6\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0092\n",
      "Run 3/5 - Alpha = 0.6\n",
      "✅ Run 3: Acc = 99.67%, MDE = 0.0137\n",
      "Run 4/5 - Alpha = 0.6\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0120\n",
      "Run 5/5 - Alpha = 0.6\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0100\n",
      "\n",
      "[Alpha = 0.7] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.7\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0149\n",
      "Run 2/5 - Alpha = 0.7\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0111\n",
      "Run 3/5 - Alpha = 0.7\n",
      "✅ Run 3: Acc = 99.71%, MDE = 0.0115\n",
      "Run 4/5 - Alpha = 0.7\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0102\n",
      "Run 5/5 - Alpha = 0.7\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0114\n",
      "\n",
      "[Alpha = 0.8] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.8\n",
      "✅ Run 1: Acc = 99.59%, MDE = 0.0123\n",
      "Run 2/5 - Alpha = 0.8\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0130\n",
      "Run 3/5 - Alpha = 0.8\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0143\n",
      "Run 4/5 - Alpha = 0.8\n",
      "✅ Run 4: Acc = 99.71%, MDE = 0.0113\n",
      "Run 5/5 - Alpha = 0.8\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0148\n",
      "\n",
      "[Alpha = 0.9] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.9\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0095\n",
      "Run 2/5 - Alpha = 0.9\n",
      "✅ Run 2: Acc = 99.59%, MDE = 0.0170\n",
      "Run 3/5 - Alpha = 0.9\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0176\n",
      "Run 4/5 - Alpha = 0.9\n",
      "✅ Run 4: Acc = 99.63%, MDE = 0.0108\n",
      "Run 5/5 - Alpha = 0.9\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0088\n",
      "\n",
      "[Alpha = 1.0] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 1.0\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0108\n",
      "Run 2/5 - Alpha = 1.0\n",
      "✅ Run 2: Acc = 99.59%, MDE = 0.0109\n",
      "Run 3/5 - Alpha = 1.0\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0112\n",
      "Run 4/5 - Alpha = 1.0\n",
      "✅ Run 4: Acc = 99.63%, MDE = 0.0096\n",
      "Run 5/5 - Alpha = 1.0\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0074\n"
     ]
    }
   ],
   "source": [
    "# import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import pandas as pd\n",
    "# import os\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # 假設 COORDINATES 是一個 dict，將 class index 映射到 (x, y) 實座標\n",
    "# def compute_mean_distance_error(y_true, y_pred, coordinates):\n",
    "#     \"\"\"\n",
    "#     y_true, y_pred: 一維的 NumPy 陣列，分別存放真實和預測的 label（整數）\n",
    "#     coordinates: dict, label -> (x, y)\n",
    "#     \"\"\"\n",
    "#     errors = []\n",
    "\n",
    "#     for true_label, pred_label in zip(y_true, y_pred):\n",
    "#         # 取出對應的座標\n",
    "#         if true_label not in coordinates or pred_label not in coordinates:\n",
    "#             # 若某個 label 不在座標字典內，就跳過（或視需求處理）\n",
    "#             print(f\"Label {true_label} or {pred_label} not in coordinates.\")\n",
    "#             continue\n",
    "#         true_coord = np.array(coordinates[true_label])\n",
    "#         pred_coord = np.array(coordinates[pred_label])\n",
    "#         # 計算歐氏距離\n",
    "#         error = np.linalg.norm(pred_coord - true_coord)\n",
    "#         errors.append(error)\n",
    "#     return np.mean(errors) , errors\n",
    "\n",
    "# COORDINATES = {\n",
    "#     # 下邊界 (1-10 和 40-31)\n",
    "#     1: (0, 0), 40: (0.6, 0), 39: (1.2, 0), 38: (1.8, 0), 37: (2.4, 0),\n",
    "#     36: (3.0, 0), 35: (3.6, 0), 34: (4.2, 0), 33: (4.8, 0), 32: (5.4, 0), 31: (6.0, 0),\n",
    "#     # 左邊界 (1-11)\n",
    "#     2: (0, 0.6), 3: (0, 1.2), 4: (0, 1.8), 5: (0, 2.4),\n",
    "#     6: (0, 3.0), 7: (0, 3.6), 8: (0, 4.2), 9: (0, 4.8), 10: (0, 5.4), 11: (0, 6.0),\n",
    "#     # 上邊界 (11-21)\n",
    "#     12: (0.6, 6.0), 13: (1.2, 6.0), 14: (1.8, 6.0), 15: (2.4, 6.0),\n",
    "#     16: (3.0, 6.0), 17: (3.6, 6.0), 18: (4.2, 6.0), 19: (4.8, 6.0),\n",
    "#     20: (5.4, 6.0), 21: (6.0, 6.0),\n",
    "#     # 右邊界 (21-31)\n",
    "#     22: (6.0, 5.4), 23: (6.0, 4.8), 24: (6.0, 4.2), 25: (6.0, 3.6),\n",
    "#     26: (6.0, 3.0), 27: (6.0, 2.4), 28: (6.0, 1.8), 29: (6.0, 1.2), 30: (6.0, 0.6),\n",
    "#     # 中間點 (41-49)\n",
    "#     41: (3.0, 0.6), 42: (3.0, 1.2), 43: (3.0, 1.8),\n",
    "#     44: (3.0, 2.4), 45: (3.0, 3.0), 46: (3.0, 3.6),\n",
    "#     47: (3.0, 4.2), 48: (3.0, 4.8), 49: (3.0, 5.4)\n",
    "# }\n",
    "\n",
    "# def labels_to_coords(label_tensor, coord_dict):\n",
    "#     coords = []\n",
    "#     for label in label_tensor:\n",
    "#         # 將 0-index 轉換成 1-index (例如 0 -> 1, 1 -> 2, ..., 48 -> 49)\n",
    "#         coords.append(coord_dict[label.item() + 1])\n",
    "#     return torch.tensor(coords, dtype=torch.float32, device=label_tensor.device)\n",
    "\n",
    "# # Criterion\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "# criterion_reg = nn.MSELoss()\n",
    "\n",
    "# # 遍歷 alpha\n",
    "# alphas = np.arange(0.1, 1.1, 0.1)\n",
    "# num_runs = 5\n",
    "# epochs = 300\n",
    "# patience = 20\n",
    "\n",
    "# summary_results = []\n",
    "\n",
    "# # 假設 train_loader, val_loader, test_loader 已定義\n",
    "# # 並且每個 batch 回傳 (csi_input, rssi_input, labels)\n",
    "\n",
    "# for alpha in alphas:\n",
    "#     test_accs = []\n",
    "#     test_mdes = []\n",
    "#     print(f\"\\n[Alpha = {alpha:.1f}] 開始 {num_runs} 次訓練\")\n",
    "#     for run in range(1, num_runs + 1):\n",
    "#         print(f\"Run {run}/{num_runs} - Alpha = {alpha:.1f}\")\n",
    "#         model = CSIRSSI_DualHead_BN(num_classes=49, rssi_dim=4).to(device)\n",
    "#         optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "#         scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "#         best_val_loss = float('inf')\n",
    "#         counter = 0\n",
    "\n",
    "#         for epoch in range(epochs):\n",
    "#             model.train()\n",
    "#             for csi_inputs, rssi_inputs, labels in train_loader:\n",
    "#                 csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "#                 target_class = torch.argmax(labels, dim=1)\n",
    "#                 optimizer.zero_grad()\n",
    "#                 class_out, reg_out = model(csi_inputs, rssi_inputs)\n",
    "#                 loss_cls = criterion(class_out, target_class)\n",
    "#                 loss_reg = criterion_reg(reg_out, labels_to_coords(target_class, COORDINATES))\n",
    "#                 loss = loss_cls + alpha * loss_reg\n",
    "#                 loss.backward()\n",
    "#                 optimizer.step()\n",
    "\n",
    "#             model.eval()\n",
    "#             val_loss = 0.0\n",
    "#             with torch.no_grad():\n",
    "#                 for csi_inputs, rssi_inputs, labels in val_loader:\n",
    "#                     csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "#                     target_class = torch.argmax(labels, dim=1)\n",
    "#                     class_out, reg_out = model(csi_inputs, rssi_inputs)\n",
    "#                     loss_cls = criterion(class_out, target_class)\n",
    "#                     loss_reg = criterion_reg(reg_out, labels_to_coords(target_class, COORDINATES))\n",
    "#                     val_loss += (loss_cls + alpha * loss_reg).item() * csi_inputs.size(0)\n",
    "\n",
    "#             val_loss /= len(val_loader.dataset)\n",
    "#             scheduler.step(val_loss)\n",
    "\n",
    "#             if val_loss < best_val_loss:\n",
    "#                 best_val_loss = val_loss\n",
    "#                 counter = 0\n",
    "#             else:\n",
    "#                 counter += 1\n",
    "#                 if counter >= patience:\n",
    "#                     break\n",
    "\n",
    "#         # Testing\n",
    "#         model.eval()\n",
    "#         all_true, all_pred = [], []\n",
    "#         with torch.no_grad():\n",
    "#             for csi_inputs, rssi_inputs, labels in test_loader:\n",
    "#                 csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "#                 target_class = torch.argmax(labels, dim=1)\n",
    "#                 class_out, _ = model(csi_inputs, rssi_inputs)\n",
    "#                 pred = torch.argmax(class_out, dim=1)\n",
    "#                 all_pred.extend(pred.cpu().numpy())\n",
    "#                 all_true.extend(target_class.cpu().numpy())\n",
    "\n",
    "#         y_true = np.array(all_true) + 1\n",
    "#         y_pred = np.array(all_pred) + 1\n",
    "#         acc = 100 * np.mean(y_true == y_pred)\n",
    "#         mde, _ = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "#         test_accs.append(acc)\n",
    "#         test_mdes.append(mde)\n",
    "#         print(f\"✅ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "#     alpha_id = int(round(alpha * 10))\n",
    "#     folder_name = f\"repeat/{alpha_id:02d}\"\n",
    "#     os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "#     df_runs = pd.DataFrame({\n",
    "#         'run': list(range(1, num_runs + 1)),\n",
    "#         'accuracy': test_accs,\n",
    "#         'mde': test_mdes\n",
    "#     })\n",
    "#     df_runs.to_csv(f\"{folder_name}/bn2_csirssi_cls_reg_results{alpha_id:02d}.csv\", index=False)\n",
    "\n",
    "#     summary_results.append({\n",
    "#         'alpha': alpha,\n",
    "#         'avg_acc': np.mean(test_accs),\n",
    "#         'std_acc': np.std(test_accs),\n",
    "#         'avg_mde': np.mean(test_mdes),\n",
    "#         'std_mde': np.std(test_mdes)\n",
    "#     })\n",
    "\n",
    "# summary_df = pd.DataFrame(summary_results)\n",
    "# #summary_df.to_csv(\"csirssi_alpha_comparison_summary.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#best path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "使用裝置: cuda\n",
      "\n",
      "[Alpha = 0.1] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.1\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0081\n",
      "Run 2/5 - Alpha = 0.1\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0092\n",
      "Run 3/5 - Alpha = 0.1\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0109\n",
      "Run 4/5 - Alpha = 0.1\n",
      "✅ Run 4: Acc = 99.63%, MDE = 0.0106\n",
      "Run 5/5 - Alpha = 0.1\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0098\n",
      "📁 Results & all errors saved to repeat/01\n",
      "\n",
      "[Alpha = 0.2] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.2\n",
      "✅ Run 1: Acc = 99.51%, MDE = 0.0153\n",
      "Run 2/5 - Alpha = 0.2\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0099\n",
      "Run 3/5 - Alpha = 0.2\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0109\n",
      "Run 4/5 - Alpha = 0.2\n",
      "✅ Run 4: Acc = 99.71%, MDE = 0.0137\n",
      "Run 5/5 - Alpha = 0.2\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0109\n",
      "📁 Results & all errors saved to repeat/02\n",
      "\n",
      "[Alpha = 0.3] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.3\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0083\n",
      "Run 2/5 - Alpha = 0.3\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0155\n",
      "Run 3/5 - Alpha = 0.3\n",
      "✅ Run 3: Acc = 99.71%, MDE = 0.0103\n",
      "Run 4/5 - Alpha = 0.3\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0072\n",
      "Run 5/5 - Alpha = 0.3\n",
      "✅ Run 5: Acc = 99.59%, MDE = 0.0134\n",
      "📁 Results & all errors saved to repeat/03\n",
      "\n",
      "[Alpha = 0.4] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.4\n",
      "✅ Run 1: Acc = 99.63%, MDE = 0.0121\n",
      "Run 2/5 - Alpha = 0.4\n",
      "✅ Run 2: Acc = 99.76%, MDE = 0.0083\n",
      "Run 3/5 - Alpha = 0.4\n",
      "✅ Run 3: Acc = 99.76%, MDE = 0.0081\n",
      "Run 4/5 - Alpha = 0.4\n",
      "✅ Run 4: Acc = 99.63%, MDE = 0.0110\n",
      "Run 5/5 - Alpha = 0.4\n",
      "✅ Run 5: Acc = 99.76%, MDE = 0.0066\n",
      "📁 Results & all errors saved to repeat/04\n",
      "\n",
      "[Alpha = 0.5] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.5\n",
      "✅ Run 1: Acc = 99.67%, MDE = 0.0142\n",
      "Run 2/5 - Alpha = 0.5\n",
      "✅ Run 2: Acc = 99.71%, MDE = 0.0082\n",
      "Run 3/5 - Alpha = 0.5\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0120\n",
      "Run 4/5 - Alpha = 0.5\n",
      "✅ Run 4: Acc = 99.71%, MDE = 0.0087\n",
      "Run 5/5 - Alpha = 0.5\n",
      "✅ Run 5: Acc = 99.71%, MDE = 0.0089\n",
      "📁 Results & all errors saved to repeat/05\n",
      "\n",
      "[Alpha = 0.6] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.6\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0112\n",
      "Run 2/5 - Alpha = 0.6\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0099\n",
      "Run 3/5 - Alpha = 0.6\n",
      "✅ Run 3: Acc = 99.71%, MDE = 0.0115\n",
      "Run 4/5 - Alpha = 0.6\n",
      "✅ Run 4: Acc = 99.71%, MDE = 0.0081\n",
      "Run 5/5 - Alpha = 0.6\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0101\n",
      "📁 Results & all errors saved to repeat/06\n",
      "\n",
      "[Alpha = 0.7] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.7\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0066\n",
      "Run 2/5 - Alpha = 0.7\n",
      "✅ Run 2: Acc = 99.71%, MDE = 0.0110\n",
      "Run 3/5 - Alpha = 0.7\n",
      "✅ Run 3: Acc = 99.71%, MDE = 0.0075\n",
      "Run 4/5 - Alpha = 0.7\n",
      "✅ Run 4: Acc = 99.67%, MDE = 0.0163\n",
      "Run 5/5 - Alpha = 0.7\n",
      "✅ Run 5: Acc = 99.76%, MDE = 0.0068\n",
      "📁 Results & all errors saved to repeat/07\n",
      "\n",
      "[Alpha = 0.8] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.8\n",
      "✅ Run 1: Acc = 99.63%, MDE = 0.0112\n",
      "Run 2/5 - Alpha = 0.8\n",
      "✅ Run 2: Acc = 99.63%, MDE = 0.0132\n",
      "Run 3/5 - Alpha = 0.8\n",
      "✅ Run 3: Acc = 99.55%, MDE = 0.0148\n",
      "Run 4/5 - Alpha = 0.8\n",
      "✅ Run 4: Acc = 99.76%, MDE = 0.0076\n",
      "Run 5/5 - Alpha = 0.8\n",
      "✅ Run 5: Acc = 99.67%, MDE = 0.0127\n",
      "📁 Results & all errors saved to repeat/08\n",
      "\n",
      "[Alpha = 0.9] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 0.9\n",
      "✅ Run 1: Acc = 99.76%, MDE = 0.0083\n",
      "Run 2/5 - Alpha = 0.9\n",
      "✅ Run 2: Acc = 99.63%, MDE = 0.0134\n",
      "Run 3/5 - Alpha = 0.9\n",
      "✅ Run 3: Acc = 99.63%, MDE = 0.0128\n",
      "Run 4/5 - Alpha = 0.9\n",
      "✅ Run 4: Acc = 99.71%, MDE = 0.0112\n",
      "Run 5/5 - Alpha = 0.9\n",
      "✅ Run 5: Acc = 99.76%, MDE = 0.0063\n",
      "📁 Results & all errors saved to repeat/09\n",
      "\n",
      "[Alpha = 1.0] 開始 5 次訓練\n",
      "Run 1/5 - Alpha = 1.0\n",
      "✅ Run 1: Acc = 99.71%, MDE = 0.0109\n",
      "Run 2/5 - Alpha = 1.0\n",
      "✅ Run 2: Acc = 99.67%, MDE = 0.0115\n",
      "Run 3/5 - Alpha = 1.0\n",
      "✅ Run 3: Acc = 99.67%, MDE = 0.0109\n",
      "Run 4/5 - Alpha = 1.0\n",
      "✅ Run 4: Acc = 99.63%, MDE = 0.0116\n",
      "Run 5/5 - Alpha = 1.0\n",
      "✅ Run 5: Acc = 99.59%, MDE = 0.0129\n",
      "📁 Results & all errors saved to repeat/10\n",
      "=== 所有 alpha 執行完畢 ===\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"使用裝置:\", device)\n",
    "alphas = np.arange(0.1, 1.1, 0.1)\n",
    "num_runs = 5\n",
    "epochs = 300\n",
    "patience = 20\n",
    "\n",
    "for alpha in alphas:\n",
    "    test_accs = []\n",
    "    test_mdes = []\n",
    "    all_run_errors = []\n",
    "\n",
    "    print(f\"\\n[Alpha = {alpha:.1f}] 開始 {num_runs} 次訓練\")\n",
    "    for run in range(1, num_runs + 1):\n",
    "        print(f\"Run {run}/{num_runs} - Alpha = {alpha:.1f}\")\n",
    "        model = CSIRSSI_DualHead_BN(num_classes=49, rssi_dim=4).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15)\n",
    "        best_val_loss = float('inf')\n",
    "        counter = 0\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            model.train()\n",
    "            for csi_inputs, rssi_inputs, labels in train_loader:\n",
    "                csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                target_class = torch.argmax(labels, dim=1)\n",
    "                optimizer.zero_grad()\n",
    "                class_out, reg_out = model(csi_inputs, rssi_inputs)\n",
    "                loss_cls = criterion(class_out, target_class)\n",
    "                loss_reg = criterion_reg(reg_out, labels_to_coords(target_class, COORDINATES))\n",
    "                loss = loss_cls + alpha * loss_reg\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            model.eval()\n",
    "            val_loss = 0.0\n",
    "            with torch.no_grad():\n",
    "                for csi_inputs, rssi_inputs, labels in val_loader:\n",
    "                    csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                    target_class = torch.argmax(labels, dim=1)\n",
    "                    class_out, reg_out = model(csi_inputs, rssi_inputs)\n",
    "                    loss_cls = criterion(class_out, target_class)\n",
    "                    loss_reg = criterion_reg(reg_out, labels_to_coords(target_class, COORDINATES))\n",
    "                    val_loss += (loss_cls + alpha * loss_reg).item() * csi_inputs.size(0)\n",
    "\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            scheduler.step(val_loss)\n",
    "\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                counter = 0\n",
    "                # 覆蓋存最佳模型\n",
    "                torch.save(model.state_dict(), \"repeat/rssicsi_reg_b_tmp.pth\")\n",
    "            else:\n",
    "                counter += 1\n",
    "                if counter >= patience:\n",
    "                    break\n",
    "\n",
    "        # === 測試階段：用最佳模型測試 ===\n",
    "        model.load_state_dict(torch.load(\"repeat/rssicsi_reg_b_tmp.pth\"))\n",
    "        model.eval()\n",
    "        all_true, all_pred = [], []\n",
    "        with torch.no_grad():\n",
    "            for csi_inputs, rssi_inputs, labels in test_loader:\n",
    "                csi_inputs, rssi_inputs, labels = csi_inputs.to(device), rssi_inputs.to(device), labels.to(device)\n",
    "                target_class = torch.argmax(labels, dim=1)\n",
    "                class_out, _ = model(csi_inputs, rssi_inputs)\n",
    "                pred = torch.argmax(class_out, dim=1)\n",
    "                all_pred.extend(pred.cpu().numpy())\n",
    "                all_true.extend(target_class.cpu().numpy())\n",
    "\n",
    "        y_true = np.array(all_true) + 1\n",
    "        y_pred = np.array(all_pred) + 1\n",
    "        acc = 100 * np.mean(y_true == y_pred)\n",
    "        mde, errors = compute_mean_distance_error(y_true, y_pred, COORDINATES)\n",
    "\n",
    "        test_accs.append(acc)\n",
    "        test_mdes.append(mde)\n",
    "        all_run_errors.append(errors)\n",
    "        print(f\"✅ Run {run}: Acc = {acc:.2f}%, MDE = {mde:.4f}\")\n",
    "\n",
    "    # === 儲存 summary 與所有 error，檔名加 _b ===\n",
    "    alpha_id = int(round(alpha * 10))\n",
    "    folder_name = f\"repeat/{alpha_id:02d}\"\n",
    "    os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "    # Summary per run\n",
    "    df_runs = pd.DataFrame({\n",
    "        'run': list(range(1, num_runs + 1)),\n",
    "        'accuracy': test_accs,\n",
    "        'mde': test_mdes\n",
    "    })\n",
    "    df_runs.to_csv(f\"{folder_name}/bn2_csirssi_cls_reg_results{alpha_id:02d}_error2_b.csv\", index=False)\n",
    "\n",
    "    # All errors (long format)\n",
    "    error_records = []\n",
    "    for run_idx, errors in enumerate(all_run_errors):\n",
    "        for sample_idx, e in enumerate(errors):\n",
    "            error_records.append({\n",
    "                \"run\": run_idx + 1,\n",
    "                \"sample_idx\": sample_idx + 1,\n",
    "                \"error\": e\n",
    "            })\n",
    "    df_errors = pd.DataFrame(error_records)\n",
    "    df_errors.to_csv(f\"{folder_name}/bn2_csirssi_cls_reg_all_errors2_{alpha_id:02d}_b.csv\", index=False)\n",
    "    print(f\"📁 Results & all errors saved to {folder_name}\")\n",
    "\n",
    "print(\"=== 所有 alpha 執行完畢 ===\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
