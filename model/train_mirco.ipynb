{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 極小場域定位"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import sys, os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from csidataset import *\n",
    "import data_loader\n",
    "from data_loader import *\n",
    "sys.path.append(\"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool\")\n",
    "import denoise\n",
    "from model import *\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = \"/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point1.xlsx': 1, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point2.xlsx': 2, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point3.xlsx': 3, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point4.xlsx': 4, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point5.xlsx': 5, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point6.xlsx': 6, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point7.xlsx': 7, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point8.xlsx': 8, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point9.xlsx': 9, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point10.xlsx': 10, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point11.xlsx': 11, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point12.xlsx': 12, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point13.xlsx': 13, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point14.xlsx': 14, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point15.xlsx': 15, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point16.xlsx': 16, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point17.xlsx': 17, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point18.xlsx': 18, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point19.xlsx': 19, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point20.xlsx': 20, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point21.xlsx': 21, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point22.xlsx': 22, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point23.xlsx': 23, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point24.xlsx': 24, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point25.xlsx': 25, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point26.xlsx': 26, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point27.xlsx': 27, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point28.xlsx': 28, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point29.xlsx': 29, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point30.xlsx': 30, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point31.xlsx': 31, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point32.xlsx': 32, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point33.xlsx': 33, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point34.xlsx': 34, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point35.xlsx': 35, '/media/mcs/1441ae67-d7cd-43e6-b028-169f78661a2f/kyle/csi_tool/csi_dataset/localization_phone/micro/0303/80Mhz/csv/reference_point36.xlsx': 36}\n"
     ]
    }
   ],
   "source": [
    "reference_points = {\n",
    "    f\"{base_path}/reference_point{i}.xlsx\": i for i in range(1, 37)\n",
    "}\n",
    "\n",
    "# 確認字典內容\n",
    "print(reference_points)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(reference_points):\n",
    "    data = []          \n",
    "    rp_labels = []     \n",
    "\n",
    "    for path, ref_id in reference_points.items():\n",
    "        df = pd.read_excel(path)\n",
    "        data.append(df.values)\n",
    "        rp_labels.extend([ref_id] * len(df))  # 只保留 Reference Point ID\n",
    "\n",
    "    data = pd.DataFrame(np.vstack(data))\n",
    "    rp_labels = pd.Series(rp_labels, name=\"Reference Point ID\")  # 轉為 Pandas Series\n",
    "\n",
    "    return data, rp_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data, rp_labels = load_data(reference_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7164, 470)\n",
      "(7164,)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(data.shape)       # 顯示數據\n",
    "print(rp_labels.shape)  # 顯示 Reference Point ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_data = np.array(data.iloc[:, :234])\n",
    "phase_data = np.array(data.iloc[:, 234:-2])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_d = denoise.preprocess_csi_for_fingerprint2(amp_data)\n",
    "phase_d = denoise.preprocess_csi_for_fingerprint2(phase_data)\n",
    "\n",
    "#amp_phase_d = np.concatenate((amp_d, phase_d), axis=1)\n",
    "#print(amp_phase_d.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "one_hot_labels = encoder.fit_transform(np.array(rp_labels).reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7164, 234)\n",
      "(7164, 234)\n",
      "(7164, 36)\n"
     ]
    }
   ],
   "source": [
    "print(amp_d.shape)\n",
    "print(phase_data.shape)\n",
    "print(one_hot_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "amp_train, amp_temp, y_train, y_temp = train_test_split(amp_d, one_hot_labels, test_size=0.3, random_state=42)\n",
    "amp_val, amp_test, y_val, y_test = train_test_split(amp_temp, y_temp, test_size=1/3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5014, 234)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "amp_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "train_dataset = CSIDataset(amp_train, y_train)\n",
    "val_dataset = CSIDataset(amp_val, y_val)\n",
    "test_dataset = CSIDataset(amp_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mcs/anaconda3/envs/kyle_ai/lib/python3.12/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 初始化模型\n",
    "maxlen = amp_d.shape[1]  # 序列長度\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 128\n",
    "num_classes = one_hot_labels.shape[1]\n",
    "num_layers = 1\n",
    "dropout_rate = 0.1\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TransformerClassifier(maxlen, embed_dim, num_heads, ff_dim, num_classes, num_layers, dropout_rate).to(device)\n",
    "\n",
    "# 損失函數\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# 優化器\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 學習率調整器\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=15, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------\n",
      "        Layer (type)               Output Shape         Param #\n",
      "================================================================\n",
      "         Embedding-1             [-1, 234, 128]          29,952\n",
      "            Linear-2             [-1, 234, 128]             256\n",
      "TokenAndPositionEmbedding-3             [-1, 234, 128]               0\n",
      "MultiheadAttention-4  [[-1, 234, 128], [-1, 234, 234]]               0\n",
      "           Dropout-5             [-1, 234, 128]               0\n",
      "         LayerNorm-6             [-1, 234, 128]             256\n",
      "            Linear-7             [-1, 234, 128]          16,512\n",
      "              ReLU-8             [-1, 234, 128]               0\n",
      "            Linear-9             [-1, 234, 128]          16,512\n",
      "          Dropout-10             [-1, 234, 128]               0\n",
      "        LayerNorm-11             [-1, 234, 128]             256\n",
      " TransformerBlock-12             [-1, 234, 128]               0\n",
      "AdaptiveAvgPool1d-13               [-1, 128, 1]               0\n",
      "           Linear-14                   [-1, 64]           8,256\n",
      "          Dropout-15                   [-1, 64]               0\n",
      "           Linear-16                   [-1, 36]           2,340\n",
      "================================================================\n",
      "Total params: 74,340\n",
      "Trainable params: 74,340\n",
      "Non-trainable params: 0\n",
      "----------------------------------------------------------------\n",
      "Input size (MB): 0.00\n",
      "Forward/backward pass size (MB): 12510.09\n",
      "Params size (MB): 0.28\n",
      "Estimated Total Size (MB): 12510.37\n",
      "----------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torchsummary import summary\n",
    "\n",
    "summary(model, input_size=(234, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/200] | Train Loss: 3.1588 | Train Acc: 10.35% | Val Loss: 2.4013 | Val Acc: 25.47% | time: 0.63 s\n",
      "✅ 儲存最佳模型 (Val Loss: 2.4013) 至 best_model_mirco.pth\n",
      "Epoch [2/200] | Train Loss: 1.7396 | Train Acc: 45.25% | Val Loss: 1.1278 | Val Acc: 70.90% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 1.1278) 至 best_model_mirco.pth\n",
      "Epoch [3/200] | Train Loss: 0.8944 | Train Acc: 74.59% | Val Loss: 0.6023 | Val Acc: 84.30% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.6023) 至 best_model_mirco.pth\n",
      "Epoch [4/200] | Train Loss: 0.6097 | Train Acc: 82.27% | Val Loss: 0.5472 | Val Acc: 84.44% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.5472) 至 best_model_mirco.pth\n",
      "Epoch [5/200] | Train Loss: 0.4549 | Train Acc: 86.94% | Val Loss: 0.4149 | Val Acc: 89.32% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.4149) 至 best_model_mirco.pth\n",
      "Epoch [6/200] | Train Loss: 0.3988 | Train Acc: 87.55% | Val Loss: 0.3756 | Val Acc: 90.23% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3756) 至 best_model_mirco.pth\n",
      "Epoch [7/200] | Train Loss: 0.3707 | Train Acc: 87.85% | Val Loss: 0.3652 | Val Acc: 89.18% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3652) 至 best_model_mirco.pth\n",
      "Epoch [8/200] | Train Loss: 0.3294 | Train Acc: 89.79% | Val Loss: 0.3919 | Val Acc: 89.32% | time: 0.56 s\n",
      "Epoch [9/200] | Train Loss: 0.3036 | Train Acc: 89.99% | Val Loss: 0.3829 | Val Acc: 89.04% | time: 0.56 s\n",
      "Epoch [10/200] | Train Loss: 0.2928 | Train Acc: 89.93% | Val Loss: 0.3538 | Val Acc: 90.44% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3538) 至 best_model_mirco.pth\n",
      "Epoch [11/200] | Train Loss: 0.2675 | Train Acc: 90.71% | Val Loss: 0.4210 | Val Acc: 88.63% | time: 0.56 s\n",
      "Epoch [12/200] | Train Loss: 0.2571 | Train Acc: 91.32% | Val Loss: 0.3136 | Val Acc: 91.42% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3136) 至 best_model_mirco.pth\n",
      "Epoch [13/200] | Train Loss: 0.2832 | Train Acc: 90.47% | Val Loss: 0.3664 | Val Acc: 90.23% | time: 0.56 s\n",
      "Epoch [14/200] | Train Loss: 0.2287 | Train Acc: 92.00% | Val Loss: 0.3273 | Val Acc: 91.84% | time: 0.56 s\n",
      "Epoch [15/200] | Train Loss: 0.2118 | Train Acc: 92.20% | Val Loss: 0.4902 | Val Acc: 87.65% | time: 0.56 s\n",
      "Epoch [16/200] | Train Loss: 0.2159 | Train Acc: 92.28% | Val Loss: 0.3360 | Val Acc: 91.00% | time: 0.56 s\n",
      "Epoch [17/200] | Train Loss: 0.2292 | Train Acc: 91.56% | Val Loss: 0.3068 | Val Acc: 92.18% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3068) 至 best_model_mirco.pth\n",
      "Epoch [18/200] | Train Loss: 0.2112 | Train Acc: 92.18% | Val Loss: 0.3092 | Val Acc: 92.46% | time: 0.56 s\n",
      "Epoch [19/200] | Train Loss: 0.1984 | Train Acc: 92.70% | Val Loss: 0.3055 | Val Acc: 92.25% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.3055) 至 best_model_mirco.pth\n",
      "Epoch [20/200] | Train Loss: 0.2117 | Train Acc: 92.42% | Val Loss: 0.3671 | Val Acc: 89.74% | time: 0.56 s\n",
      "Epoch [21/200] | Train Loss: 0.2003 | Train Acc: 92.96% | Val Loss: 0.3097 | Val Acc: 91.14% | time: 0.56 s\n",
      "Epoch [22/200] | Train Loss: 0.1656 | Train Acc: 93.90% | Val Loss: 0.3180 | Val Acc: 91.00% | time: 0.56 s\n",
      "Epoch [23/200] | Train Loss: 0.1674 | Train Acc: 93.42% | Val Loss: 0.2892 | Val Acc: 93.09% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2892) 至 best_model_mirco.pth\n",
      "Epoch [24/200] | Train Loss: 0.1816 | Train Acc: 93.52% | Val Loss: 0.3267 | Val Acc: 91.77% | time: 0.56 s\n",
      "Epoch [25/200] | Train Loss: 0.1950 | Train Acc: 92.60% | Val Loss: 0.2734 | Val Acc: 94.00% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2734) 至 best_model_mirco.pth\n",
      "Epoch [26/200] | Train Loss: 0.1652 | Train Acc: 93.86% | Val Loss: 0.3349 | Val Acc: 91.91% | time: 0.56 s\n",
      "Epoch [27/200] | Train Loss: 0.1589 | Train Acc: 94.06% | Val Loss: 0.2732 | Val Acc: 94.14% | time: 0.56 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2732) 至 best_model_mirco.pth\n",
      "Epoch [28/200] | Train Loss: 0.1678 | Train Acc: 93.88% | Val Loss: 0.2825 | Val Acc: 94.00% | time: 0.55 s\n",
      "Epoch [29/200] | Train Loss: 0.1579 | Train Acc: 94.26% | Val Loss: 0.3714 | Val Acc: 90.72% | time: 0.55 s\n",
      "Epoch [30/200] | Train Loss: 0.1586 | Train Acc: 94.18% | Val Loss: 0.2972 | Val Acc: 93.09% | time: 0.55 s\n",
      "Epoch [31/200] | Train Loss: 0.1509 | Train Acc: 94.58% | Val Loss: 0.4081 | Val Acc: 90.44% | time: 0.55 s\n",
      "Epoch [32/200] | Train Loss: 0.1618 | Train Acc: 93.82% | Val Loss: 0.3012 | Val Acc: 93.23% | time: 0.55 s\n",
      "Epoch [33/200] | Train Loss: 0.1425 | Train Acc: 94.48% | Val Loss: 0.3371 | Val Acc: 92.46% | time: 0.55 s\n",
      "Epoch [34/200] | Train Loss: 0.1353 | Train Acc: 95.09% | Val Loss: 0.3097 | Val Acc: 92.81% | time: 0.55 s\n",
      "Epoch [35/200] | Train Loss: 0.1429 | Train Acc: 94.71% | Val Loss: 0.2882 | Val Acc: 94.63% | time: 0.55 s\n",
      "Epoch [36/200] | Train Loss: 0.1307 | Train Acc: 95.25% | Val Loss: 0.3272 | Val Acc: 92.95% | time: 0.55 s\n",
      "Epoch [37/200] | Train Loss: 0.1471 | Train Acc: 94.85% | Val Loss: 0.3320 | Val Acc: 92.67% | time: 0.55 s\n",
      "Epoch [38/200] | Train Loss: 0.1227 | Train Acc: 95.49% | Val Loss: 0.2740 | Val Acc: 93.79% | time: 0.55 s\n",
      "Epoch [39/200] | Train Loss: 0.1202 | Train Acc: 95.59% | Val Loss: 0.2602 | Val Acc: 94.63% | time: 0.55 s\n",
      "✅ 儲存最佳模型 (Val Loss: 0.2602) 至 best_model_mirco.pth\n",
      "Epoch [40/200] | Train Loss: 0.1399 | Train Acc: 94.99% | Val Loss: 0.2790 | Val Acc: 93.93% | time: 0.55 s\n",
      "Epoch [41/200] | Train Loss: 0.1273 | Train Acc: 95.33% | Val Loss: 0.3155 | Val Acc: 93.23% | time: 0.55 s\n",
      "Epoch [42/200] | Train Loss: 0.1331 | Train Acc: 95.07% | Val Loss: 0.3176 | Val Acc: 94.49% | time: 0.55 s\n",
      "Epoch [43/200] | Train Loss: 0.1570 | Train Acc: 94.52% | Val Loss: 0.2820 | Val Acc: 94.42% | time: 0.55 s\n",
      "Epoch [44/200] | Train Loss: 0.1255 | Train Acc: 95.59% | Val Loss: 0.3147 | Val Acc: 92.88% | time: 0.55 s\n",
      "Epoch [45/200] | Train Loss: 0.1117 | Train Acc: 95.63% | Val Loss: 0.2829 | Val Acc: 94.77% | time: 0.55 s\n",
      "Epoch [46/200] | Train Loss: 0.1165 | Train Acc: 95.89% | Val Loss: 0.3028 | Val Acc: 94.14% | time: 0.56 s\n",
      "Epoch [47/200] | Train Loss: 0.0973 | Train Acc: 96.37% | Val Loss: 0.2857 | Val Acc: 94.77% | time: 0.55 s\n",
      "Epoch [48/200] | Train Loss: 0.0989 | Train Acc: 96.33% | Val Loss: 0.2755 | Val Acc: 94.56% | time: 0.55 s\n",
      "Epoch [49/200] | Train Loss: 0.0930 | Train Acc: 96.39% | Val Loss: 0.3016 | Val Acc: 94.84% | time: 0.55 s\n",
      "Epoch [50/200] | Train Loss: 0.1283 | Train Acc: 95.43% | Val Loss: 0.3248 | Val Acc: 92.74% | time: 0.55 s\n",
      "Epoch [51/200] | Train Loss: 0.1393 | Train Acc: 94.87% | Val Loss: 0.2933 | Val Acc: 94.77% | time: 0.55 s\n",
      "Epoch [52/200] | Train Loss: 0.1086 | Train Acc: 95.97% | Val Loss: 0.3914 | Val Acc: 91.97% | time: 0.55 s\n",
      "Epoch [53/200] | Train Loss: 0.1173 | Train Acc: 95.47% | Val Loss: 0.3618 | Val Acc: 91.56% | time: 0.55 s\n",
      "Epoch [54/200] | Train Loss: 0.1013 | Train Acc: 96.21% | Val Loss: 0.3135 | Val Acc: 94.14% | time: 0.55 s\n",
      "Epoch [55/200] | Train Loss: 0.1032 | Train Acc: 96.47% | Val Loss: 0.3017 | Val Acc: 95.39% | time: 0.55 s\n",
      "Epoch [56/200] | Train Loss: 0.1070 | Train Acc: 96.09% | Val Loss: 0.3306 | Val Acc: 93.65% | time: 0.55 s\n",
      "Epoch [57/200] | Train Loss: 0.1067 | Train Acc: 95.97% | Val Loss: 0.2708 | Val Acc: 95.12% | time: 0.55 s\n",
      "Epoch [58/200] | Train Loss: 0.0977 | Train Acc: 96.51% | Val Loss: 0.3854 | Val Acc: 92.18% | time: 0.55 s\n",
      "Epoch [59/200] | Train Loss: 0.1132 | Train Acc: 95.99% | Val Loss: 0.3131 | Val Acc: 94.91% | time: 0.55 s\n",
      "Early stop at epoch 59\n",
      "訓練完成！\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 儲存最佳模型\n",
    "best_val_loss = float('inf')\n",
    "best_model_path = \"best_model_mirco.pth\"\n",
    "\n",
    "# 訓練參數\n",
    "epochs = 200\n",
    "\n",
    "# early stop\n",
    "patience = 20\n",
    "counter = 0  \n",
    "\n",
    "# 訓練過程中的 loss 和 accuracy\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # ---- 訓練階段 ----\n",
    "    start_time = time.perf_counter()\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    total_train = 0\n",
    "\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # CrossEntropyLoss 需要 class index\n",
    "        loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_loss += loss.item() * inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        train_correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader.dataset)\n",
    "    train_acc = 100 * train_correct / total_train\n",
    "\n",
    "    # ----驗證階段----\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    total_val = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, torch.argmax(labels, dim=1))\n",
    "\n",
    "            val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_val += labels.size(0)\n",
    "            val_correct += (predicted == torch.argmax(labels, dim=1)).sum().item()\n",
    "\n",
    "    avg_val_loss = val_loss / len(val_loader.dataset)\n",
    "    val_acc = 100 * val_correct / total_val\n",
    "\n",
    "    end_time = time.perf_counter()\n",
    "    epoch_time = end_time - start_time\n",
    "\n",
    "    # 紀錄每個 epoch 的 loss 和 accuracy\n",
    "    train_losses.append(avg_train_loss)\n",
    "    val_losses.append(avg_val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "\n",
    "    # 輸出當前 epoch 的結果\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {avg_train_loss:.4f} | Train Acc: {train_acc:.2f}% | \"\n",
    "          f\"Val Loss: {avg_val_loss:.4f} | Val Acc: {val_acc:.2f}% | time: {epoch_time:.2f} s\")\n",
    "\n",
    "    # ---- 儲存最佳模型 ----\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        print(f\"✅ 儲存最佳模型 (Val Loss: {best_val_loss:.4f}) 至 {best_model_path}\")\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "\n",
    "    if counter >= patience:\n",
    "        print(f\"Early stop at epoch {epoch+1}\")\n",
    "        break\n",
    "\n",
    "print(\"訓練完成！\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kyle_ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
